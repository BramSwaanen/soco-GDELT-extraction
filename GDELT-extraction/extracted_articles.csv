Date,NewsPaper,Headline,MainText
20220429,cbsnews,A tiny research robot is living with an Antarctica penguin colony,"It's a story of unlikely friendship. A small research robot is currently living with an Emperor penguin colony in Antarctica, providing vital information for researchers in Cape Cod, CBS Boston reports.ECHO, the robot, belongs to the Woods Hole Oceanographic Institution and rolls around the tundra collecting data used to study marine ecosystems.The small robot takes readings and collects data like a normal researcher, but his existence allows researchers to collect real-time information year round and minimize the impact their presence could have on the animals' lives. Researchers say the penguins seem to be getting along swimmingly with the robot. ""The really cool thing is if I as a human would try to get as close as ECHO, they would be running away,"" the Marine Animal Remote Sensing Lab's Daniel Zitterbart told Popular Science. ""We don't want to scare the animals. Our general aim is to do more science with less impact. And humans have much bigger impact on the animals than the robot actually has."""
20230725,foxnews,Our schools’ war on AI is a national security threat,"Since the beginning of the AI race, the United States has been working hard to stay ahead.&nbsp;But it is time to realize that America’s superiority is at risk. The key intertwined contributing factors include the broken U.S. education system and fear of job loss.&nbsp; To stop the American AI leadership erosion we must think and act differently. To change the future, we must first change our thinking. Recent surveys suggest that 61% of Americans feel that AI is a threat to humanity, and another 17% are unsure if it is or not! Clearly, then, it is not surprising that Americans are concerned about AI in education despite its benefits for empowering students and preserving global leadership.&nbsp;  American workers are afraid of job losses sensationalized in the media. Labor unions like screenwriters are following old habits of pushing back on technology. This is the same mindset that forced the migration of U.S. manufacturing to China in the ’70s. TEACHERS TAKE AI CONCERNS INTO THEIR OWN HANDS AMID WARNING TECH POSES ‘GREATEST THREAT' TO SCHOOLS The anticipated job changes are real across the globe. AI is quickly impacting knowledge workers like administrative employees and legal professionals and challenging the conventional role of many in other jobs. But the answer is not to avoid AI but to learn to work differently. The impact on the skills needed for tomorrow is also very real. But, again,&nbsp;the answer is not to avoid teaching it to the new generation in hopes that it will go away.&nbsp; The U.S. must transform its mindset and education system to implement AI-led programs that protect its economic and national security. The country should also help workers transition to a new world dominated by AI-powered jobs. The mindset must drives teachers to think and teach differently and students to learn and apply their capabilities differently. Only then will those students help America secure her global competitiveness.  America’s mindset toward AI, particularly in the classroom, undermines its ability to compete in the AI race. By including AI in public school curriculums and teaching kids how to focus on creativity and understand the ethical use of tools, China's Hong Kong is preparing upcoming generations to lead a world redefined by AI.&nbsp; America is training for the now. We are not teaching our youth how to create, innovate and take things to the next level. Instead, politicians are introducing legislation to keep AI away from kids, and college professors want written assessments and oral exams to avoid ""cheating"" with AI tools. The world’s AI-influenced future holds great promise for innovation, productivity and convenience. But the overburdened and outdated American education system must be redesigned to acknowledge and respond to this new reality.&nbsp; IN EDUCATION, ‘AI IS INEVITABLE,’ AND STUDENTS WHO DON'T USE IT WILL ‘BE AT A DISADVANTAGE’: AI FOUNDER  As financial returns have become the primary measure of institutional success, teacher shortages and crowded classrooms have become a nagging problem across the board. Simultaneously, underpaid teachers are reluctant and unprepared to change their teaching drastically to fit an unknown new world.&nbsp; Politics has entered classrooms and learning to improve and globally compete has replaced itself with learning to push a social causes, impacting education priorities. Additionally, both parties’ partisan views of AI in education prevent new consensus curriculums from being formed. America’s future can only be on solid footing if its education system is responsive to the changes in the world. Education about AI requires the way we teach to change. AI chatbots in education could teach children to read in months, be a cost-effective alternative to tutoring, and let teachers do their jobs better.&nbsp;  We must stop dictating the mechanics of learning and teach people the purpose of their studies. We must abandon teaching mathematical routines and formulas and focus on creativity and innovation. We have to focus on what is important to express and not be afraid of an AI machine writing it. We have to learn to use AI to make our kids smarter and prepared for a fast-evolving humanity. PERSONALIZED CHATBOT TUTORS WILL LIKELY REVOLUTIONIZE TRADITIONAL EDUCATION AND BENEFIT STUDENTS: AI EXPERT Hunting for food and collecting water used to be the standard way of providing for your family. We taught those skills to our children, but in modern societies, the supermarket does that job. Not long ago, personal computers and remote learning were considered a threat to education. Way before AI existed, people believed tradition and technology were in a fight to the death. That belief was not true, and this new disbelief in AI’s role in our future is not either. Children always find a way to cheat. We’ve survived the proliferation of calculators, spell checkers, and other innovations. In the process of change, the consequences that have resulted may not have appeared ideal, but the benefits have been required for humanity to evolve.  Without adequate education, children will be susceptible to deep fakes, scamming and other abuse. They’ll be ignorant about the world and lack the skills future employers will need. The real danger is that if we don't act now, in a few short years we won’t have the right leaders to operate in a hyper-competitive world. America will be stuck in the past while India, China and other nations dominate the world. No matter how behind in the AI race some countries may seem, technology moves quicker than humans do. AI can become the determining factor of success or ruin for an entire country. Ukraine's AI-assisted military tactics have been life-saving, but the Dutch Tax Authority’s algorithms have caused suicide and poverty, and other tools may have more extreme effects. America may be in the lead now, but her security and prosperity will always be at risk while competition stays alert and ambitious. CLICK HERE TO GET THE OPINION NEWSLETTER On the worldwide stage, the United States has no room for mistakes with AI. We must educate our youth or prepare for complete economic instability. Americans must get over the fear of AI. Teachers must learn to change and not be afraid of losing their jobs. Students must understand AI and know how to maneuver through the dangers and the opportunities it offers. To build a better future, in the words of President Lyndon B. Johnson, we have to endure the painful while focusing on building a new solid educational foundation for growth and prosperity. CLICK HERE TO GET THE FOX NEWS APP"
20230725,foxnews,A translator for your kids: How using AI as a 'parenting co-pilot' will help parents communicate better,"Helping kids do their homework, breaking down complex topics for toddlers and telling captivating bedtime stories are daily duties for parents, but one father said using an AI assistant has helped him save time and better understand his children. ""AI is an extraordinarily powerful new set of capabilities that parents can leverage and should leverage,"" said Dmitry Shapiro, founder of YouAI, an artificial intelligence task agent tool. ""What we have access to now in the form of these AIs is a thing that we can converse with, that has all of the knowledge that has ever been written down about parenting, that it's digested and learned, and can be our personal co-pilot."" Artificial intelligence in recent months has become a powerful tool across industries, helping doctors identify diseases earlier and assisting fast-food restaurants sell more burgers. It's also been controversial in some spheres like education, with some seeing it as harmful, while others view it as a powerful tool for students and teachers. New York Public Schools, for example, restricted using ChatGPT in the classroom earlier this year, but later reversed that decision. WHY MOM AND DAD SHOULD LEVERAGE AI AS A PARENTING TOOL:   WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Shapiro's YouAI has multiple custom chatbots designed for different parenting tasks such as generating custom bedtime stories, helping kids with homework and explaining complicated subjects to them. &nbsp; ""You can think of it as like a translator where you can say things to it in adult language and it can sort of then paraphrase it so that a 5-year-old or a 9-year-old or whatever can understand,"" Shapiro, a father of five, told Fox News. Parents should use AI as a ""personal parenting assistant that we can have at all times with us for all situations and be able to, in real-time, get the information we need to be able to help our children, engage with them, calm down their flare-ups or sort of anything else,"" he added.&nbsp; WHAT IS ARTIFICIAL INTELLIGENCE (AI)?  The founder also designed an AI version of Mr. Rogers that his children can chat with to calm them down and diffuse tantrums.&nbsp; But there's opposition to children accessing AI too early or without supervision.&nbsp; Snapchat recently introduced a chatbot feature called My AI to its over 775 million users, many of which are children and teens. Doctors have warned that Snapchat users seeking mental health support from My AI could receive fabricated information.&nbsp; WHAT IS CHATGPT?  In May, Sen. Rick Scott introduced the Artificial Intelligence Shield for Kids (ASK) Act, which would forbid companies from offering AI to children without parental consent. ""I think we need to have parents involved if their child’s going to see anything with AI technology,"" the Florida Republican told Fox News at the time. The bill was recommended to the Senate Committee on Commerce, Science, and Transportation for further review. CLICK HERE TO GET THE FOX NEWS APP Still, Shapiro is adamant about AI's benefits for children. AI is the most ""profoundly transformative"" parenting tool since ""perhaps the invention of print,"" Shapiro told Fox News.&nbsp; To watch the full interview with Shapiro, click here."
20230725,cnn,ChatGPT creator pulls AI detection tool due to ‘low rate of accuracy’,"Less than six months after ChatGPT-creator OpenAI unveiled an AI detection tool with the potential to help teachers and other professionals detect AI generated work, the company has pulled the feature. OpenAI quietly shut down the tool last week citing a “low rate of accuracy,” according to an update to the original company blog post announcing the feature.  “We are working to incorporate feedback and are currently researching more effective provenance techniques for text,” the company wrote in the update. OpenAI said it is also committed to helping “users to understand if audio or visual content is AI-generated.” The news may renew concerns about whether the companies behind a new crop of generative AI tools are equipped to build safeguards. It also comes as educators prepare for the first full school year with tools like ChatGPT publicly available. The sudden rise of ChatGPT quickly raised alarms among some educators late last year over the possibility that it could make it easier than ever for students to cheat on written work. Public schools in New York City and Seattle banned students and teachers from using ChatGPT on the district’s networks and devices. Some educators moved with remarkable speed to rethink their assignments in response to ChatGPT, even as it remained unclear how widespread use of the tool was among students and how harmful it could really be to learning. Against that backdrop, OpenAI announced the AI detection tool in February to allow users to check if an essay was written by a human or AI. The feature, which worked on English AI-generated text, was powered by a machine learning system that takes an input and assigns it to several categories. After pasting a body of text such as a school essay into the new tool, it gave one of five possible outcomes, ranging from “likely generated by AI” to “very unlikely.” But even on its launch day, OpenAI admitted the tool was “imperfect” and results should be “taken with a grain of salt.”  “We really don’t recommend taking this tool in isolation because we know that it can be wrong and will be wrong at times – much like using AI for any kind of assessment purposes,” Lama Ahmad, policy research director at OpenAI, told CNN at the time. While the tool might provide another reference point, such as comparing past examples of a student’s work and writing style, Ahmad said “teachers need to be really careful in how they include it in academic dishonesty decisions.” Although OpenAI may be shelving its tool for now, there are some alternatives on the market.  Other companies such as Turnitin have also rolled out AI plagiarism detection tools that could help teachers identify when assignments are written by the tool. Meanwhile, Princeton student Edward Tuan introduced a similar AI detection feature, called ZeroGPT."
20230221,cbsnews,The 5 best robot vacuums for pet hair in 2024,"It's spring, which means it's time think seriously about cleaning. To get rid of those pesky pet hairs that your dog or cat has managed to shed, we recommend tapping the power of tech. Some of the latest robot vacuums are designed specifically to pick up dirt, dust and pet hair from carpets and hard flooring -- without you having to expend any effort. The experts at CBS Essentials have found the best robot vacuums that specialize in tackling the hair your pets shed all over your carpet and floors. These handy robots also excel at sucking up allergy-inducing dust, pet dander and outdoor dirt. We even found robot vacuums smart enough to dodge dog poop and other obstacles.Best for pet hair overall: iRobot Roomba j7+ robot vacuumSmartest robot vacuum for pet hair: Samsung Jet Bot+ robot vacuum with Clean StationBest budget robot vacuum for pet hair: iRobot Roomba 694 robot vacuumBest robot vacuum for pet hair under $125: Lefant M210 robot vacuumBest robot vacuum and mop combo for pet hair: Roborock S8 Pro Ultra 	 Best robot vacuums for pet hair in 2024  If you're looking for a robot vacuum to suck up pet hair, dander and dirt, then check out these feature-packed options.      Best for pet hair overall: iRobot Roomba j7+ robot vacuum   The iRobot Roomba j7+ is designed with dog owners in mind. This smart vacuum includes iRobot's P.O.O.P. (Pet Owner Official Promise) guarantee. Your Roomba j7+ is guaranteed to avoid pet waste, or iRobot will replace your vacuum for free. This robot features a powerful three-stage cleaning system with iRobot's most powerful suction. It even uses an edge-sweeping brush to get into corners. The Roomba j7+ also features dual multi-surface rubber brushes that flex to adjust to different floor types. Best of all, they don't get tangled in pet hair. When it's finished cleaning, the device automatically empties into a base for easy dirt disposal. Just empty the cleaning station every 60 days.        The robot vacuum actively avoids pet messes and obstacles such as water bowls. The vacuum's self-emptying capabilities means you don't need to constantly empty a dustbin, while the three-stage cleaning system provides a thorough clean of floors and carpets.  Smartest robot vacuum for pet hair: Samsung Jet Bot AI+ robot vacuum The Samsung Jet Bot AI+ robot vacuum uses sensors and artificial intelligence to map cleaning zones. It automatically distinguishes hard floors from carpets to use the proper suction, up to five watts. It comes complete with a cleaning station with five-layer HEPA filtration that traps 99.999% of fine dust. Pet owners will appreciate the self-cleaning extractors that break hairs into smaller pieces, so they don't get tangled around your robot vacuum's rollers. It offers targeted cleaning, allowing users to select which areas of their home the robot should focus on. Meanwhile, LiDAR sensors create accurate room maps for the Jet Bot AI+ to navigate, so there's no need to worry about your robotic vacuum going to places it shouldn't. The robot works in conjunction with the Samsung SmartThings app, just like all of the company's other smart products.One great feature is that the robot's front camera can share real-time video by streaming it to your mobile phone. The robot offers up to 90 minutes of cleaning time. Best budget robot vacuum for pet hair: iRobot Roomba 694The budget-minded Roomba 694 is Wi-Fi-enabled. Control the vacuum with your connected smartphone or tablet via the iRobot Home app. The Roomba 694 has a 90-minute run time before it automatically docks and recharges. And it offers personalized cleaning options, all selectable using the mobile app.The dual multi-surface brush is designed to clean a variety of floor types, which is great if you have carpet, hardwood and laminate. While it's doing its thing, the vacuum's auto-adjust cleaning head automatically adapts its height to clean carpets and hard floors. The robot is equipped with a full suite of advanced sensors to help it navigate under and around furniture and along edges. The cliff-detect feature keeps it from falling down stairs.The three-stage cleaning system and dual multi-surface brushes grab dirt from carpets and hard floors alike, while a separate edge-sweeping brush takes care of corners and edges. If you need 50% more suction power, be sure to check out the Roomba Combo i5+ ($489). With this more advanced model, the patented-dirt detecting technology allows the Roomba Combo i5+ robot to detect dirtier areas of your home and clean them more thoroughly when the vacuum bin is installed. It also handles wet floor mopping. Best robot vacuum under $125: Lefant M210 Lefant's M210 robot vac features built-in, anti-collision, infrared sensors, so it won't bang into its surroundings or knock over your pet's water bowl. Plus, this robot vacuum features strong suction power to easily pick up pet hair. It also detects ""stuck areas,"" and adjusts its cleaning path automatically. Download the Lefant app to pair the Wi-Fi-enabled vaccum with your smartphone or tablet to control the robot remotely.   Thanks to its 11-inch diameter and 2.99-inch height, this robot works easily under or around beds, sofas and other furniture to provide a more thorough cleaning. It's powered by a strong digital motor that offers impressive suction power, especially for the money. The unique brushless suction ensures that pet hair and dirt go into the dustbin easily. Unlike robot vacuums that rely on traditional roller brushes, this one does not get tangled in hair.The Lefant M210 is a more-than-affordable robot vacuum option for pet households.  Using the app, you simply set up a schedule and a type of cleaning, and the robot will handle the rest. Plus, this robot accepts voice commands using Amazon Alexa or Google Assistant.Best robot vacuum and mop combo for pet hair: Roborock S8 Pro UltraAs part of the Roborock Ultra series, this robot is ready to tackle dirty carpets as a vacuum and then instantly switch to being a high-powered mop for hard floors. With its all-in-one docking system, the S8 Pro Ultra can automatically complete the mop washing and drying process, preventing mold growth and unpleasant odors. The dock even cleans itself and refills the robot water tank for you, extending its mopping range to up to 3,230 square feet.Meanwhile, the automatic, self-emptying dust bag holds up to seven weeks' worth of dust. The redesigned dual rubber brush system cleans with more effectiveness than ever, and promises fewer hair-tangling disasters. The more powerful 6000Pa suction drives the robot vacuum to work faster to debris, hair, dust and more from hard floors and carpets. The robot's battery charges from empty to full in just four hours and then provides up to 180 minutes of continuous cleaning time. With 3D structured light and infrared image technology, the Roborock S8 Pro Ultra is smart enough to identify objects, determine their size and location, and then bypass them in both bright and dark environments. Thanks to LiDAR navigation and 3D mapping, detailed maps are generated for more accurate and customized cleaning. Meanwhile, the Roborock App offers you total control over the robot. Amazon Alexa, Google Home and Siri Shortcuts support let you command your robot with the power of your voice, too.What pet owners should look for in a robot vacuumPet owners should never settle on a subpar robot vacuum. Entry-level robot vacuums usually lack features and the suction power needed by households with pets. Seek out a robot vac with high-efficiency filters that trap dog and cat allergens. And if your pet is prone to accidents, consider a vacuum with object detection that can avoid pet messes. You should also keep an eye out for brush rollers with anti-tangling features, especially if your pet is a heavy shedder. You'll also want a robot vacuum with a cleaning base, if you can afford the upgrade. Unlike older robot vacuums that need to be emptied once every couple of days, a robot vacuum with a cleaning base can go for weeks before being emptied. "
20230221,cbsnews,Vanderbilt University apologizes for using ChatGPT to write letter on MSU shooting,"Vanderbilt University is drawing heat from its student body for using ChatGPT to generate a communitywide letter addressing the recent mass shooting at Michigan State University. The office of Equity, Diversity and Inclusion (EDI) at Vanderbilt's Peabody College of Education last week issued a statement that many have criticized as impersonal and lacking empathy. ""The recent Michigan shootings are a tragic reminder of the importance of taking care of each other, particularly in the context of creating inclusive environments,"" the letter's opening line read. ""As members of the Peabody campus community, we must reflect on the impact of such an event and take steps to ensure that we are doing our best to create a safe and inclusive environment for all.""A paragraph further down began with a sentence that struck community members as generic.""Another important aspect of creating an inclusive environment is to promote a culture of respect and understanding,"" the letter stated. ""This means valuing the diversity of experiences, perspectives, and identities on our campus, and actively working to create a space where everyone feels welcomed and supported.""The message continued to tout the merits of creating ""a safe and inclusive environment on campus."" In small font, just above the signature line, a disclaimer appeared, indicating that the entire statement was a ""paraphrase from OpenAI's ChatGPT AI language model.""""Sick and twisted irony""Students and community members blasted the university for the misstep, accusing administrators of orchestrating a public relations stunt. Vanderbilt senior Laith Kayat, whose sister attends MSU, called the use of ChatGPT ""disgusting,"" Vanderbilt's student newspaper, the Vanderbilt Hustler, reported. ""There is a sick and twisted irony to making a computer write your message about community and togetherness because you can't be bothered to reflect on it yourself,"" Kayat told The Hustler.                          ""[Administrators] only care about perception and their institutional politics of saving face.""Kayat called on administrators to do better than rely on a robot to lead the university.Administrators never reviewed letterOn Friday, a day after the initial letter was issued, Nicole M. Joseph, associate dean for EDI, sent a follow-up email saying her office had made an error in judgement by using ChatGPT to reflect on the MSU shooting.""As with all new technologies that affect higher education, this moment gives us all an opportunity to reflect on what we know and what we still must learn about AI,"" Joseph's follow-up letter read, according to the Vanderbilt Hustler. Peabody College Dean Camilla Benbow said the controversial missive was never reviewed by her office before it was distributed. ""The development and distribution of the initial email did not follow Peabody's normal processes providing for multiple layers of review before being sent,"" she said in a statement to CBS MoneyWatch. The dean's office is conducting an investigation into the incident, according to Benbow. In the meantime, Associate Dean Joseph and Assistant Dean Hasina Mohyuddin will be on temporary leave, she said. ""I am also deeply troubled that a communication from my administration so missed the crucial need for personal connection and empathy during a time of tragedy,"" she added in her statement. Poor judgmentThere are scenarios in which ChatGPT can be highly effective at drafting communications. Some chief executives have come to rely on the tool to write speeches and act as a ""thought partner."" Debates are underway about the kinds of jobs the technology will eventually eliminate. But for now, the software lacks a core feature that makes humans unique: judgment. ChatGPT can summarize data and generate text, but doesn't possess the emotional intelligence of humans, according to Columbia Business School professor Oded Netzer. Understanding the ""why"" behind facts and figures and expressing genuine emotion are still ""the types of tasks that require judgment and that humans are still very valuable in,"" he said."
20230221,foxnews,Microsoft imposes limits on Bing chatbot after multiple incidents of inappropriate behavior,"Chatbots are quickly becoming the way of the future, yet they still have issues.&nbsp; Microsoft is the latest tech company with problems with its new Bing search engine, which uses the same technology as the viral OpenAI chatbot ChatGPT. &nbsp; The technology is meant to answer people as a human would, though now Microsoft is putting caps on its capabilities.&nbsp; CLICK TO GET KURT’S CYBERGUY NEWSLETTER WITH QUICK TIPS, TECH REVIEWS, SECURITY ALERTS AND EASY HOW-TO’S TO MAKE YOU SMARTER&nbsp; What is Microsoft Bing?  Microsoft Bing is a web search engine that is owned and operated by Microsoft (pretty much their own version of Google). It works just like any other search engine, where you can type in questions and get answers, including articles, images, videos, shopping, maps and more.&nbsp; Now, Microsoft has introduced a new Chat option where you can ask Bing a question, and it will give a more exact, typed-out answer rather than feeding you multiple articles for you to read on the topic.&nbsp; For example, if you're looking to make a three-course meal with no nuts or seafood, you can simply type, ""I need to throw a dinner party for six people who don't eat nuts or seafood. Can you suggest a three-course menu?"" and the search engine will give you a list of options you can make with suggestions for appetizers, main courses, and dessert.&nbsp; SNEAKY LEGIT WAY TO SCORE FREE VIRTUAL TECH SUPPORT  Can anyone use Microsoft Bing?  Anyone can use Microsoft Bing if they join what Microsoft calls ""the new Bing."" You can request access by going to Bing.com and selecting ""Join the waitlist."" &nbsp; When you have cleared the waitlist, you will receive an email letting you know that you can now access the new Bing at Bing.com. Once you have access, you can start typing in your usual search box, and Bing will give you detailed answers.&nbsp;  CONGRESS BLOCKS FUNDING REQUEST FOR MICROSOFT HEADSETS AFTER TESTING CONCERNS: REPORT What issues has the new Bing been having?  It has been reported that the new Bing has been having some malfunctions since its initial release. Many new users got excited and wanted to see how long they could converse back and forth with the search engine, and these longer conversations began to overwhelm it.&nbsp; Some people posted screenshots of their conversations to social media, showing how the new Bing was convinced that the year was 2022 and not 2023 and would gaslight users by saying things like ""Please don't doubt me"" and ""I'm Bing, I know the date.""&nbsp; Other people have found the chatbot's answers amusing. However, since Microsoft is investing around $10 billion in this new way of communication, the company is now setting limits to make sure that it actually works as it is supposed to.&nbsp;  WINDOWS GOTCHAS: HOW TO AVOID THE TOP 5 MOST COMMON MISTAKES What kind of limits is Microsoft implementing to access the new Bing?  Microsoft noticed that the new Bing would only act inappropriately when the conversations with its users were carried on for too long. Because of this, the tech company is implementing limits on how many questions you can ask.&nbsp; The new Bing can now answer five questions per session and 50 questions in a day. This means that you can ask it 5 questions on the same topic before you have to switch topics.&nbsp;&nbsp; The company says that the chatbot is still very much a work in progress and that current users are helping them to improve the technology so that it can be more reliable in the future.&nbsp; For some insight into AI, I recently interviewed ChatGPT as if it were a human; here's what the AI had to say that gave me the chills.&nbsp; Have you tried the new ChatGPT or Microsoft Bing yet? We want to hear about your experience.&nbsp; CLICK HERE TO GET THE FOX NEWS APP For more of my tips, subscribe to my free CyberGuy Report Newsletter by clicking the ""Free newsletter"" link at the top of my website.&nbsp; Copyright 2023 CyberGuy.com. All rights reserved. CyberGuy.com articles and content may contain affiliate links that earn a commission when purchases are made.&nbsp;"
20240522,foxnews,South Korea urges global cooperation for AI development at Seoul summit,"South Korea's science and information technology minister said on Wednesday the world must cooperate to ensure the successful development of AI, as a global summit on the rapidly evolving technology hosted by his country wrapped up. The AI summit in Seoul, which is being co-hosted with Britain, discussed concerns such as job security, copyright and inequality on Wednesday, after 16 tech companies signed a voluntary agreement to develop AI safely a day earlier. A separate pledge was signed on Wednesday by 14 companies including Alphabet's Google, Microsoft, OpenAI and six Korean companies to use methods such as watermarking to help identify AI-generated content, as well as ensure job creation and help for socially vulnerable groups. FOX NEWS AI NEWSLETTER: HOW ARTIFICIAL INTELLIGENCE IS RESHAPING MODERN WARFARE ""Cooperation is not an option, it is a necessity,"" Lee Jong-Ho, South Korea's Minister of Science and ICT (information and communication technologies), said in an interview with Reuters.  ""The Seoul summit has further shaped AI safety talks and added discussions about innovation and inclusivity,"" Lee said, adding he expects discussions at the next summit to include more collaboration on AI safety institutes. The first global AI summit was held in Britain in November, and the next in-person gathering is due to take place in France, likely in 2025. Ministers and officials from multiple countries discussed on Wednesday cooperation between state-backed AI safety institutes to help regulate the technology. CLICK HERE TO GET THE FOX NEWS APP AI experts welcomed the steps made so far to start regulating the technology, though some said rules needed to be enforced. ""We need to move past voluntary... the people affected should be setting the rules via governments,"" said Francine Bennett, Director at the AI-focused Ada Lovelace Institute. AI services should be proven to meet obligatory safety standards before hitting the market, so companies equate safety with profit and stave off any potential public backlash from unexpected harm, said Max Tegmark, President of Future of Life Institute, an organisation vocal about AI systems' risks. South Korean science minister Lee said that laws tended to lag behind the speed of advancement in technologies like AI. ""But for safe use by the public, there needs to be flexible laws and regulations in place."""
20230622,foxnews,Missing Titanic submarine: Canadian underwater robot searches ocean floor as oxygen levels dwindle,"The U.S. Coast Guard announced Thursday that the Canadian vessel Horizon Arctic deployed a remotely operated vehicle (ROV) ""that has reached the sea floor and began its search"" for the missing OceanGate Titan submarine.&nbsp; It is the first time during the search that a vessel is combing the floor of the Atlantic Ocean for the missing vessel and its five passengers.&nbsp; Previous search efforts have involved the use of aircraft and sonar.&nbsp; ""The French vessel L'Atalante has just deployed their ROV,"" the Coast Guard also said.&nbsp; LIVE UPDATES: SEARCH FOR MISSING OCEANGATE TITAN SUBMARINE&nbsp;   The Titan submarine vanished Sunday morning with five people on board: OceanGate CEO Stockton Rush, French mariner Paul-Henry Nargeolet, British businessman and explorer Hamish Harding, Pakistani businessman Shahzada Dawood and his son Sulaiman Dawood.&nbsp; The oxygen inside the Titan is estimated by the Coast Guard to run out some time Thursday morning.&nbsp; In a statement issued Wednesday night, the Coast Guard said the sub ""was launched at 8 a.m. EDT [Sunday] and expected to resurface at 3 p.m., but one hour and 45 minutes into their dive, they lost contact with the Polar Prince.""&nbsp; On OceanGate's website, it lists the Titan sub as having 96 hours of life support for a crew of five passengers. &nbsp; The Coast Guard also said Wednesday night that ""[u]nderwater sounds have been detected in the search area, resulting in the redirection of remotely operated vehicle (ROV) operations to explore the origin. &nbsp; ""These recordings have been shared with the U.S. Navy for analysis to help guide future search efforts,"" the Coast Guard added.&nbsp; DEEP-SEA EXPERT WORRIES ‘BANGING’ COULD BE ’OVERLY OPTIMISTIC’ AS TITANIC SUB MAY HAVE ALREADY RUN OUT OF AIR&nbsp;  The source of these sounds has not been confirmed.&nbsp; The Coast Guard said an area twice the size of Connecticut has been searched so far, 900 nautical miles east of Cape Cod, Massachusetts.&nbsp; The cause of the sub’s disappearance remains unknown and more ships have been arriving throughout the week at the search site.&nbsp; As of Wednesday afternoon, the Coast Guard was categorizing the international response as a search and rescue mission.&nbsp;   CLICK HERE TO GET THE FOX NEWS APP&nbsp; ""We are smack dab in the middle of search and rescue, and we'll continue to put every available asset that we have in an effort to find the Titan and the crew members,"" U.S. Coast Guard Capt. Jamie Frederick told reporters.&nbsp; ""I don't know whether it's operable or whether it's sitting on the ocean floor or whether it's in the sea column or whether it's in the surface. You know, it's all speculation,"" Frederick later said. ""And we're just not in the business of speculation. We're in the business of searching, and we're putting everything we can with the data.""&nbsp;"
20230622,nbcnews,‘It’s horrifying’: Discord CEO on child abuse issues after NBC News investigation ,"Discord CEO Jason Citron said Thursday that he found reports of  child exploitation on the popular chat platform ""horrifying"" and that Discord took the issue ""very seriously."" His comments, at Bloomberg’s Tech Summit in San Francisco, came the day after NBC News published an investigation into the issue. “As a parent, it’s horrifying."" Citron said in response to questions from Bloomberg journalist Emily Chang. ""We take this stuff very seriously.” Citron noted that Discord employs a dedicated child safety team that is tasked with trying to prevent exploitation on the platform “in a way that respects the privacy of all the people who are not doing these things.” The investigation revealed that since the platform's creation in 2015, at least 35 child abduction, grooming, or exploitation prosecutions involved communications via Discord, and 165 child sexual abuse material prosecutions involved the platform. Additionally, NBC News identified hundreds of active Discord servers promoting child exploitation. “What we see is only the tip of the iceberg,” said Stephen Sauer, the director of the tipline at the Canadian Centre for Child Protection.  Like many social media companies, Discord scans uploaded images and videos and compares them to a known database of child sexual abuse material, but it leaves most other moderation to communities themselves. Discord has clearly stated that it is not proactively scanning most messages that are posted in its communities. Citron said artificial intelligence could help solve some issues around child exploitation. ""One of the challenges I think that all of the folks in our industry have is that we have so many things happening at scale on the platform and it's so hard to sort of identify things,"" he said. John Redgrave, vice president of trust and safety at Discord, told NBC News that the company was working with THORN, a company devoted to building technology solutions to detect and prevent child exploitation, on a model that could detect grooming behavior. AI has also been criticized, however, for potentially aggravating child safety issues.  This month, the FBI warned that adults were using AI to generate manipulated images of children for the purpose of sextortion, blackmailing minors with the images for even more sexual content or money."
20230309,foxnews,Are you ready for AI voice cloning on your phone?,"Experts at Samsung are currently working to have the software assistant called Bixby clone a user's voice when answering calls. Artificial intelligence is making big waves in the world of tech, and this is just another big step in that direction. However, voice cloning is certainly causing some concern when it comes to privacy and consent as well. What is voice cloning? Voice cloning is the creation of an artificial simulation of a person's voice using artificial intelligence technology. When this concept first came about, a person would need to produce a&nbsp;large amount of recorded speech to clone their voice. CLICK TO GET KURT’S CYBERGUY NEWSLETTER WITH QUICK TIPS, TECH REVIEWS, SECURITY ALERTS AND EASY HOW-TO’S TO MAKE YOU SMARTER However, since the software is developing at such a rapid pace, you can now generate a clone of a voice with just a few minutes of recorded speech.  Samsung’s Bixby upgrade is allowing English speakers to answer calls by typing a message. Once that message is typed, Bixby can convert it to audio and communicate it to&nbsp;the caller directly on their behalf. There is also a feature known as the Bixby Custom Voice Creator, which lets you record different sentences for Bixby to analyze and create an AI-generated copy of your voice and tone. HOW HACKERS ARE USING CHAPTGPT TO CREATE MALWARE TO TARGET YOU What are the pros of voice cloning? There are many pros of cloning a person's voice. First, there's accessibility where voice cloning can assist people who have lost their ability to speak due to illness or injury. Voice cloning can also be used to create personalized digital assistants, chatbots and other virtual entities. In addition, it can be used to personalize customer experiences by creating a unique and recognizable voice for a brand. There's also the cost savings aspect of voice cloning. It&nbsp;can significantly reduce the cost of creating voiceovers for videos and other media. Instead of hiring a professional voice actor, companies can use voice cloning technology to create a synthetic voice that sounds just like a human voice. Finally, voice cloning can save time by automating certain tasks that would normally require human intervention, like customer service chatbots that can be programmed to respond to common queries using a cloned voice.  What are the cons of voice cloning? First, it can be considered a serious violation of privacy. People can use voice cloning as a way of stealing someone's identity. Since you only need a few minutes of recorded speech to do so, a scammer can easily steal someone's voice and use it for whatever means they wish. And because the concept is so new, there isn't much out there to stop them.  IS A FOLDING IPHONE ON THE WAY? APPLE JUST GRANTED NEW PATENT Voice cloning has also been used as a way to promote hateful rhetoric. Back in February, one person decided to take President Biden's voice and use it in a&nbsp;video to make it look like he was attacking transgender people. It was quickly determined that the video was fake, however, it still got thousands of views on social media. Another con is that it&nbsp;has the potential to replace human voiceover artists and customer service representatives, leading to the loss of jobs in these industries. MOVE OVER SIRI – APPLE’S NEW AUDIO BOOK AI VOICE SOUNDS LIKE A HUMAN Scammers are also using voice cloning to make them sound more legit when calling more vulnerable people. They convince these people to transfer large sums of money into their accounts, and there is no way to track them down afterward because the voice they used was not even their own. How do you feel about AI voice cloning taking over? Let us know your thoughts. CLICK HERE TO GET THE FOX NEWS APP For more of my tips, subscribe to my free CyberGuy Report Newsletter by clicking the ""Free newsletter"" link at the top of this story. Copyright 2023 CyberGuy.com.&nbsp;All rights reserved."
20221018,cbsnews,Holiday 2022 robot vacuum buying guide: The best robot vacuum to gift this year is $200 off at Amazon now,"Robot vacuums are great gifts for busy friends or family members this Christmas and Hanukkah. Everyone will enjoy the gift of a clean home around the holidays without the hassle of lugging around a vacuum.     Not sure where to begin when buying a robot vacuum? Find the best robot vacuums for the 2022 holiday season with this gift guide, including a $200 off deal on our most popular model.      Top products in this article        iRobot Roomba j7+ robot vacuum, $600 (reduced from $800) Samsung Jet Bot+ robot vacuum with clean station, $700 (reduced from $800)      iRobot Roomba j7+ self-emptying robot vacuum with Braava Jet M6 robot mop, $854 (reduced from $1,250)        Most people don't particularly enjoy vacuuming their carpets, so why not give a gift that will ease that burden and give your loved ones more time to relax this winter? Robot vacuums save time and do the work themselves. The recipient can even schedule the robot vacuum to cleanup while they're at work so they get to come home to freshly cleaned floors everyday. Now that's really a gift that'll keep on giving.      Does the person that your shopping for already have a robot vacuum? Consider gifting them a robot mop to make their at-home floor care even easier. Robot mops like the iRobot Braava come in handy during the winter when people and pets tend to track in more mud and snow.        Keep reading to find the perfect robot vacuum (or mop) to give as a gift this holiday season. Plus, we've found some deals to help you save big this holiday season. You can even use the savings to treat yourself to a new robot vacuum to make the pre- and post-holiday cleanup a breeze.         	 	 	 	 	 	 	 	 	 	The best robot vacuums to give as holiday gifts         All featured robot vacuum cleaners below boast Amazon user-review ratings of four stars (out of five) or higher. Note that more expensive robot vacuums tend to have the most advanced features, such as obstacle detection and avoidance, laser guidance and intelligent mapping. Some are even on sale now.  	 	iRobot Roomba j7+ robot vacuum     The iRobot Roomba j7+ is designed with the issue of dog poop in mind. This smart vacuum includes iRobot's P.O.O.P. guarantee, or ""Pet Owner Official Promise."" Your Roomba j7+ is guaranteed to avoid pet waste or iRobot will replace your vacuum for free.            The vacuum features a powerful three-stage cleaning system with iRobot's most powerful suction. The home-cleaning device uses an edge-sweeping brush to get into corners. The Roomba j7+ features dual multi-surface rubber brushes that flex to adjust to different floor types. Best of all, they don't get tangled with pet hair.          When it's done cleaning, the device automatically empties into its included clean base for easy dirt disposal with enclosed bags. Just empty the cleaning station once every 60 days.          iRobot Roomba j7+ robot vacuum, $600 (reduced from $800)           The iRobot Roomba j7 is a bit more affordable and also offers the P.O.O.P. promise. Right now, you can get it for $250 off on Amazon. (A cleaning station is not included.)     iRobot Roomba j7 robot vacuum, $350 (reduced from $600)      	iRobot Roomba i3+ EVO robot vacuum with automatic dirt disposalThe iRobot Roomba i3+ EVO uses ""Imprint Smart Mapping"" technology to map your home. Use your connected phone to direct the Wi-Fi-enabled robot vacuum to clean any room you want. You can even schedule a future clean. This Roomba vacuum is compatible with Amazon Alexa and Google Assistant.                The smart appliance learns your cleaning habits, and can suggest extra cleanings during peak pollen and pet-shedding seasons. And don't even worry about dumping out your dustbin. The Roomba i3+ EVO features iRobot's ""Clean Base Automatic Dirt Disposal"" system, and empties your accumulated dirt into an enclosed bag.           iRobot Roomba i3+ EVO robot vacuum with automatic dirt disposal, $398 (reduced from $550)           	 	 	 	 	 	 	 	 	 	Samsung Jet Bot+ robot vacuum with Clean Station         While this more affordable Jet Bot+ robot vacuum by Samsung doesn't feature 3D recognition with AI, it does have LiDAR sensor navigation, five watts of adjustable suction and the all-important self-emptying Clean Station. Mapping can be controlled via your phone with the Samsung SmartThings App. Remotely check the Jet Bot+'s cleaning status, pause or stop cleaning and view the cleaning history.           Samsung Jet Bot+ robot vacuum with clean station, $700 (reduced from $800)           	 	 	 	 	 	 	 	 	 	iRobot Roomba 694 robot vacuum         The Roomba 694 is Wi-Fi-enabled. Control the vac with your connected smartphone or device via the iRobot Home app. The Roomba 694 has a 90-minute run time before it automatically docks and recharges. It's an especially fitting gift for friends or family members with pets.         On Amazon, one reviewer praised the iRobot device's ability to keep a pet-friendly household clean. ""We have two dogs, one that sheds moderately,"" the customer wrote. ""I purchased in hopes that it at least would help between regular vacuuming. I vacuumed first with my Dyson then set it free. When it was done with the job, I didn't expect much in the dust trap... I was wrong! It was full! Super impressed.""           iRobot Roomba 694 robot vacuum, $179 (reduced from $274)           	 	 	 	 	 	 	 	 	 	Shark Ion robot vacuum         This Shark robot vac features side brushes, channel brushes and a multi-surface brush roll to handle dirt and debris on all surfaces. Use the SharkClean app on your connected smartphone or device to control when -- and where -- your robot vacuum cleans. The vac offers 120 minutes of run time. Choose from three colors.          Shark Ion robot vacuum (gray), $208 (reduced from $230)           	 	 	 	 	 	 	 	 	 	Lefant M210 robot vacuum cleaner         Lefant's M210 robot vac features built-in, anti-collision infrared sensors (so it won't bang into its surroundings). The robot vacuum detects ""stuck areas,"" and adjusts its cleaning path automatically. Download the Lefant app to pair the Wi-Fi-enabled vac with your smartphone or device -- the better to control the appliance remotely. The robot vacuum features 100 minutes of run time.          Lefant M210 robot vacuum cleaner, $100 (reduced from $260)           	 	 	 	 	 	 	 	 	 	Laresar Grande 1 self-charging robotic vacuum         The 4.4-star-rated Laresar smart robot can vacuum and mop your floors (water tank sold separately). The machine is equipped with sensors that detect stairs and prevent falls. The robot vacuum is Wi-Fi compatible and can be controlled by smartphone. Download the Laresmart app to schedule cleanings, swap cleaning modes and control cleaning direction.          Apply the 10% off coupon before checkout at Amazon to save even more on this budget robot vacuum.          Laresar Grande 1 self-charging robotic vacuum, $153 after coupon (reduced from $249)           	 	 	 	 	 	 	 	 	 	Jet Bot AI+ robot vacuum with object recognition The Samsung Jet Bot AI+ robot vacuum has a bunch of cool features, including 30 watts of adjustable suction, 3D object recognition with AI and powerful LiDAR navigation. This robot vacuum can recognize what objects to avoid, so you won't have to deal with it constantly crashing into the couch or a pile of laundry on the floor. Have a very specific clean in mind? Mapping can be controlled via your phone.          You can even watch your robot vacuum operate no matter where you are, using Samsung's SmartThings App. The Jet Bot AI+ comes with a front camera that can live stream in real time. It boasts its own no-touch ""Clean Station"" that will empty your dustbin using Samsung's Air Pulse technology. The vacuum's 0.2-liter dustbin is fully washable.          Jet Bot AI+ robot vacuum with object recognition, $1,100 (reduced from $1,299)            	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	Yeedi Vac 2 robot vacuum and mop                  Want to give them a robot vacuum and a mop? Check out the budget-minded Yeedi Vac 2, available on Amazon.       It's a great gift for household with pets. This multi-purpose cleaning device features 3D object avoidance which allows it to recognize any objects in its path and move around them to avoid getting stuck. It's designed to avoid your dog's food bowl, water bowl and toys.               Yeedi Vac 2 robot vacuum and mop, $245 (reduced from $350)                Right now you can get the upgraded Yeedi Vac 2 Pro version with a longer run time for $54 more.               Yeedi Vac 2 Pro robot vacuum and mop, $299 after coupon (reduced from $450)                   	 	 	 	 	 	 	 	 	 	iRobot Roomba j7+ and Braava Jet M6 robot mop bundle         On Amazon, you can buy a 4.4-star-rated combo that pairs the iRobot Roomba 7+ with the Braava Jet M6 robot mop.           The iRobot Roomba 7+ uses an edge-sweeping brush to get into corners. It features dual, multi-surface rubber brushes that flex to adjust to different floor types -- and help prevent them from getting tangled with pet hair. Billed as a self-cleaning vac, the Roomba 7+ automatically empties itself into enclosed bags.          The Braava Jet M6 robot mop cleaner, also by iRobot, delivers a jet spray that can help you tackle messes on finished hard floors of stone, tile, wood and more.        iRobot Roomba j7+ self-emptying robot vacuum with Braava Jet M6 robot mop, $854 (reduced from $1,250)           	 	 	 	 	 	 	 	 	 	iRobot Roomba i4+This Roomba robot vacuum cleaner features a high-efficiency filter that captures 99% of cat and dog allergens. It empties itself into its base after every cleaning. Just dump out the dirt every 60 days, or as needed.           iRobot Roomba i4+, $516 (reduced from $650)           	 	 	 	 	 	 	 	 	 	Roborock Q5+ with self-empty dock         This 4.6-star-rated robot vacuum by Roborock comes with serious smarts: It uses LiDAR navigation to create an editable map of your home, so it knows not to bump into furniture or tumble down steps. It supports app- and voice-based controls. Plus, its 2.5-liter filtered dust bag promises seven weeks of hands-free cleaning before it requires replacing.          Be sure to apply the coupon at Amazon before checkout to save an extra $100 on the price.          Roborock Q5+ robot vacuum with self-empty dock, $600 after coupon (reduced from $700)           	Shark EZ Wi-Fi robot vacuum with XL self-empty base Target has one of the best deals you can get right now on a robot vacuum with a self-empty base. The 4.4-star-rated vacuum cleans in orderly, neat rows. The Shark EZ robot vacuum supports voice commands and includes a cleaning base that holds 45 days worth of dirt and debris.  Shark EZ Wi-Fi robot vacuum with self-empty base, $300 (reduced from $580)  	 	 	 	 	 	Best robot mops to give as a gift in 2022      If someone you know already has a robot vacuum that doesn't mop, giving them a robot mop can be thoughtful. It is more convenient to be able to control both devices from the same app, so you might want to stick with the same brand they already have.       	 	 	 	 	 	Samsung Jetbot mop      Need a robot cleaning solution for non-carpeted areas? The Samsung Jetbot Mop robot mop is good for cleaning tile, vinyl, laminate and hardwoods. Plus, you can use it in hand-held mode to clean bathroom walls, countertops and more.           Samsung Jetbot mop, $299  	 	 	 	 	 	 	 	iRobot Braava Jet M6 robot mop         The Braava Jet M6 robot mop cleaner delivers a jet spray that can help you tackle holiday messes on finished hard floors of stone, tile, wood and more. The mop learns the layout of your home and builds a customized smart map to clean in neat rows without missing a spot. The mop can also be used to target small areas for spills or other messes.       The iRobot Braava Jet M6 mop is compatible with Amazon Alexa and the iRobot mobile app.       iRobot Braava Jet M6 robot mop, $299 (reduced from $450)           	 	 	 	 	 	 	iRobot Braava Jet 240 robot mop       The iRobot Baava Jet 240 is the most affordable robot mop from iRobot's popular Braava line. The budget robot mop provides a deep clean with its precision jet spray and vibrating cleaning head. There are three cleaning modes: wet mopping, damp mopping and dry sweeping. The Braava Jet 240 selects the correct cleaning mode based on the Braava jet pad type.       iRobot Braava Jet 240 robot mop, $180 (reduced from $199)        	 	 	 	 	 	 	Related content from CBS Essentials        The Ultimate Black Friday 2022 GuideBest robot vacuum for avoiding dog poop (plus more pet-friendly vacuums)Amazon Toys We Love List: The most popular and best toys of the 2022 holiday season, according to Amazon2022 Holiday Gift Guide: The best stocking stuffers to give this Christmas2022 Holiday Gift Guide: The best deals under $100 to shop this Christmas and HanukkahA Walmart Plus membership will save you money on gas, Paramount Plus and more. Is it worth it?The best robot mops in 2022Essentials Fall 100: The iRobot Roomba i7+ is our best selling robot vacuum"
20221018,foxnews,Oakland police researching using robots to deploy lethal shotgun rounds in emergency situations: report,"Police in Oakland, California, are reportedly exploring the option of using robots equipped with a device that could be deployed to fire live shotgun rounds against suspects in emergency circumstances. ""Yes, we are looking into that and doing more research at this time,"" Oakland Police Lt. Omar Daza-Quiroz told The Intercept in a report detailing the debate between Oakland police and a city oversight council on whether to use deadly force via robots during certain emergency circumstances. The report described a Sept. 21 Oakland Police Commission subcommittee meeting that included a discussion on accessories that police robots use, such as a ""percussion actuated non-electric disruptor,"" known as a PAN disruptor, that sometimes deploys a&nbsp;blank shotgun shell or pressurized water while the device operator stands at a safe distance. Daza-Quiroz told the committee that officers make sure a blank round is in the device when Jennifer Tu, a fellow with the American Friends Service Committee and Oakland Police Commission subcommittee member, asked if ""a live round"" can ""physically go in"" and ""what happens if a live round goes in?"" PHILADELPHIA POLICE ATTACKED BY ATV, DIRT-BIKE RIDERS WHO HURLED BRICKS AND BOTTLES AT OFFICERS  ""Yeah, physically, a live round can go in,"" Daza-Quiroz said. ""Absolutely, and you’d be getting a shotgun round."" Daza-Quiroz was then asked by Commissioner Jesse Hsieh if the police department plans to use live rounds in the device. ""No,"" Daza-Quiroz said before adding that there are potential scenarios where that could be a necessary move. CA MAN WHO PLANNED ON BLOWING UP DEMOCRATIC HQ MUST UNDERGO PSYCH EXAM ""I mean, is it possible we have an active shooter in a place we can’t get to? And he’s fortified inside a house?"" Daza-Quiroz said. ""Or we’re trying to get to a person."" The civilian commission expressed skepticism over the idea, which police say has never been used since they introduced robots in 2011. ""It's a lot easier to pull out a rifle or a gun and shoot someone than to put a live round into this thing,"" Hsieh said.&nbsp;""But I think we are all concerned about the dystopian sort of universe where a robot sneaks into our room and shoots us, which I know is not the intention but is certainly a scary thought and where our mind goes."" SERIAL KILLER HOAX? SEATTLE POLICE DENY RUMORS KILLER IS ON THE LOOSE TARGETING WOMEN IN THEIR 30S  Daza-Quiroz told the commission that the department did not want to include language prohibiting a robot’s use of deadly force ""because what if we need it for some situation later on?"" The plan to include language allowing police to use deadly force via robots has reportedly been put on hold, but police are exploring the idea. The Intercept reported that the two sides agreed that the robots will only be allowed to deploy pepper spray for the time being. 5TH SAN ANTONIO POLICE OFFICER DIES FROM SUICIDE IN LAST 7 MONTHS, EXPERTS WEIGH IN: 'STOP THE DEMONIZATION'  ""We will not be arming robots with lethal rounds anytime soon, and if and when that time comes, each event will be assessed prior to such deployment,"" Daza-Quiroz said. The Oakland Police Department did not immediately respond to a request for comment from Fox News Digital. Police in Dallas, Texas, used an explosive device attached to a robot to kill a suspect in a coordinated sniper attack that killed five officers in 2016, which is believed to be the first time law enforcement used a robot to kill a suspect. CLICK HERE TO GET THE FOX NEWS APP ""We cornered one suspect, and we tried to negotiate for several hours. Negotiations broke down, we had an exchange of gunfire with the suspect,"" Dallas Police Chief David Brown said at the time. ""We saw no other option but to use our bomb robot and place a device on its extension for it to detonate where the suspect was."""
20240421,cbsnews,Mercy Hospital surgeon teaches high school students world of robotic surgery,"SACRAMENTO -- A unique experience for high school students, taking a step into the world of robotic surgery. Mercy Hospital hosted students to learn the ins and outs of the future of surgical medicine. Inside an operating room and under the guidance of Mercy Hospital surgeon Tyler Robinson, the students suited up to learn the meticulous process of scrubbing in.""We're getting more hands-on,"" said Davis senior high school student Taylor Rogers. And a chance to operate one of the hospital's surgical robots.""I thought it was really cool how people can operate without having to be there. I thought that was super interesting and very innovative,"" said Kathlin Whitehead, a senior at Davis High School.The event is part of a 10-week program in partnership with the high school to expose students to real-life experiences, and ultimately inspire and strengthen the next generation of healthcare workers.""It's given them the chance to see all the different healthcare opportunities. Specifically, the robot, because it is the future and our students are also the future,"" said Jennifer Johnston, the advanced robotics coordinator at Mercy General Hospital. The use of robotics in surgery has become much more common around the world over the past 10 years.Robinson said the technology helps doctors perform surgery more efficiently, it's less invasive, more precise and makes recovery much quicker.""We can use intuitive surgical robots, we can perform really complicated surgeries through a small little incision,"" Robinson said. For a generation of youth growing up relying on technology, Robinson said they're primed for this kind of innovation.""I do think they're going to be especially well-tuned to some of the technology changing all of our worlds, surgery included,"" Robinson said.This is the first high school class to be invited to this kind of demonstration but they say it definitely won't be their last."
20240421,cnn,The Mona Lisa rapping? New Microsoft AI animates faces from photos,"The Mona Lisa can now do more than smile, thanks to new artificial intelligence technology from Microsoft. Last week, Microsoft researchers detailed a new AI model they’ve developed that can take a still image of a face and an audio clip of someone speaking and automatically create a realistic looking video of that person speaking. The videos — which can be made from photorealistic faces, as well as cartoons or artwork — are complete with compelling lip syncing and natural face and head movements. In one demo video, researchers showed how they animated the Mona Lisa to recite a comedic rap by actor Anne Hathaway. Outputs from the AI model, called VASA-1, are both entertaining and a bit jarring in their realness. Microsoft said the technology could be used for education or “improving accessibility for individuals with communication challenges,” or potentially to create virtual companions for humans. But it’s also easy to see how the tool could be abused and used to impersonate real people. It’s a concern that goes beyond Microsoft: as more tools to create convincing AI-generated images, videos and audio emerge, experts worry that their misuse could lead to new forms of misinformation. Some also worry the technology could further disrupt creative industries from film to advertising. For now, Microsoft said it doesn’t plan to release the VASA-1 model to the public immediately. The move is similar to how Microsoft partner OpenAI is handling concerns around its AI-generated video tool, Sora: OpenAI teased Sora in February, but has so far only made it available to some professional users and cybersecurity professors for testing purposes. “We are opposed to any behavior to create misleading or harmful contents of real persons,” Microsoft researchers said in a blog post. But, they added, the company has “no plans to release” the product publicly “until we are certain that the technology will be used responsibly and in accordance with proper regulations.” Making faces move Microsoft’s new AI model was trained on numerous videos of people’s faces while speaking, and it’s designed to recognize natural face and head movements, including “lip motion, (non-lip) expression, eye gaze and blinking, among others,” researchers said. The result is a more lifelike video when VASA-1 animates a still photo. For example, in one demo video set to a clip of someone sounding agitated, apparently while playing video games, the face speaking has furrowed brows and pursed lips. The AI tool can also be directed to produce a video where the subject is looking in a certain direction or expressing a specific emotion. When looking closely, there are still signs that the videos are machine-generated, such as infrequent blinking and exaggerated eyebrow movements. But Microsoft said it believes its model “significantly outperforms” other, similar tools and “paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.”"
20231222,cbsnews,"At Dallas airport, artificial intelligence is helping reunite travelers with their lost items","Dallas — Mikha Sabu and a team of specialists patrol the busy terminals at Dallas-Fort Worth International Airport, picking up precious cargo left behind by passengers and bringing it back to the lost and found.  ""Once we find that item for them, they will be so happy,"" Sabu, who works in the airport's lost and found department, told CBS News. In a typical year at DFW, more than 18,000 items are reported lost by travelers. But with the help of artificial intelligence, about 90% of the lost items found are returned to their owners, the airport said. How does it work? A person needs to first report the item missing, and then include details about the missing object. The AI software then tries to match the item details with pictures and descriptions of things that were found. Once it's a confirmed match, the item is then shipped to the owner.The Lost and Found software, which is operated by Hallmark Aviation Services, is also used at 11 other airports. Shimaa Fadul, who runs daily operations at the DFW lost and found, explains that by looking for distinguishing marks, like stickers or serial numbers, AI can help find any item. So far this year, Fadul's team has found nearly 600 rings and more than 400 watches, including seven Rolexes that were all left behind by their owners.However, Fadul says one of the most valuable items that her team found was a wedding dress that was returned just 24 hours before the bride's big day.""And you cannot imagine that she doesn't have anything to wear on her wedding day,"" Fadul said, adding that the airport overnighted the dress to her, and it made it in time for the wedding.  It marks a ""real"" problem that is being solved with ""artificial"" intelligence."
20240223,cbsnews,"Legendary shipwreck's treasure of ""incalculable value"" will be recovered by underwater robot, Colombia says","Colombia's government on Friday announced an expedition to remove items of ""incalculable value"" from the wreck of the legendary San Jose galleon, which sank in 1708 while laden with gold, silver and emeralds estimated to be worth billions of dollars. The 316-year-old wreck, often called the ""holy grail"" of shipwrecks, has been controversial, because it is both an archaeological and economic treasure.Culture Minister Juan David Correa told AFP that more than eight years after the discovery of the wreck off Colombia's coast, an underwater robot would be sent to recover some of its bounty.Between April and May, the robot would extract some items from ""the surface of the galleon"" to see ""how they materialize when they come out (of the water) and to understand what we can do"" to recover the rest of the treasures, said Correa.The operation will cost more than $4.5 million and the robot will work at a depth of 600 meters to remove items such as ceramics, pieces of wood and shells ""without modifying or damaging the wreck,"" Correa told AFP aboard a large naval ship.The location of the expedition is being kept secret to protect what is considered one of the greatest archaeological finds in history from malicious treasure hunters.The San Jose galleon was owned by the Spanish crown when it was sunk by the British navy near Cartagena in 1708. Only a handful of its 600-strong crew survived.""It makes it very touchy because one is not supposed to intervene in war graves,"" Justin Leidwanger, an archaeologist at Stanford University who studies ancient shipwrecks, told Live Science. The ship had been heading back from the New World to the court of King Philip V of Spain, laden with treasures such as chests of emeralds and some 200 tons of gold coins.Before Colombia announced the discovery in 2015, it was long sought after by treasure hunters.""As if we were in colonial times""The discovery of the galleon sparked a tug-of-war over who gets custody of its bounty.Spain insists that the bounty is theirs since it was aboard a Spanish ship, while Bolivia's Qhara Qhara nation says it should get the treasures as the Spanish forced the community's people to mine the precious metals.The government of leftist president Gustavo Petro, in power since 2022, wants to use the country's own resources to recover the wreck and ensure it remains in Colombia.The idea is ""to stop considering that we are dealing with a treasure that we have to fight for as if we were in colonial times, with the pirates who disputed these territories,"" Correa, the culture minster, said.Spain's ambassador to Colombia Joaquin de Aristegui  said he has instructions to offer Colombia a ""bilateral agreement"" on the protection of the wreck.Bolivia's Indigenous people have expressed their willingness to work with Petro's government and have now asked for the return of only a few pieces from the ship.""Not only for the symbolic issue but more for the spiritual issue,"" native leader Samuel Flores told AFP. ""We just want our ancestors to be at peace.""The expedition to start recovering the shipwreck's trove comes as a case is underway at the UN's Permanent Court of Arbitration between Colombia and the U.S.-based salvage company Sea Search Armada -- which claims it found the wreck first over 40 years ago.The company is demanding $10 billion dollars, half the wreck's estimated value today.In June 2022, Colombia said that a remotely operated vehicle reached 900 meters below the surface of the ocean, showing new images of the wreckage.The video showed the best-yet view of the treasure that was aboard the San Jose — including gold ingots and coins, cannons made in Seville in 1655 and an intact Chinese dinner service.At the time, Reuters reported the remotely operated vehicle also discovered two other shipwrecks in the area, including a schooner thought to be from about two centuries ago."
20240223,nbcnews,Google says Gemini AI glitches were product of effort to address 'traps',"Google apologized Friday for a series of public mishaps by its artificial intelligence tool Gemini, which was denounced by some users this week after it generated historically inaccurate images such as nonwhite Nazi soldiers.  The company said in a blog post that it was still working on a fix for the app and was continuing to temporarily block the creation of new images of people until a solution is in place.  “It’s clear that this feature missed the mark,” Prabhakar Raghavan, a senior vice president at Google, wrote in the blog post.  “Some of the images generated are inaccurate or even offensive. We’re grateful for users’ feedback and are sorry the feature didn’t work well,” he wrote.  Gemini is primarily a conversational AI app competing with OpenAI’s ChatGPT to explore the possibilities of generative AI, including using text prompts to create images. Google, under pressure from investors and others, had worked on similar ideas internally for years and released its app, initially called Bard, after ChatGPT unexpectedly took off in popularity starting in late 2022.  But images from Gemini became the subject of mockery on social media after people posted examples of ahistorical images. They included illustrations of World War II German soldiers who were Black or Asian, despite the racist ideology of the Nazi military and government. The app also created images of nonwhite American Founding Fathers, when in reality they were all white men.  The criticism came especially from conservative figures who accused Google of embracing political correctness. Tech figures including Elon Musk, who has a competing AI chatbot as part of his app X, have singled out individual Google employees for criticism.  Google acknowledged Wednesday that Gemini was offering inaccuracies and a day later it paused image generations that include people.  Google said Friday the intent had been to avoid falling into “some of the traps we’ve seen in the past with image generation technology — such as creating violent or sexually explicit images.” It also said that Gemini is targeted to a worldwide audience, so the diversity of people depicted is important.  But prompts for a specific type of person or people in a particular historical context “should absolutely get a response that accurately reflects what you ask for,” Google said.  “Over time, the model became way more cautious than we intended and refused to answer certain prompts entirely — wrongly interpreting some very anodyne prompts as sensitive,” the company said.  Google did not give a timeline for turning back on the ability to generate images of people, and it said the process of building a fix “will include extensive testing.” "
20240223,foxnews,"Sen. Tom Cotton torches Google AI system as 'racist, preposterously woke, Hamas-sympathizing'","Sen. Tom Cotton, R-Ark., slammed Google's AI chatbot Gemini as ""preposterously woke""&nbsp;on Friday for its refusal to produce any images of White people.&nbsp; The company paused the chatbot's image generation on Thursday after social media users pointed out that the system was creating inaccurate historical images that sometimes replaced White people, like the Founding Fathers, with images of Black, Native American and Asian people. ""Google deserves condemnation for creating a racist, preposterously woke, Hamas-sympathizing AI system,"" Cotton said in a statement on X, formerly Twitter. ""Republican lawmakers will remember this the next time Google comes asking for antitrust help."" Cotton pointed out that ""the problem also lies at the White House,"" which pushed an executive order last year that bolsters ""AI safety and security, protects Americans’ privacy, advances equity and civil rights,"" according to an October Biden-Harris fact sheet. ""This debacle is a good reminder of why federal control over AI would be a disaster,"" Cotton continued. ""It would force every AI system to be as broken and as dishonest as Google's."" GOOGLE TO PAUSE GEMINI IMAGE GENERATION AFTER AI REFUSES TO SHOW IMAGES OF WHITE PEOPLE  On Wednesday, Google apologized for the errors.&nbsp; ""We're aware that Gemini is offering inaccuracies in some historical image generation depictions,"" Google said on Wednesday. Gemini, formerly known as Google Bard, is one of many multimodal large language models (LLMs) currently available to the public. As is the case with all LLMs, the human-like responses offered by these AIs can change from user to user. Based on contextual information, the language and tone of the prompter, and training data used to create the AI responses, each answer can be different, even if the question is the same. GOOGLE APOLOGIZES AFTER NEW GEMINI AI REFUSES TO SHOW PICTURES, ACHIEVEMENTS OF WHITE PEOPLE  In a statement to Fox News Digital, Gemini Experiences Senior Director of Product Management Jack Krawczyk addressed the responses from the AI that had led social media users to voice concern. ""We're working to improve these kinds of depictions immediately,"" Krawczyk said. ""Gemini's AI image generation does generate a wide range of people. And that's generally a good thing, because people around the world use it. But it's missing the mark here."" Prior to Krawczyk's tenure with Google, he allegedly tweeted that ""white privilege is f—king real"" and America is rampant with ""egregious racism,"" according to posts circulating on X that appear to be his.&nbsp; ""White privilege is f—king real,"" Krawczyk allegedly wrote in a tweet on April 13, 2018, according to screenshots on X. ""Don’t be an a—hole and act guilty about it – do your part in recognizing bias at all levels of egregious."" One alleged post, which Elon Musk also shared a picture of, referenced President Joe Biden and Vice President Kamala Harris.&nbsp; ""I've been crying in intermittent bursts for the past 24 hours since casting my ballot. Filling in that Biden/Harris line felt cathartic,"" the Google director allegedly wrote.&nbsp; FOX Business' Nikolas Lanum, Chris Pandolfo and Reuters contributed to this report.&nbsp;"
20231231,nbcnews,The 5 issues and trends experts expect states to tackle in 2024,"2024 will be a monumental presidential election year. But when it comes to policy, it will be state governments that see the most action over the next 12 months. When state legislatures kick off their fresh sessions in the coming weeks — 37 will go into session in January and another nine will follow in February — lawmakers will immediately dive into a host of big policy issues. Some of those areas — like how to tackle artificial intelligence and deepfakes — will be relatively new. For others, like how state governments can best deal with major workforce shortages, legislators will be picking up where they left off last year. Meanwhile, in areas like abortion rights, it will be organizers attempting to place measures on the November ballot, not lawmakers, who are taking the lead. “2024 will be an incredibly important year as we think about the progress that can be made at the state level,” said Jessie Ulibarri, co-executive director of the State Innovation Exchange, a group of state legislators that works to advance traditionally progressive policies. Here are the top five issues and trends experts expect to see emerge at the state level in 2024. Abortion rights Abortion rights has been a political boon for Democrats since the U.S. Supreme Court overturned Roe v. Wade, but those successes have only occasionally taken the form of state legislation. Rather, Democrats have seen their biggest victories in advancing abortion rights over the last two years take the form of state ballot measures — and more are on the horizon in 2024. Abortion rights supporters have already made major progress in at least 10 states to put the issue on the ballot next year. Groups are collecting signatures to let voters decide on ballot initiatives in Florida, Arizona, Nevada, Montana, Nebraska, South Dakota and Missouri. In Maryland, New York and Colorado lawmakers — who control the amendment process in those states — have already succeeded in putting measures on the 2024 ballot that would enshrine abortion rights in those states’ constitutions. “2023 was the first year since the Dobbs decision, and what we saw across the board, regardless of political context, was that the people of America are ready, willing and able to organize to advance reproductive freedom and access to abortion care in red and blue states alike,” said Ulibarri. “And that will remain a consistent effort in this next year, when there will be many more states considering ballot measures.” Abortion rights advocates are also warning that 2024 will see efforts by conservative lawmakers, attorneys and judges in states including Ohio, Kansas and Michigan to block implementation of the passed initiatives by proposing new anti-abortion bills and threatening lawsuits. AI and deepfakes Advancements in artificial intelligence and deepfake technology have grown exponentially in just the past year. State legislatures haven’t kept up. That lag has been especially clear as it pertains to bills that seek to tackle political deepfakes, leaving potential threats unchecked heading into a presidential election year. In 2023, just three states enacted laws attempting to address AI’s effects on political campaigns. But the few pieces of legislation that are in place — some focus on disclosure, others on prohibition — are likely to serve as models for other states going forward. While most states haven’t yet released details of pre-filed bills for upcoming state legislative sessions, state politics observers predict many will attempt to address the issue next year. “This is clearly a significant problem,” said Daniel Weiner, who as director of the elections and government program at the nonpartisan Brennan Center is closely following the challenges presented by AI and deepfakes. “Start incrementally, do what you can, see how it works,” Weiner said, describing how he thinks state governments should approach legislation. Tim Storey, the CEO of the National Conference of State Legislatures, added that the issues “are going to get a lot of attention and energy” in 2024. “It’s going to be a major theme in 2024 sessions,” he said. States are also likely to begin more seriously looking to regulate other areas of AI and deepfakes, he explained. “It’s happening so fast with AI, states know there’s going to have to be some regulatory guardrails around the integration of AI — both in terms of people’s personal lives and their commercial lives,” Storey said. Workforce shortages In critical fields such as education, medicine, health care and criminal justice, states in recent years have endured a jarring shortage of workers. To find, attract and retain essential workers like teachers, nurses and corrections officers, states have tried to lean hard into legislation that incentivizes — or at least eases obstacles for — people looking to enter or stay in those fields. State bills over the last several years have focused heavily on student debt forgiveness and pay increases. With baby boomers continuing to retire, and the effects of a wave of pandemic-motivated workforce departures still robust, states — blue, red and purple — are expected to keep their foot on the gas in the space in their 2024 legislative sessions. “This is one of those issues that is impacting every single state,” Storey said. “The workforce issue will continue to come into play,” he added, predicting that bills in upcoming sessions could focus on changing requirements for credentialing, licensure and in some cases degrees for certain in-demand professions. In 2023, lawmakers in some states also attempted to address worker shortages by loosening child labor laws. While that trend could continue in some states, some experts predict that at least a handful of states will instead move to shore up child labor protections in 2024 to make sure that their legislatures don’t deal with shortages by allowing children into the workforce. “Unfortunately, we’ve seen this year some states attempt to roll back child labor laws, and other states are going to take that up next year,” said Ulibarri. “And many states will actually be looking to enshrine deeper protections to protect kids from being put into the workforce too early.” Immigration In border states, governors and lawmakers from both parties have increasingly taken matters into their own hands amid a historic number of migrants attempting to cross into the U.S. In just the last few weeks, both Democrats and Republicans have taken huge — and sometimes controversial — legislative steps to tackle the issue. Texas Gov. Greg Abbott, a Republican, recently signed legislation allowing police to arrest migrants who cross the border illegally. Days earlier, Arizona Gov. Katie Hobbs, a Democrat, requested from President Joe Biden more than $500 million to reimburse the state for border security expenses, as well as the reassignment of National Guard troops who could help reopen a key border crossing in the state. Immigration mostly falls under the purview of the federal government. But if the ongoing inaction from the White House and Congress continues much further into 2024, border states will continue to try to tackle it, experts predicted. “You do have a lot of states where they’re done waiting on Washington to address these issues, and looking at it individually, which is difficult,” Storey said. “But this is one of those issues that a state-by-state solution is more complicated.” Meanwhile, the decision by Texas and other red states to continue busing recently arrived migrants to blue cities like Chicago, New York and Denver has resulted in states far from the border trying to deal with the issue as well. “It’s not just California, Arizona, Texas anymore. It really is an issue that that everyone appreciates is at their doorstep,” Storey said. Growing tensions Not many state political observers saw the historic spike in expulsions, impeachment threats and punitive bills enacted by the party in power targeting members of the opposition coming in 2023. But they do in 2024. “I think we will continue to see significant tensions in legislative bodies until we attend to the conditions of governments,” Ulibarri said. In Tennessee, Republican legislators expelled two Black Democrats from the state House in unprecedented votes earlier this year, drawing national attention and accusations of racism. In Montana, Republicans in the state House voted to bar Democratic state Rep. Zooey Zephyr, the state’s first transgender lawmaker, from participating in debates on the chamber floor. And in Oregon, state officials — implementing a newly approved constitutional amendment designed to punish legislators from reelection if they miss 10 or more floor sessions — moved to ban 10 Republican lawmakers from running for re-election after they participated in a six-week walkout in protest over guns, abortion and other issues. With the bitterness that comes with a presidential election year all but certain to continue fanning those flames on the state level, there’s little hope of the trend fading, experts said. In Wisconsin, where Republicans have threatened to impeach a liberal state Supreme Court justice who won her April election by 11 percentage points, as well as the top elections official in the state, GOP Assembly Speaker Robin Vos refused to rule out taking action against either person in the upcoming session. “We are seeing an era of partisan legislation,” Storey said, referring not only to policy, but to punitive measures as well. “I think we’re going to be in that mode for some time longer.” CLARIFICATION (Jan. 2, 2024, 6:21 p.m. ET): This article has been updated to reflect that the State Innovation Exchange consists of state legislators."
20230919,foxnews,Pedophiles on dark web turning to AI program to generate sexual abuse content,"An internet watchdog is sounding the alarm over the growing trend of sex offenders collaborating online to use open source artificial intelligence to generate child sexual abuse material. ""There’s a technical community within the offender space, particularly dark web forums, where they are discussing this technology,"" Dan Sexton, the chief technology officer at the Internet Watch Foundation (IWF), told The Guardian in a report last week. ""They are sharing imagery, they’re sharing [AI] models. They’re sharing guides and tips."" Sexton's organization has found that offenders are increasingly turning to open source AI models to create illegal child sexual abuse material (CSAM) and distribute it online. Unlike closed AI models such as OpenAI’s Dall-E or Google’s Imagen, open source AI technology can be downloaded and adjusted by users, according to the report. Sexton said the ability to use such technology has spread among offenders, who take to the dark web to create and distribute realistic images. NEW AI OFFERS 'PERSONAL PROTECTION' AGAINST ABDUCTIONS, CRIMINAL THREATS  ""The content that we’ve seen, we believe is actually being generated using open source software, which has been downloaded and run locally on people’s computers and then modified. And that is a much harder problem to fix,"" Sexton said. ""It’s been taught what child sexual abuse material is, and it’s been taught how to create it."" Sexton said the online discussions that take place on the dark web include images of celebrity children and publicly available images of children. In some cases, images of child abuse victims are used to create brand-new content. ""All of these ideas are concerns, and we have seen discussions about them,"" Sexton said.  WHAT IS ARTIFICIAL INTELLIGENCE (AI)? Christopher Alexander, the chief analytics officer of Pioneer Development Group, told Fox News Digital one of the new dangers of this technology is that it could be used to introduce more people to CSAM. On the other hand, AI could be used to help scan the web for missing people, even using ""age progressions and other factors that could help locate trafficked children."" ""So, generative AI is a problem, AI and machine learning is a tool to combat it, even just by doing detection,"" Alexander said. ""The extreme dangers created by this technology will have massive implications on the well-being of the internet. Where these companies fail, Congress must aggressively step up to the plate and act to protect both children and the internet as a whole."" Meanwhile, Jonathan D. Askonas, an assistant professor of politics and a fellow at the Center for the Study of Statesmanship at the Catholic University of America, told Fox News Digital that ""lawmakers need to act now to bolster laws against the production, distribution, and possession of AI-based CSAM, and to close loopholes from the previous era."" CLICK HERE FOR MORE US NEWS IWF, which searches the web for CSAM and helps to coordinate its removal, could find itself overwhelmed by tips to remove such content from the web in the era of AI, Sexton said, noting that the proliferation of such material was already widespread across the web. ""Child sexual abuse online is already, as we believe, a public health epidemic,"" Sexton said, according to The Guardian. ""So, this is not going to make the problem any better. It’s only going to potentially make it worse."" Ziven Havens, the policy director at the Bull Moose Project, told Fox News Digital that it will be up to Congress to act in order to protect both children and the internet.  CLICK HERE TO GET THE FOX NEWS APP ""By using already available images of real abuse victims, AI CSAM varies very little from that of non-AI-created CSAM. It is equally morally corrupt and disgusting. The extreme dangers created by this technology will have massive implications on the well-being of the internet,"" Havens said. ""Where these companies fail, Congress must aggressively step up to the plate and act to protect both children and the internet as a whole."""
20230919,foxnews,Abortion chatbot Charley helps women end their pregnancies: 'Let's get started',"For those women who are considering terminating their pregnancies, a new chatbot called Charley aims to help them start the process of getting an abortion. The chatbot, which launched on Sept. 12, is available on Charley’s website, greeting visitors with the message, ""Need an abortion? Let’s get started."" On its website, Charley is described as ""designed by abortion experts, made for abortion seekers."" PREGNANT WOMAN WITH BRAIN CANCER REFUSES ABORTION: ‘KILLING MY BABY WOULDN’T HAVE SAVED ME’ One of its co-founders is Cecile Richards, former president of Planned Parenthood. Richards ""oversees legal, political, and policy matters and leads fundraising efforts"" for Charley, according to the chatbot’s website. Another co-founder is Tom Subak, former chief strategy officer at Planned Parenthood.  Charley isn’t an app — it lives online, on its own website.&nbsp; While individuals can freely visit the site, the company is also seeking medical providers who will agree to embed the chatbot directly on their own websites, ""to meet abortion seekers wherever they are online,"" said Nicole Cushman, Charley’s New York-based content manager, in an interview with Fox News Digital. Cushman, who has held leadership positions at Planned Parenthood, said the idea for the chatbot came about after Roe v. Wade was overturned — with the goal of ""improving people’s online search experience."" MARCH FOR LIFE 2023 REFLECTS RENEWED EFFORT TO WIPE OUT ABORTION STATE BY STATE: FAITH LEADERS WEIGH IN ""Our research showed that people were turning primarily to Google for information about abortion options in the post-Roe landscape, and that it was very challenging for abortion seekers to connect to available options,"" she said. People ""were ending up in an endless Google loop."" ""This was particularly the case if they were living in a state with an abortion ban or restriction — they were ending up in an endless Google loop.""  Charley’s creators envisioned a ""simple, effective way to pull together information from a range of sources"" and ""cut through the confusion,"" Cushman told Fox News Digital.&nbsp; How Charley works Unlike large language models like ChatGPT, Charley doesn’t allow people to type questions. Instead, the chatbot uses a ""decision tree"" format that guides visitors through a series of pre-written prompts, including the desired type of abortion and the date of their last menstrual period.&nbsp; It also asks for a zip code to determine the specific abortion laws in the visitor’s state of residence. 'PRO-LIFE GENERATION IS ALIVE AND WELL' AS FURIOUS FIGHT FOR THE UNBORN CONTINUES For example, when Fox News Digital entered a zip code in Ohio, the response was: ""Currently, abortion care is legal in Ohio, but only up to 22 weeks. This means that, if you act quickly, you‘ll be able to get abortion care in your state. If you need more time or can’t get an appointment before then, you may still have options in another state."" For abortion seekers under 18 years of age, Charley notifies them whether state law requires a parent’s permission to get an abortion — and also offers assistance for minors to ask a judge for permission to get the procedure on their own.  At the end of the series of questions, the chatbot provides a summary of expected costs, alternate funding options and a directory of resources to find an abortion provider. ""Those resources might include a link to a directory to locate the nearest clinic, a link to telehealth providers — or help lines for legal, medical, financial or emotional support,"" Cushman told Fox News Digital. ""Our research showed that people were turning primarily to Google for information about abortion options."" The pre-scripted information provided by the chatbot was developed by a team of ""medical and legal experts,"" she added. Potential risks of the abortion chatbot Charley is designed as a ""triage solution"" to provide information and education so that people can make ""an informed decision"" about their next steps, Cushman said. ""For some people, the next best step may be to make an appointment to see a provider in person, or to call a hotline for more direct support,"" she noted. ""There’s no harm in chatting with Charley, but it’s not the end of their journey,"" she also said.  At some points during the chat, Charley may quickly hand off the visitor to an external resource — for example, if she is experiencing a medical emergency or potential pregnancy complications, Cushman noted. She also said, however, that not all pregnant women require in-person care before seeking an abortion. ABORTION SURVIVORS, IN WAKE OF SUPREME COURT RULING, REVEAL THEIR 'TRAUMA' BUT REJOICE IN A 'NEW DAWN' ""Plenty of research shows that telehealth is a safe and effective way to access medication abortion,"" Cushman told Fox News Digital.&nbsp; ""If there are no extenuating circumstances — especially if someone is earlier in the pregnancy — they can navigate through Charley to access additional resources or other hotlines."" Security and privacy has been an area of ""heightened concern"" among people searching for abortion care online, Cushman said.  The chatbot does not ask for any identifying information, she pointed out — just the person's zip code and date of her last menstrual period. ""We put fear of surveillance and criminalization front and center when designing Charley,"" she noted. ""We don’t use any tracking tools, cookies or pixels, and we don’t share information with any third party."" All conversations are deleted from their system ""regularly,"" Cushman said. Experts stress face-to-face discussion Dr. Kecia Gaither, a double board-certified OB/GYN and director of maternal fetal medicine at NYC Health + Hospitals/Lincoln in the Bronx, described Charley as a ""brilliant tool to assist women in locales where reproductive options are either restricted or prohibited."" ""Given the reality that almost half of the U.S. has banned or restricted reproductive options, Charley will likely serve as a lifeline to many women,"" she told Fox News Digital. ""Compassionate and comprehensive care is essential, especially during something so personal as an abortion."" Gaither said there are a ""multitude of reasons why reproductive options are needed,"" pointing to scenarios like ""congenital fetal anomalies"" or issues where giving birth could ""compromise the mother’s health or even kill her."" The doctor did add, however, that face-to-face discussion with a health provider is always recommended as the first avenue for any woman seeking reproductive options.  Dr. Laura Purdy, a board-certified family medicine physician in Miami, Florida, said she values in-person interaction to ensure that women who are considering abortion are aware of the emotional implications of their decision — which can range from anxiety to grief. ""Chatbots are a great way to offer advice, and I can understand their appeal,"" she told Fox News Digital.&nbsp; ""However, the health of women is a very personal matter that demands a lot of attention."" ""I would recommend thoroughly researching the side effects that an abortion can have on a woman’s body, and see a doctor after that decision to ensure that your mental state is being cared for."" ""I think compassionate and more comprehensive care is essential, especially during something so personal as an abortion."" Purdy, who practices telemedicine herself, said that she ""very much values technology to improve women’s health care."" But with telehealth, she said, ""you are still talking to a real human, who provides empathy and individual care."" CLICK HERE TO SIGN UP FOR OUR HEALTH NEWSLETTER ""Ultimately, it is a preference, but I would recommend thoroughly researching the side effects that an abortion can have on a woman’s body, and see a doctor after that decision to ensure that your mental state is being cared for,"" she recommended. Dr. Marc Siegel, clinical professor of medicine at NYU Langone Medical Center and a Fox News medical contributor, said that while he is a ""big believer"" in AI applications in health care, ""I don't think this one works.""  To test the bot, Siegel entered information as if someone in an early stage of pregnancy, he told Fox News Digital. ""It is front-loaded with basic information regarding where and how, and [offers] options, but is less interactive than I was expecting,"" he said. ""It is also kind of clunky, which is especially problematic in an area where sensitivity and empathy are required."" CLICK HERE TO GET THE FOX NEWS APP The doctor also noted that the chatbot has an orientation in the direction of ""subtly moving people toward abortions by not providing an alternative focus of valuing the life that could come from this."" Dr. Siegel added, ""Imagine if the bot started out by asking, ‘Do you know how many women wish they were in your position? Do you know how many have tried to become pregnant but can't? Here are those statistics.’"" For more Health articles, visit www.foxnews/health"
20240201,foxnews,Voters face ‘significant threat’ from wave of AI-generated fraud as experts race to stop election interference,"After a robocall targeted New Hampshire residents with a fraudulent phone call from President Biden, experts are warning that voters may be inundated with content generated by artificial intelligence (AI) with the potential to interfere with the 2024 primary and presidential elections. While threat actors are using AI to overcome existing security measures and make attacks bigger, faster, and more covert, researchers are now leveraging AI tools to create new defensive capabilities. But Optiv Vice President of Cyber Risk, Strategy and Board Relations James Turgal told Fox News Digital, make no mistake, generative AI poses a ""significant threat."" ""I believe the greatest impact could be AI's capacity to disrupt the security of party election offices, volunteers and state election systems,"" he said. Turgal, a former FBI veteran, noted that a threat actor's goals can include changing vote totals, undermining confidence in electoral outcomes or inciting violence. Even worse, they can now do so on a massive scale. SUPREME COURT CHIEF JUSTICE REPORT URGES CAUTION ON USE OF AI AHEAD OF CONTENTIOUS ELECTION YEAR  ""In the end, the threat posed by AI to the American election system is no different than the use of malware and ransomware deployed by nation-states and organized crime groups against our personal and corporate networks on a daily basis,"" he said. ""The battle to mitigate these threats can and should be fought by both the United States government and the private sector."" To mitigate the threat, Turgal suggested that election offices should have policies to defend against social engineering attacks and staff must participate in deep fake video training that informs them on attack vectors, such as email text and social media platforms, in-person and telephone-based attempts. He also stressed that private sector companies that create AI tools, including large language chatbots, have a responsibility to ensure that chatbots provide accurate information on elections. To do this, companies must confirm their AI models are trained to state their limitations to users and redirect them to authoritative sources, such as official election websites. When asked by Fox News Digital whether voters may see a larger quantity of AI-generated voices that could potentially sway their decisions, NASA Jet Propulsion Laboratory (JPL) Chief Technology and Innovation Officer Chris Mattmann said, ""The cat's out of the bag."" The spoof AI call of Biden is currently under investigation by the New Hampshire attorney general's office and is of unknown origin. Experts said that because programs that can replicate voices are widely available as applications and online services, it is nearly impossible to determine which program created it. WATCHDOG WARNS SEVERAL FEDERAL AGENCIES ARE BEHIND ON AI REQUIREMENTS  The voice, which is a digital manipulation of Biden, told New Hampshire voters that casting their ballot on Tuesday, January 23, would only help Republicans on their ""quest"" to elect Trump once again. The voice also claimed their vote would make a difference in November but not during the primary. While federal laws prosecute knowing attempts to limit people's ability to vote or sway their voting decisions, regulation on deceptively using AI is still yet to be implemented. When these audio clips reach such speed and authenticity and are added to AI-generated video trained on millions of hours of clips found in the public lexicon, Mattmann said the predicament gets even worse. ""It's at that point when they're literally indiscernible, even from computer programs, that we have a big problem when we start not being able to do some of these steps, like attribution and detection. That's the moment that you hear everyone worrying about, including myself,"" he said. Years ago, spoof calls and spam calls were typically generated based on a combination of statistical methods and audio dubbing and clipping. Since public figures, such as Biden, are on record with a voice saying many words known to the public, the use of software and careful attention to things like voice tone and background can produce audio that is difficult to differentiate from authentic recordings. Even on a weak computer, these AI voice models can fully replicate a human voice in around two to eight hours. However, Microsoft's new text-to-speech algorithm, VALL-E, has shaved voice cloning down to a fraction of the time. WATCHDOG WARNS SEVERAL FEDERAL AGENCIES ARE BEHIND ON AI REQUIREMENTS  ""This can take three seconds of your voice and it can clone it. And when I say clone it, I mean clone it in the sense that words that Joe Biden has never said, it can actually make him say,"" Mattmann said. The rapid acceleration of voice cloning software is largely due to data collected by virtual voice assistants. Products like Amazon Alexa, Google Assistant, Apple's Siri and more have been capturing snippets of people's voices for years, all of which are used to train the AI and make them better at replicating speech. &nbsp; In some cases, tech giants have even kept consumers' voice recordings without their informed consent, which is mandated by law. In May, Amazon was ordered to pay $25 million to settle a lawsuit after regulators said the company violated privacy laws when they kept children's voice recordings ""forever."" ""Basically, they've been listening to our data for years, whether we clicked yes or not,"" Mattmann said. AI-manipulated content is not exclusive to threat actors. Over the last year, several political groups and politicians have used the tech to target their opponents. Florida Gov. Ron DeSantis was the first United States presidential hopeful to use the technology in a political attack advertisement. In June, DeSantis' now-disbanded campaign posted AI images of former President Donald Trump and Dr. Anthony Fauci hugging. Twitter, responding to concerns that the images may manipulate voters, soon added a ""context"" bubble for readers. ""It was through the looking glass. It was the Alice in Wonderland moment. It's like, gosh, you know, they could immediately the same day respond with this,"" Mattmann said. TOP REPUBLICAN TALKS AI ARMS RACE: 'YOU'LL HAVE MACHINES COMPETING WITH EACH OTHER'  In December, another political ad using AI was unveiled, this time by the House Republicans' campaign arm. The advertisement featured AI-generated pictures depicting migrant camps across some of the most famous monuments and national parks throughout the United States. Mattmann, an experienced AI expert, said the federal government has asked the Federal Election Commission and other organizations to experiment with labels that can be added to AI-generated campaign products to inform voters about how they were created. However, this rule has yet to be adopted. CLICK HERE TO GET THE FOX NEWS APP While there are methodologies and computer programs to discern if content in the political space is AI-generated, he said many research groups do not have access to them and there are few, if any, campaigns that understand the technical aspect. ""In political campaigns and things like that too, this is going to take us into a realm audio-wise and just beyond that, audio, text, multimodal video, in which they're going to have tools in not this election cycle but the next one that is ready for this. But the challenge is now,"" Mattmann added.&nbsp; Get the latest updates from the 2024 campaign trail, exclusive interviews and more at our Fox News Digital election hub."
20240201,cnn,FCC seeks to make AI-generated robocalls illegal,"The Federal Communications Commission is seeking to make AI-generated robocalls illegal. The agency’s announcement comes after a recent robocall with an AI voice resembling President Joe Biden targeted thousands of New Hampshire voters and as US officials brace for artificial intelligence to make it easier to spread disinformation in the 2024 election. The FCC proposes making AI-generated calls illegal under the Telephone Consumer Protection Act (TCPA), saying it would make “voice cloning technology used in common robocalls scams targeting consumers illegal.” “The rise of these types of calls has escalated during the last few years as this technology now has the potential to confuse consumers with misinformation by imitating the voices of celebrities, political candidates, and close family members,” the FCC said in a news release Wednesday. “By taking this step, the FCC will provide new tools to State Attorneys General across the country to go after bad actors behind these nefarious robocalls and hold them accountable under the law.” The TCPA, enacted in 1991, regulates telemarketing calls and robocalls to help limit junk calls. It has been used in anti-robocall crackdowns, including a case against conservative activists Jacob Wohl and Jack Burkman for carrying out a voter suppression campaign during the 2020 election. The campaign by Wohl and Burkman prompted the FCC to fine them $5million, a record-breaking figure at the time. CNN reported earlier this week that House Democrats are fighting back against AI-generated robocalls with a sweeping proposal to overhaul the nation’s robocall rules. The number of robocalls placed in the US peaked at around 58.5 billion in 2019, according to estimates by YouMail, a robocall blocking service. Last year, the figure was closer to 55 billion. The fake robocall that emerged last week imitating Biden and telling voters not to vote in the New Hampshire primary was for some policymakers the opening salvo in an election season poised to be plagued by disinformation. This story and its headline have been updated. CNN’s Sean Lyngaas contributed to this report."
20231215,cbsnews,Sacramento State to launch national institute on campus researching AI in education,"SACRAMENTO -- Like the dawn of the internet age, artificial intelligence is on the cusp of driving nearly all future innovation.Sacramento State announced Thursday what they call a first-of-its-kind program to perfect how to use AI in classrooms across the country. The university is launching the National Institute for Artificial Intelligence in Education this January, one of the first in the nation. It will be led by faculty AI expert Dr. Alexander ""Sasha"" Sidorkin, current dean of Sac State's College of Education. ""It is a disruptive technological advancement that a lot of people don't know what to do with,"" said Dr. Luke Wood, president of Sacramento State. Wood says AI is already changing education as we know it as it rapidly advances. This new initiative aims to master artificial intelligence, which is the science of making machines think like humans. It's a technology that can be used for good or bad. If left unchecked and unregulated, Wood says you can almost guarantee the outcomes will be negative. ""We're going to be a leader in a space that a lot of institutions are shying away from,"" Wood added. ""That allows us to be able to put ourselves on a national stage in a way that nobody else in the western United States could do.""  The research is meant to find the best ways to use AI ethically for both teachers and students. Faculty will also train on putting it into practice. ""Other institutions are going to want to learn from that so they can better support their students,"" Wood said.It's no secret that AI tools like chatbots can be used for cheating.This push is to make AI not a shortcut, but a tool.""Are students writing their own papers anymore? That's a real conversation we have to have. But do we have guidelines that say how you can use it? So there's so many different implications, which can be scary, and then there's so many that can be positive that can help us address all problems of our society.""Some Sac State students are already using AI in their daily lives. ""I'll ask it to populate an article or just to summarize it for me so I can convert it to my own words. Usually, for the most part, I do most of the work,"" one student told CBS13. ""Our professors, they run our work through an AI-generating system to see how much of our paper was AI-generated. That way, it can come back with red flags."" Some are still among those who haven't yet tested the waters. ""I don't really understand it. I would be interested in learning more about it,"" said another student. University leaders want to seize the opportunity, focusing on tomorrow for both the students and the technology. ""What if we could create a better future where our students could be more productive for the companies they are going to work with?"" Wood asked. Wood added that Sac State will also hire seven new faculty members to be a part of the institute with a focus on AI and quantum computing. The university says the specialists will create adjacent tools, like specialized bots powered by Application Programming Interface (API), to help develop ways of using the new technology for instruction and student support."
20231215,foxnews,Bipartisan lawmakers eye AI safeguards for US agriculture industry,"FIRST ON FOX: Lawmakers are eyeing safeguards for integrating artificial intelligence (AI) technology into the U.S.’s agricultural sector. A new bill introduced by Rep. Randy Feenstra, R-Iowa, and backed by both sides of the aisle aims to enforce standards for AI programs connected to everyday Americans’ food, fuel and other necessities. Feenstra, whose district is heavily rural, told Fox News Digital that AI is becoming increasingly relevant in the farming industry but that existing guardrails on new technology aren’t keeping up with that boom, he suggested. AUTHORS’ COPYRIGHT LAWSUIT AGAINST OPENAI OVER CHATGPT BEGINS  ""From precision agriculture to veterinary software, the latest developments in agricultural technology – including artificial intelligence – have the power to lower input costs for farmers, protect the health of livestock and poultry, and make farming operations more efficient,"" Feenstra said.&nbsp; ""We must be equally active in certifying that these new technologies, products and processes work as they should and uphold the highest industry standards."" AI COMPLICATES COPYRIGHT LAW  His bill, the Farm Tech Act, would protect farmers from ""faulty or misleading technologies by requiring the USDA (U.S. Department of Agriculture) to verify the legitimacy and effectiveness of agricultural software and other technologies that are increasingly used on farms across Iowa and the United States,"" he said. Bill co-sponsor Rep. David Valadao, R-Calif., said, ""As new technology like artificial intelligence becomes more common in our agriculture operations, we need to make sure these new tools are safe for consumers and producers alike."" WHAT IS ARTIFICIAL INTELLIGENCE (AI)?  CLICK HERE TO GET THE FOX NEWS APP It’s also being co-sponsored by Rep. Eric Sorensen, D-Ill. This year has seen a flurry of AI legislation introduced in Congress as lawmakers race to keep up with the rapidly advancing technology, but debate is still ongoing over whether and how to regulate it."
20231215,cnn,Experts call for more diversity to combat bias in artificial intelligence,"Calvin Lawrence has dedicated his career to artificial intelligence. But even after decades of experience in computer engineering, he said one thing remains incredibly rare. “I’ve worked on many AI projects over the last 25 years, not more than two [of my colleagues] looked like me,” Lawrence, who is Black, said. Artificial intelligence holds the promise of rapidly reshaping our society, but with that promise, Lawrence said, comes the challenge of confronting and dismantling biases that can be encoded into emerging technology. Lawrence is the author of the book, “Hidden in White Sight,” which examines how AI contributes to systemic racism. AI is informed by the data it’s built upon and at times that data can be racist, sexist and flawed. In August, a Black mom in Detroit sued the city after she says she was falsely arrested while eight months pregnant because officers linked her to a crime through facial recognition technology. Detroit’s police chief later blamed “poor investigative work.” A 2022 study found a robot trained by AI was more likely to associate Black men with being criminals, or women with being homemakers. The team of researchers concluded the continued use of such technology risked “amplifying malignant stereotypes” that fuel racism and misogyny. In New York City, the local health department recently expanded a coalition challenging clinical algorithms that adjust for race because they say the outcomes are often harmful to people of color. These algorithms have been shown to overestimate a person of color’s health, according to a statement from the New York City Department of Health and Mental Hygiene, which can cause a delay in treatment. In a statement shared with CNN, a spokesperson for OpenAI, the company behind ChatGPT and other artificial general intelligence (AGI) models, said bias is a significant issue across the industry and OpenAI is dedicated to “researching and reducing bias, and other risks, in our models.” “We are continuously iterating on our models to reduce bias and mitigate harmful outputs,” the company said in a statement, adding that for every new model released, OpenAI publishes research on how they are working to achieve those goals. The best way to ensure AI reflects the experiences of people of color, Lawrence said, is to make sure they’re employed and engaged in every step of the process. “You certainly don’t have a lot of Black folks or data scientists participating in the process of deploying and designing AI solutions,” he said. “The only way you can get them to have seats at the table, you have to educate them.” Increasing diversity Studies have found that the lack of diversity and representation in technology fields begins well before college. Students of color generally have less access to foundational computer science courses in high school, a 2023 report by the Code.org Advocacy Coalition found. While 89% of Asian students and 82% of White students had access to these courses respectively, 78% of Black and Hispanic and 67% of Native American students had this same privilege. “These opportunities are not evenly distributed, and that is a problem,” said Andres Lombana-Bermudez, a faculty associate at the Harvard University Berkman Klein Center for Internet and Society.  That disparity in access can also lead to fewer people of color studying computer science and artificial intelligence at the collegiate level, Lombana-Bermudez said. In 2022, more than two-thirds of all doctorates in computer science, computer engineering or information in the United States were awarded to non-permanent U.S. residents for whom no ethnicity is available, according to the 2022 Computing Research Association’s Taulbee Survey. Nearly 19% of degrees went to White doctoral candidates and 10.1% were awarded to Asian candidates, as compared with only 1.7% for Hispanic graduates and 1.6% for Black graduates. Lawrence said he believes diversifying the field of artificial intelligence could make the technology safer and more ethical. Lawrence said he started the nonprofit, AI 4 Black Kids, which works to educate Black children about artificial intelligence and machine learning from a young age, with the hope of one day increasing representation in the field. “AI is trained on so few historical points of view … the goal for me is, having more Black people involved in that process,” he said. The nonprofit offers mentorship programs to kids aged 5 to 19, as well as scholarships and college counseling, Lawrence said. Combating bias in AI requires not only increasing racial diversity, but a diversity of thought as well, Lombana-Bermudez said. He encourages employing sociologists, lawyers, political scientists and other types of humanities-oriented academics to help contribute to the conversation around AI and ethics. Lombana-Bermudez said his hope is that future generations may alleviate some of the problems of bias and inaccessibility because they’re growing up alongside the technology. “I am hopeful that this will change and in the future, we will have better technologies,” he said. “But it’s a struggle, and it’s not easy. It is complex.”"
20230126,foxnews,ChatGPT leads lawmakers to call for regulating artificial intelligence,"The rise of the chatbot ChatGPT, with its ability to generate informed, sophisticated text, is leading lawmakers to push for government intervention in the realm of artificial intelligence. Democrats and Republicans alike are growing increasingly concerned over the development of new AI technologies, and how they could impact society if there are no rules in place. ""Obviously, I think it's something we need to pay close attention to,"" Sen. Josh Hawley, R-Mo., told Fox News when asked about how Congress might approach AI. Others have used ChatGPT itself to illustrate their point that Congress needs to act, and soon. Rep. Ted Lieu, D-Calif., wrote in a New York Times op-ed on the subject earlier this week, and even used ChatGPT to write the first paragraph by entering the prompt: ""Write an attention grabbing first paragraph of an op-ed on why artificial intelligence should be regulated."" AL GORE EXPLAINS GLOBAL AI PROGRAM THAT IS SPYING ON THOUSANDS OF FACILITIES TO MONITOR EMISSIONS  Lieu noted in the piece that, having a degree in computer science, he is ""enthralled"" and ""excited"" by artificial intelligence, but cautioned that ""as a member of Congress, I am freaked out by AI, specifically AI that is left unchecked and unregulated."" Lieu is pushing for the establishment of a federal agency to regulate AI, so that experts can propose rules, although he recognized that it would be a difficult undertaking.  ARTIFICIAL INTELLIGENCE CHATBOT PASSES ELITE BUSINESS SCHOOL EXAM, OUTPERFORMS SOME IVY LEAGUE STUDENTS Rep. Jake Auchincloss, D-Mass., is believed by his staff to be the first member of Congress to deliver remarks on the House floor that were written by artificial intelligence. Auchincloss spoke briefly about a bill that would establish a U.S.-Israel artificial intelligence center. Auchincloss warned against lawmakers falling too far behind AI technology, comparing the situation to social media, which developed so fast Congress could not keep up.  CLICK HERE TO GET THE FOX NEWS APP For that reason, he said, Congress should act sooner rather than later to craft laws. The Associated Press contributed to this report."
20230126,cbsnews,"AI-powered ""robot"" lawyer won't argue in court after jail threats","A ""robot"" lawyer powered by artificial intelligence was set to be the first of its kind to help a defendant fight a traffic ticket in court next month. But the experiment has been scrapped after ""State Bar prosecutors"" threatened the man behind the company that created the chatbot with prison time. Joshua Browder, CEO of DoNotPay, on Wednesday tweeted that his company ""is postponing our court case and sticking to consumer rights.""Browder also said he will not be sending the company's robot lawyer to court. The AI creation — which runs on a smartphone, listens to court arguments and formulates responses for the defendant — was designed to tell the defendant what to say in real time, through headphones. But according to Browder, the prospect for bringing the first robot lawyer into the court room wasn't worth the risk of spending six months in jail.Backlash from lawyers against Browder's proposed stunt suggests that those in the legal profession have concerns over AI-powered chatbots usurping their jobs. The AI lawyer was set to take its first case on February 22, Browder had announced on Twitter.""On February 22nd at 1.30PM, history will be made. For the first time ever, a robot will represent someone in a US courtroom.   DoNotPay A.I will whisper in someone's ear exactly what to say. We will release the results and share more after it happens. Wish us luck!"" he tweeted. He did not disclose the name of the client or the court.DoNotPay has already used AI-generated form letters and chatbots to help people secure refunds for in-flight Wifi that didn't work, as well as to lower bills and dispute parking tickets, according to Browder. All told, the company has relied on these AI templates to win more than 2 million customer service disputes and court cases on behalf of individuals against institutions and organizations, he added.It has raised $27.7 million from tech-focused venture capital firms, including Andreessen Horowitz and Crew Capital.""In the past year, AI tech has really developed and allowed us to go back and forth in real time with corporations and governments,"" he told CBS MoneyWatch of recent advances. ""We spoke live [with companies and customer service reps] to lower bills with companies; and what we're doing next month is try to use the tech in a courtroom for the first time.""DoNotPay had said that it would have covered any fines if the robot were to lose the case. Legal in some, but not most courtrooms Some courts allow defendants to wear hearing aids, some versions of which are Bluetooth-enabled. That's how Browder determined that DoNotPay's technology could legally be used in this case. However, the tech that runs DoNotPay isn't legal in most courtrooms. Some states require that all parties consent to be recorded, which rules out the possibility of a robot lawyer entering many courtrooms. Of the 300 cases DoNotPay considered for a trial of its robot lawyer, only two were feasible, Browder said. ""It's within the letter of the law, but I don't think anyone could ever imagine this would happen,"" Browder said. ""It's not in the spirit of law, but we're trying to push things forward and a lot of people can't afford legal help. If these cases are successful, it will encourage more courts to change their rules.""Lawyers ""would not support this""The ultimate goal of a ""robot"" lawyer, according to Browder, is to democratize legal representation by making it free for those who can't afford it, in some cases eliminating the need for pricey attorneys.""What we are trying to do is automate consumer rights,"" Browder said.  ""New technologies typically fall into the hands of big companies first,  and our goal is put it in hands of the people first.""But given that the technology is illegal in many courtrooms, he doesn't expect to be able to commercialize the product any time soon. When he initially announced that DoNotPay's robot lawyer would appear in court, lawyers threatened him and told him he'd be sent to jail, he told CBS MoneyWatch.""There are a lot of lawyers and bar associations that would not support this,"" Browder said. Putting ChatGPT through law schoolBrowder wants to arm individuals with the same tools that large corporations can typically access, but are out of reach for those without deep resources. AI-powered chatbot ChatGPT has exploded in popularity recently for its ability to spit out coherent essays on wide-ranging topics in under one minute. The technology has drawn interest from investors, with Microsoft on Monday announcing a multibillion dollar investment in parent company OpenAI. Princeton student says his new app helps teachers find ChatGPT cheatsBut Browder highlighted its shortcomings and in some cases, lack of sophistication. ""ChatGPT is very good at holding conversations, but it's terrible at knowing the law. We've had to retrain these AIs to know the law,"" Browder said. ""AI is a high school student, and we're sending it to law school."" "
20230126,cbsnews,BuzzFeed to use OpenAI technology to create content,"Online media company BuzzFeed plans to use artificial intelligence powered by OpenAI, the company behind ChatGPT, to help it generate content.In a memo distributed to BuzzFeed staff on Thursday and obtained by CBS MoneyWatch, CEO Jonah Peretti said AI will play an increasingly large role in the company's operations. Specifically, it plans to use the technology to move beyond curation to help create personality quizzes that ask users questions and generate text write-ups based on their responses.AI ChatGPT is helping CEOs think. Will it also take your job?Artists sue AI company for billions, alleging ""parasite"" app used their work for freePeretti also said AI will assist workers to enhance their content. ""To be clear, we see the breakthroughs in AI opening up a new era of creativity that will allow humans to harness creativity in new ways with endless opportunities and applications for good,"" he said.He noted that AI-created content will move from an ""R&amp;D stage to part of our core business"" this year. It will be used to build quizzes, help staffers brainstorm and personalize content for BuzzFeed's audience. BuzzFeed also hopes the technology will energize its business. The media company has struggled to boost growth, with its stock down nearly 40% over the last year, even with Thursday's large gain. In its latest quarter, BuzzFeed reported a net loss of $27 million on revenue of $104 million, although sales rose 15 from the year-ago period.BuzzFeed shares surged more than 150% to $2.39 in afternoon trading. OpenAI has recently taken the tech world by storm and has already been tested by companies in a number of industries that are experimenting with its capabilities and diverse applications. Its ""generative"" AI has drawn attention from leaders of industry and investors alike, and has been used to write high school essays, create legal documents, help author legislation and even write a speech delivered this week by Rep. Jake Auchincloss, D.-Mass., in the House of Representatives.Experts expect it to take over rote administrative tasks and replace some workers, while also enhancing the quality of many jobs.It will free up skilled professionals to focus on more thoughtful tasks that require the judgement of a human, experts believe.""When I ask ChatGPT what it thinks is going on with this company, it does what junior executives would do, which is they tell me what they see in a table. They say this parameter went down and this one went up in a very clear, coherent manner. But it doesn't move beyond that into the 'so what?'"" Columbia Business School professor Oded Netzer said. ""These are the types of tasks that require judgment and that humans are still very valuable in."""
20220829,cbsnews,"How a robot helps dementia patients at Minnesota nursing home: ""We are taking them back in time""","Roseville, Minnesota â Jill Breckenridge has a new friend at her Minnesota nursing home â but it's not a person. It's a special robot named Pepper that can talk and even dance with the residents to keep them active. Amid staffing shortages at nursing homes, the robot is part of a pilot program that's helping seniors both physically and mentally. Pepper's special power is using new technology to bring up old memories to help dementia patients. Breckenridge, 83, was diagnosed with Alzheimer's disease, but when Pepper shows her a video with pictures of her past, the memories come flooding back. ""I loved my horse, Lucky Strike. And they would get the cart and we put that horse in and he took us to the different rodeos and I always ran first,"" she recalled. Arshia Khan, a professor at the University of Minnesota Duluth, is the brain behind the robots. Khan said she was ""almost in tears"" after seeing Breckenridge interact with Pepper.""It was like, that is what I wanted. We are taking them back in time, because they have lost that time,"" Khan said. ""It's gone, forgotten. But I'm able to bring that back to them at least for a little while."" Breckenridge's daughter, Sharon Fenn, said she could tell her mother was having ""a wonderful time"" with the robot. ""I could tell â¦ when I was watching her. She was beaming,"" Fenn said. "
20230523,foxnews,"Digital seance: New AI tech will mimic speaking to dead family, friends","Artificial intelligence can't bring back the dead, but it may be able to simulate speaking to a lost loved one in an effort to help humans through the grieving process. The high-tech revamp of the traditional seance comes amid the wild growth of large language models, a form of AI that is trained on copious amounts of text. ChatGPT's release year has sparked discussion on how far the tech can go as the chatbot mimics human conversation and answers prompts from humans. Jarren Rocks, product designer and manager at the Los Angeles-based software development company AE Studio, is working on a program called Seance AI, which will allow people to talk with a chatbot that mimics their dead loved ones. ""It's essentially meant to be a short interaction that can provide a sense of closure. That's really where the main focus is here,"" Rocks told the outlet Futurism. ""It's not meant to be something super long term. In its current state, it's meant to provide a conversation for closure and emotional processing."" AI APP'S ABILITY TO RESURRECT LOST LOVED ONES SPARKS FEARS TECHNOLOGY IS CROSSING FANTASY-REALITY RUBICON  Humanity has long been fascinated with trying to communicate with the dead, hitting a fever pitch in the late 19th century when people flocked to attend seances and at least 4 million Americans identified as ""spiritualists."" Even cultural figures on the world stage, such as Mark Twain and Queen Victoria, dabbled in the occult by attending seances, according to the New Yorker. Such activities, however, were shunned by many other Americans and Christians, with the Catholic Church issuing a decree in 1898 that condemned spiritualistic practices and another decree in 1917 that prohibited seances.&nbsp; USING AI TO CHALLENGE DEATH'S FINALITY  With the planned AI seances, only a chatbot will be communicating with the living, but Rocks said he’s leaning into the ""magical"" aspect of the tech. ""We're trying to make it sound as magical and as mystical as possible,"" he told Futurism of Seance AI’s name. Rocks told Fox News Digital that the name of the program is ""intentionally striking because we're confident that we'll be able to provide real comfort to some people."" He added that he and AE Studio are ""greatly concerned about AI safety"" and that they want ""to draw attention to the potential implications of the technology"" – but he said they do not want to halt AI's progress though they support ""healthy regulation."" The program employs tech from OpenAI, the AI lab behind ChatGPT, and prompts users to tell the program the name of the person with whom they wish to speak, their age, personality traits and how they died, according to the outlet. Users will also upload text from their deceased loved one as a template on how the deceased person communicated when they were alive. ANTI-'TERMINATOR': AI NOT A 'CREATURE' WORKING TOWARD SELF-AWARENESS, OPENAI CEO ALTMAN SAYS Once the information is uploaded, the user is taken to a webpage that shows a flame and then can send a message to their simulated loved one. The chatbot responds based on the information it was given, simulating the deceased loved one, Futurism reported. Rocks told Fox News Digital that he had been considering building such technology since the advent of large language models, noting that he and his co-workers at AE Studio have all experienced loss of a loved one. ""Personally, I'm not as curious about the other side as I am addressing the grief that we deal with on this side,"" he said. ""We as people have been obsessed with understanding what is beyond death for a very long time, and while there are many grief-tech solutions for counseling, or therapy, few address personal loss so boldly.""  The program, which strikes a similar tone as a ""Black Mirror"" episode that details the hyper-realistic synthetic recreation of a dead character, is not intended to be used on a regular basis, according to Rocks. AI COULD GO 'TERMINATOR,' GAIN UPPER HAND OVER HUMANS IN DARWINIAN RULES OF EVOLUTION, REPORT WARNS ""For short conversations, I think it feels decently human. I think it falls apart a little bit [when you] start to pick up on repetitions,"" Rocks said. ""It's following a pattern, it doesn't really know exactly what's going on."" Rocks compared the program to a high-tech Ouija board that can be used for closure purposes.&nbsp; ""A traditional seance isn't something that lasts forever. Personally, I think the short time span helps encourage closure, a tool to help you process some unresolved emotions. That said, there are some potential long-term applications that could be viable, and we'll likely launch other features later on,"" he told Fox News Digital, pointing to potentially building a feature such as an ""AI ghost of someone at a grave site."" ""My key priority for Seance AI is that we provide people with tools to help them process loss,"" he added.  Artificial intelligence has gained traction among people who are grieving the loss of a loved one, including through recreating a deceased person’s voice. CLICK HERE TO GET THE FOX NEWS APP South Korea-based tech firm DeepBrain AI crafted a program called ""Re;memory,"" which allows users to upload video, audio and photos of deceased people that is then used to create a virtual version of the person that can communicate with humans. In China, tech developers are building what they dubbed as ""griefbots"" so people can communicate with deceased loved ones, according to Insider. Rocks said SeanceAI will launch Tuesday, including testing a free tier level of the program as well as a paid tier for longer-term users."
20230523,foxnews,What is Black Box AI? Experts explain the hidden decision-making of artificial intelligence machines,"New developments in artificial intelligence have thrust the technology to the forefront of public discord, but also raised concerns about the opaque decision-making process of some systems – often referred to as ""black box AI."" The term ""black box"" came from Great Britain’s Royal Air Force during WWII, Dr. Michael Capps told Fox News Digital. But when it relates to AI, the term is used to describe a decision-making process that cannot be explained. ""The whole idea of a black box is you’re not allowed to look inside and see, and that’s what we have with these artificial neural networks, with hundreds of billions of nodes inside of a box, that nobody can look into,"" Capps said. FEARS OF AI HITTING BLACK MARKET STIR CONCERNS OF CRIMINALS EVADING GOVERNMENT REGULATIONS: EXPERT  ""The black box is just the decision-making process, and what goes into that,"" Christopher Alexander, the COO of Liberty Blockchain, told Fox News Digital. ""And of course, that’s incredibly difficult, because no one can explain a decision-making process, regardless of whether it’s a human or not."" Despite these concerns, black box AI systems have enormous potential, Capps said, but added he still has concerns with the technology. ‘IT'S ALL GONNA TAKE OVER': AMERICANS REVEAL FEARS OF AI IMPACTING EVERY DAY LIFE ""These black box systems, where you can’t see what’s going on inside, can do some amazing things,"" Capps said. ""ChatGPT is an example of a black box. Nobody knows exactly why it gave you the answer it did. You can kind of guess, because you know what it’s seen, you know what we trained it on, so you can kind of guess where it came up with that, but you’ll never know for sure.""&nbsp;  Capps said there are techniques AI creators employ to try and understand why black box AI models make the decisions they do, but ultimately, these techniques are not effective. He compared the decision-making process of an AI to falling in love with a spouse.&nbsp; ""You don’t really know. … You can’t really truly explain it, and neither can a black box. It works the same way. It sees massive amounts of information, and then it puts it into one decision, and then if you ask it why it did it, it will guess,"" he said.&nbsp; This lack of understanding about a machine's outputs means they should not be used for high-stakes decision-making, Capps said.&nbsp; ""And that’s why these systems can’t really be trusted to make super important social decisions. You would never want to hand the nuclear arsenal to a black box AI that might make a decision that’s a bug. It might make a decision because it had bad training advice,"" he said. ""It might make a bad decision because someone snuck some bad training advice in, it’s called poisoning. All these things can go wrong, so we really shouldn’t use it for anything from driving a car to the nuclear arsenal, to even deciding who gets a loan.""&nbsp; OPENAI LAUNCHES CHATGPT APP FOR IOS  Alexander said another challenge with attempting to understand the decision-making process of AI systems is the proprietary technology used in their development.&nbsp; ""A lot of what goes into the black box is proprietary. So who is going to develop something if they’re going to have to give it away and show all the inner workings as soon as they create it,"" Alexander said.&nbsp; CLICK HERE TO GET THE FOX NEWS APP Artificial intelligence was thrust to the forefront of public discourse last year when OpenAI released ChatGPT, a generative AI platform which is capable of human-like conversations, and can answer questions, write stories and songs and even plan trips.&nbsp; But, concerns over bias, as well as a lack of transparency and privacy, have led to some criticism of the platform.&nbsp;"
20230523,foxnews,"Biden, McCarthy debt-ceiling talks ‘productive,’ how AI unlocks our brain and more top headlines","Good morning and welcome to Fox News’ morning newsletter, Fox News First. Subscribe now to get Fox News First in your email. And here's what you need to know to start your day ... ‘AVOID A CATASTROPHE’ - Biden issues terse statement after debt ceiling talks with House Speaker McCarthy. Continue reading … FALLING FLAT -&nbsp;Bud Light reportedly forced to take action after beer remains unsold, expires on shelves.&nbsp;Continue reading … CUTTING EDGE - Here’s how AI is being used to unlock secrets in the human brain.&nbsp;Continue reading … NO BEANS - Durham report guts left’s narrative but one group still disbelieves, writes Mark Penn.&nbsp;Continue reading …DATA FAIL? - FTC issues warning on misuse of biometric info amid rise of generative AI.&nbsp;Continue reading … - POLITICS SUSTAINED DROUGHT - Biden admin announces 'historic' plan to reduce western states' water supply. Continue reading … HEIR APPARENT&nbsp;- Senator won't seek another term, paving way for Democrat rising star.&nbsp;Continue reading … ‘HELL’ OF AN ERROR - Biden national security adviser pressed on $3 billion mistake.&nbsp;Continue reading … ‘DEEPLY OFFENSIVE’ - Two dozen Republicans call on Biden to disavow John Kerry's remarks targeting food production.&nbsp;Continue reading …  Click here for more cartoons…   MEDIA TURNING TO TECH - AI's impact on the banking industry: Association president says the 'jury is still out.' Continue reading … SCHOOLHOUSE ROCKED - Dem governor takes drastic action to stop Republicans from giving more freedom to parents. Continue reading … ‘LOVE YOUR ENEMIES’ - Whoopi, 'The View' make racial comments toward Tim Scott, he immediately cites the Bible. Continue reading … ‘WRONG AND MISLEADING’ - NAACP president scolds CNN over citing Black voter support for DeSantis. Continue reading … &nbsp; PRIME TIME JESSE WATTERS - The racial stink bomb is the left's weapon of choice. Continue reading …SEAN HANNITY - If there is a default, it will be Joe Biden's default. Continue reading … LAURA INGRAHAM -&nbsp;Where's Reverand Al on the issues plaguing the Black community?&nbsp;Continue reading … &nbsp; IN OTHER NEWS SEARCH FOR PEACE - Poland says no to any ‘artificial peace plan’ between Ukraine, Russia Continue reading … MARKLED -&nbsp;Meghan Markle's inner circle: Beyoncé, Oprah and Gwyneth Paltrow help duchess climb status ladder.&nbsp;Continue reading …MILITARY PRIDE - Memorial Day: US nonprofit puts up families of injured, fallen service members in 'beautiful' homes. Continue reading …WATCH: JUST OUT OF REACH! Watch as a cool-as-a-cucumber anteater strolls right outside the lions' enclosure at The San Antonio Zoo. The curious cats can do nothing!&nbsp;See video … &nbsp; VIDEOS WATCH:&nbsp;McCarthy wants a debt ceiling deal by the weekend.&nbsp;See video …&nbsp;WATCH:&nbsp;Americans like Tim Scott, but Trump’s lead growing: Byron York.&nbsp;See video … &nbsp; FOX WEATHER  What’s it looking like in your neighborhood?&nbsp;Continue reading… &nbsp; THE LAST WORD  ""All eyes are on the Biden White House, where Joe is still refusing to come to terms on a debt ceiling that is, well, pretty imminent. According to Democrats, the sky is falling. The American economy and the world's economy is about to collapse. And it's all Kevin McCarthy's fault when in fact it's anything but Kevin McCarthy's fault."" - SEAN HANNITY &nbsp;&nbsp; &nbsp;&nbsp; FOLLOW FOX NEWS ON SOCIAL MEDIA Facebook Instagram YouTube Twitter LinkedIn &nbsp; SIGN UP FOR OUR NEWSLETTERS Fox News First Fox News Opinion Fox News Lifestyle Fox News Entertainment (FOX411) &nbsp;&nbsp; DOWNLOAD OUR APPS Fox News Fox Business Fox Weather Fox Sports Tubi &nbsp;&nbsp; WATCH FOX NEWS ONLINE Fox News Go Thank you for making us your first choice in the morning! We’ll see you in your inbox first thing Wednesday."
20231130,foxnews,How artificial intelligence is changing health care in treating stroke victims,"I am a neurosurgeon who specializes in the treatment of acute strokes, brain bleeds, and tumors.&nbsp; Every second counts for my patients, and I am determined to help as many as I can. This Thanksgiving dinner, I left my family to operate on a patient with a life-threatening stroke. This is what you need to know about strokes and how artificial intelligence is helping surgeons like me save even more patients. Stroke is one of the leading causes of morbidity and mortality worldwide and has remained a formidable challenge in the realm of health care.&nbsp; WHAT IS ARTIFICIAL INTELLIGENCE (AI)? Not only does stroke rob us of our loved ones and shatter families, the impact of stroke from a socioeconomic perspective is also staggering. The CDC estimates that between 2018 and 2019, the economic burden of stroke in the U.S. rose to approximately $56.5 billion.&nbsp; I have witnessed many advances in the diagnosis and treatment of patients with stroke with glacial progress over the years. However, with the advent of artificial intelligence (AI), we have a new powerful ally.&nbsp;&nbsp;  Many of the&nbsp;AI&nbsp;tools physicians employ have drastically improved the fight against stroke, yet there is no substitute for the human element. ""Time is brain"" is the rallying cry for health care teams treating stroke victims. It is not coincidence that we use the acronym FAST, which stands for&nbsp;Face drooping, Arm/leg weakness, Speech difficulty, and Time&nbsp;to remind people of the signs of a stroke and to seek immediate medical assistance.&nbsp; When I received the emergency stroke call this&nbsp;Thanksgiving, I sprang into action and left my home in a flurry, passing the baton of turkey carving to my eldest son. Why the urgency? Because early intervention is crucial for patient outcomes.&nbsp;  While&nbsp;Thanksgiving&nbsp;dinner is my favorite family tradition, this patient needed help fast. Within minutes, the team and I brought the patient to the operating room to restore blood flow to his brain.&nbsp;His symptoms began to improve immediately. TALK THERAPY? AI MAY DETECT ‘EARLIEST SYMPTOMS’ OF DEMENTIA BY ANALYZING SPEECH PATTERNS By coupling what we have already learned and developed,&nbsp;AI&nbsp;algorithms have demonstrated remarkable capabilities in expediting successful treatment.&nbsp;AI&nbsp;can analyze brain scans, such as computed tomography (CT) to not only detect the presence of a stroke but also classify its type.&nbsp; The ability to make such critical differentiations helps guide physicians and nurses to select the most appropriate course of action, whether it be administering clot-busting medications, performing a lifesaving intervention by retrieving the blood clot out of vessels, or preparing for open brain surgery.  Moreover,&nbsp;AI&nbsp;assists in predicting the response to these specific interventions. This level of precision marks a significant departure from the one-size-fits-all approach, heralding a new era in stroke care where treatments are as unique as the patients themselves. This means, unfortunately, there are times I need to discuss with families that despite all our current neurosurgical advances, there are no effective interventions to be offered. AI BABIES: NEW TECHNOLOGY IS HELPING FERTILITY DOCS CHOOSE THE BEST EMBRYOS FOR IVF While there is no question that&nbsp;AI&nbsp;has propelled effective treatment for patients with stroke, there are certainly some limitations.&nbsp;It is not uncommon that given constraints of medical imaging, the&nbsp;AI&nbsp;algorithm may interpret the data incorrectly.&nbsp;&nbsp; Not only have I been involved in cases where&nbsp;AI&nbsp;has indicated surgery should be performed when objectively there was nothing to operate upon, the opposite scenario also arises.&nbsp;It is a precarious situation to call a team for emergency surgery when the&nbsp;AI&nbsp;platform instructs no intervention is required.&nbsp;  As a neurosurgeon, I am too familiar with the anxiety created when&nbsp;AI&nbsp;recommends no intervention, yet surgery is the best chance of salvation for the patient. Imagine the captain of a large commercial&nbsp;airplane performing an emergency maneuver against the recommendations of the flight&nbsp;AI, knowing that hundreds of passengers’ lives onboard hang in the balance.&nbsp;Decades of training and experience along with the degree of self-assurance required for these moments is surreal. CLICK HERE FOR MORE FOX NEWS OPINION Ethical considerations surrounding patient privacy and data security from using&nbsp;AI&nbsp;algorithms also require careful attention. Many of the&nbsp;AI&nbsp;platforms used in the treatment of acute stroke require a third-party software program outside of the hospital.&nbsp;&nbsp; In this world of malicious cyber-attacks committed against hospitals by terrorists&nbsp;aiming for financial gain, it is essential that patient data transfer is completely protected.&nbsp;Ensuring responsible use of&nbsp;AI&nbsp;in stroke care is essential.  The synergy between&nbsp;AI&nbsp;and physicians, nurses and physical therapists involved in stroke care is poised for further advancements. I am personally excited to witness how&nbsp;AI&nbsp;technologies become more integrated into the health care ecosystem, and how&nbsp;AI&nbsp;can facilitate, not replace, the human element.&nbsp; CLICK HERE TO GET THE FOX NEWS APP We must remember that, at the end of the day, we are humans treating other humans, not simply programs analyzing algorithmic equations. Before I left my home on this&nbsp;Thanksgiving&nbsp;to treat the patient suffering a stroke, I reminded my boys that while I needed to leave, I would come home. And as promised, I returned home late in the evening. My boys and my wife sat with me and together we enjoyed a wonderful dinner. Once again, I was thankful for my beautiful, healthy family.&nbsp; CLICK HERE TO READ MORE FROM DR. PAUL SAPHIER"
20231130,foxnews,Brazilian city enacts ordinance written completely by ChatGPT,"City lawmakers in Brazil have enacted what appears to be the nation’s first legislation written entirely by artificial intelligence — even if they didn't know it at the time. The experimental ordinance was passed in October in the southern city of Porto Alegre and city councilman Ramiro Rosário revealed this week that it was written by a chatbot, sparking objections and raising questions about the role of artificial intelligence in public policy. Rosário told The Associated Press that he asked OpenAI’s chatbot ChatGPT to craft a proposal to prevent the city from charging taxpayers to replace water consumption meters if they are stolen. He then presented it to his 35 peers on the council without making a single change or even letting them know about its unprecedented origin. 'SEINFELD' STAR JULIA LOUIS-DREYFUS USED AI TO WRITE ACCEPTANCE SPEECH, BUT WAS MISTAKEN FOR JULIA ROBERTS ""If I had revealed it before, the proposal certainly wouldn't even have been taken to a vote,"" Rosário told the AP by phone on Thursday. The 36-member council approved it unanimously and the ordinance went into effect on Nov. 23. ""It would be unfair to the population to run the risk of the project not being approved simply because it was written by artificial intelligence,"" he added. The arrival of ChatGPT on the marketplace just a year ago has sparked a global debate on the impacts of potentially revolutionary AI-powered chatbots. While some see it as a promising tool, it has also caused concerns and anxiety about the unintended or undesired impacts of a machine handling tasks currently performed by humans. Porto Alegre, with a population of 1.3 million, is the second-largest city in Brazil's south. The city's council president, Hamilton Sossmeier, found out that Rosário had enlisted ChatGPT to write the proposal when the councilman bragged about the achievement on social media on Wednesday. Sossmeier initially told local media he thought it was a ""dangerous precedent."" The AI large language models that power chatbots like ChatGPT work by repeatedly trying to guess the next word in a sentence and are prone to making up false information, a phenomenon sometimes called hallucination. All chatbots sometimes introduce false information when summarizing a document, ranging from about 3% of the time for the most advanced GPT model to a rate of about 27% for one of Google’s models, according to recently published research by the tech company Vectara.  In an article published on the website of Harvard Law School’s Center of Legal Profession earlier this year, Andrew Perlman, dean at Suffolk University Law School, wrote that ChatGPT ""may portend an even more momentous shift than the advent of the internet,"" but also warned of its potential shortcomings. ""It may not always be able to account for the nuances and complexities of the law. Because ChatGPT is a machine learning system, it may not have the same level of understanding and judgment as a human lawyer when it comes to interpreting legal principles and precedent. This could lead to problems in situations where a more in-depth legal analysis is required,"" Perlman wrote. Porto Alegre's Rosário wasn't the first lawmaker in the world to test ChatGPT's abilities. Others have done so in a more limited capacity or with less successful outcomes. In Massachusetts, Democratic state Sen. Barry Finegold turned to ChatGPT to help write a bill aimed at regulating artificial intelligence models, including ChatGPT. Filed earlier this year, it has yet to be voted on. Finegold said by phone on Wednesday that ChatGPT can help with some of the more tedious elements of the lawmaking process, including correctly and quickly searching and citing laws already on the books. However, it is critical that everyone knows ChatGPT or a similar tool was used in the process, he added. ""We want work that is ChatGPT generated to be watermarked,"" he said, adding that the use of artificial intelligence to help draft new laws is inevitable. ""I’m in favor of people using ChatGPT to write bills as long as it’s clear."" ISRAEL'S USE OF AI IN HAMAS WAR CAN HELP LIMIT COLLATERAL DAMAGE 'IF EXECUTED PROPERLY,' EXPERT SAYS There was no such transparency for Rosário's proposal in Porto Alegre. Sossmeier said Rosário did not inform fellow council members that ChatGPT had written the proposal. Keeping the proposal's origin secret was intentional. Rosário told the AP his objective was not just to resolve a local issue, but also to spark a debate. He said he entered a 49-word prompt into ChatGPT and it returned the full draft proposal within seconds, including justifications. ""I am convinced that ... humanity will experience a new technological revolution,"" he said. ""All the tools we have developed as a civilization can be used for evil and good. That’s why we have to show how it can be used for good."" And the council president, who initially decried the method, already appears to have been swayed. CLICK HERE TO GET THE FOX NEWS APP ""I changed my mind,"" Sossmeier said. ""I started to read more in depth and saw that, unfortunately or fortunately, this is going to be a trend."""
20220808,cnn,Baidu gets permits for first fully driverless taxi service in China,"Tech giant Baidu announced Monday that it has obtained permits to operate fully autonomous taxis without any human assistants on board in two of China’s megacities, marking a first for the country. Baidu, which operates China’s largest search engine, said it received the regulatory approvals for its autonomous ride-hailing service Apollo Go to operate on open roads during the daytime in Chongqing and Wuhan. The cities have populations of some 30 million and 11 million people, respectively. The move represents a notable step forward for Baidu and a potential shift in China’s comfort with the new technology. In other cities where the company’s robotaxis operate, including Beijing, Shanghai and Shenzhen, Baidu is required to have a human safety operator present in the vehicle.  “We have finally come to the moment that the industry has been longing for,” Wei Dong, vice president and chief safety operation officer of Baidu’s Intelligent Driving Group, said in a statement Monday. “We believe these permits are a key milestone on the path to the inflection point when the industry can finally roll out fully autonomous driving services at scale.” The permits will allow Baidu to provide fully driverless robotaxi services in designated areas in Wuhan from 9 am to 5 pm and in Chongqing from 9:30 am to 4:30 pm local time. Service will be limited at first, however, with just five robotaxis operating in each city. In the United States, robotaxi offerings remain extremely sparse and ridehailing giants like Uber and Lyft have abandoned in-house self-driving taxi efforts. In June, General Motors-backed Cruise gained permits in California to charge a fare for driverless rides in San Francisco. In July, however, Reuters reported that the National Highway Traffic Safety Administration opened a probe into a crash of a Cruise self-driving vehicle that resulted in minor injuries. Waymo One, the autonomous ride-hailing service operated by Google’s parent company Alphabet, currently offers fully autonomous rides in the Phoenix area."
20230517,foxnews,Meet my new co-pilot in the doctor's office: Artificial Intelligence,"The more I learn about the growing uses of Artificial intelligence in health care, the more convinced I become about its essential place in not just the lab or radiology suite but also in the doctor’s office. It can help usher in a world where tests and treatments are applied on an individual basis based on a patient’s unique history and predicament. ChatGPT recently passed a radiology board style exam, even as it also informed one of my patients that his hemorrhoids might be from prolonged sitting before I thought to mention that possibility to him. At the same time, AI (a program called Sybil) has recently been found to help with earlier diagnosis of lung cancer by picking up abnormalities earlier than a human eye might detect them. Another study showed that it could be employed to measure multiple factors that predict pancreatic cancer up to three years before usual diagnosis.&nbsp; AI has the advantage of searching massive data bases for comparison purposes, allowing it to bring this to bear in detecting differences that signals early pathology. Earlier diagnosis leads directly to earlier treatments and cures. Dr. Miriam Bredella, a prominent professor of radiology at Harvard, told me on Doctor Radio on SiriusXM that a crucial purpose of AI in radiology is to rescreen many thousands of studies (X-rays, CT scans, MRIs) that were done for one reason and to use an AI algorithm to find something else, such as the amount of saturated fat in bone, which can correlate to other health problems, including insulin resistance, diabetes and osteoporosis. AI TOOL HELPS DOCTORS MAKE SENSE OF CHAOTIC PATIENT DATA  A recent article in the journal Nature pointed out that AI could help primary care providers by combining early diagnoses of certain conditions including osteoporosis with treatment recommendations. AI in this context would serve as co-pilot, helping to inform busy doctors of relevant options. Doctors like me are already used to dealing with patients informed by Google searches. AI-fueled information will be more precise, and as long as it doesn’t undermine the doctor-patient relationship, will prove helpful in guiding patients. In fact, a new report from the consulting firm Accenture showed that advances in large language AI models could support or augment 40 percent of all working hours in health care. Seminars in AI application in clinical practice are taking place all over the country, from MIT to Stanford to the Mayo Clinic. CLICK HERE TO GET THE OPINION NEWSLETTER  This past week on Doctor Radio Reports, Dr. Natalia Trayanova, head of the Alliance for Cardiovascular Diagnostic and Treatment Innovations at Johns Hopkins, described digital twins, a replica of something physical, such as the heart or other organ or an entire patient, a dynamic model based on personalized data that can be used to monitor how a system is aging, providing information of how to replace a part that is wearing out without stopping the process that the part is engaged in. The information can constantly be adjusted with new data, using artificial intelligence to help make predictions in terms of disease. CLICK HERE TO GET THE FOX NEWS APP Of course AI technology is still evolving, and it is highly dependent on the quality of the dataset/health records used to train it, but amidst a huge shortage of healthcare workers AI will clearly be of help in a wide range of areas from preventive healthcare to pandemic preparedness, to drug discovery and development. Don’t get me wrong. I want medical decisions to still take place entirely between a health care provider and his or her patients. But AI, when utilized and vetted properly, can certainly speed and smooth and revolutionize the process. CLICK HERE TO READ MORE FROM DR. MARC SIEGEL"
20230517,nbcnews,Inside ChatGPT: How artificial intelligence chatbots work,"By now, you’ve heard of ChatGPT and its text generation capabilities. It has passed a business school exam, confounded teachers looking to spot cheaters and helped people craft emails to their co-workers and loved ones. That it has accomplished those tasks is notable, because exams, essays and emails require correct answers. But being correct isn’t really the point of ChatGPT — it’s more of a byproduct of its objective: producing natural-sounding text. So how do artificial intelligence chatbots work, and why do they get some answers right and some answers really, really wrong? Here’s a look inside the box. The technology behind large language models like ChatGPT is similar to the predictive text feature you see when you compose a message on your phone. Your phone will evaluate what has been typed in and calculate probabilities of what’s most likely to follow, based on its model and what it has observed from your past behavior. Anyone familiar with the process knows how many different directions a string of text can branch into.  Unlike the phone’s predictive text feature, ChatGPT is said to be generative (the G in GPT). It isn’t making one-off predictions; instead it’s meant to create text strings that make sense across multiple sentences and paragraphs. The output is meant to make sense and read as though a person wrote it, and it should match up with the prompt.  So what helps it pick a good next word, and then another word after that, and on and on?  The internal reference There is no database of facts or a dictionary inside the machine to help it “understand” words. Instead, the system treats words mathematically, as a collection of values. You can think of these values as representing some quality the word might have. For example, is the word complimentary or critical? Sweet or sour? Low or high?  In theory, you could set these values wherever you like and find that you have come close to a word. Here is a fictional example to demonstrate the idea: The generator below is designed to return a different fruit based on the three qualities. Try changing any of the qualities to see how the output changes. That technique is called word embedding, and it isn’t new. It originated in the field of linguistics in the 1950s. While the example above uses just three “qualities,” in a large language model, the number of “qualities” for every word would be in the hundreds, allowing a very precise way to identify words.  Learning to make sense When the model is new, the qualities associated with each word are set randomly, which isn’t very useful, because its ability to predict depends on their being very finely tuned. To get there, it needs to be trained on a lot of content. That is the large part of the large language model. A system like ChatGPT might be fed millions of webpages and digital documents. (Think about the entirety of Wikipedia, big news websites, blogs and digitized books.) The machine cycles through the training data one stretch at a time, blocking out a word in a sequence and calculating a “guess” at what values most closely represent what should go in the blank. When the right answer is revealed, the machine can use the difference between what it guessed and the actual word to improve. It’s a lengthy process. OpenAI, the company behind ChatGPT, hasn’t published the details about how much training data went into ChatGPT or the computer power used to train it, but researchers from Nvidia, Stanford University and Microsoft estimate that, using 1,024 graphics processing units, it would have taken 34 days to train GPT 3, ChatGPT’s predecessor. One analyst estimated that the cost of computational resources to train and run large language models could stretch into the millions.  ChatGPT also has an extra layer of training, referred to as reinforcement learning from human feedback. While previous training is about getting the model to fill in missing text, this phase is about getting it to put out strings that are coherent, accurate and conversational. During this stage, people rate the machine’s response, flagging output that is incorrect, unhelpful or even downright nonsensical. Using the feedback, the machine learns to predict whether humans will find its responses useful. OpenAI says this training makes the output of its model safer, more relevant and less likely to “hallucinate” facts. And researchers have said it is what aligns ChatGPT’s responses better with human expectations. At the end of the process, there is no record of the original training data inside the model. It doesn’t contain facts or quotes that can be referred to — just how related or unrelated words were to one another in action.  Putting the training to use This set of data turns out to be surprisingly powerful. When you type your query into ChatGPT, it translates everything into numbers using what it learned during training. Then it does the same series of calculations from above to predict the next word in its response. This time, there’s no hidden word to reveal; it just predicts.  Thanks to its ability to refer to earlier parts of the conversation, it can keep it up page after page of realistic, human-sounding text that is sometimes, but not always, correct. Limitations At this point, there are plenty of disagreements about what AI is or will be capable of, but one thing is pretty well agreed upon — and prominently featured on the interfaces of ChatGPT, Google Bard and Microsoft Bing: These tools shouldn’t be relied on when accuracy is required.  Large language models are able to identify text patterns, not facts. And a number of models, including ChatGPT, have knowledge cutoff dates, which means they can’t connect to the internet to learn new information. That’s in contrast to Microsoft’s Bing chatbot, which can query online resources.  A large language model is also only as good as the material that was used to train it. Because models identify patterns between words, feeding an AI text that is dangerous or racist means the AI will learn text patterns that are dangerous or racist. OpenAI says it has created some guardrails to prevent it from serving that up, and ChatGPT says it is “trained to decline inappropriate requests,” as we discovered when it refused to write an angry email demanding a raise. But the company also admits that ChatGPT will still sometimes “respond to harmful instructions or exhibit biased behavior.” There are many useful ways to take advantage of the technology now, such as drafting cover letters, summarizing meetings or planning meals. The big question is whether improvements in the technology can push past some of its flaws, enabling it to create truly reliable text.  Methodology Graphics by JoElla Carman. In the “Pride and Prejudice” graphic, Google Bard, OpenAI GPT-1 and ChatGPT were given the prompt “Please summarize Pride and Prejudice by Jane Austen in one sentence.” BigScience Bloom was asked to finish the sentence “In the novel Pride and Prejudice, Jane Austen.” All responses collected May 11, 2023. In the email graphic, OpenAI ChatGPT was given the prompts: “Write a positive email asking for a raise,” “Write a neutral email asking for a raise,” “Write an agitated email asking for a raise,” “Write an angry email asking for a raise.” All responses collected May 8, 2023."
20230517,foxnews,Adopting AI systems too quickly without full testing could lead to 'errors by health care workers': WHO,"As the artificial intelligence train barrels on with no signs of slowing down — some studies have even predicted that AI will grow by more than 37% per year between now and 2030 — the World Health Organization (WHO) has issued an advisory calling for ""safe and ethical AI for health."" The agency recommended caution when using ""AI-generated large language model tools (LLMs) to protect and promote human well-being, human safety and autonomy, and preserve public health."" ChatGPT, Bard and Bert are currently some of the most popular LLMs.&nbsp; In some cases, the chatbots have been shown to rival real physicians in terms of the quality of their responses to medical questions. CHATGPT FOUND TO GIVE BETTER MEDICAL ADVICE THAN REAL DOCTORS IN BLIND STUDY: ‘THIS WILL BE A GAME CHANGER’ While the WHO acknowledges that there is ""significant excitement"" about the potential to use these chatbots for health-related needs, the organization underscores the need to weigh the risks carefully. ""This includes widespread adherence to key values of transparency, inclusion, public engagement, expert supervision and rigorous evaluation.""  The agency warned that adopting AI systems too quickly without thorough testing could result in ""errors by health care workers"" and could ""cause harm to patients."" WHO outlines specific concerns In its advisory, WHO warned that LLMs like ChatGPT could be trained on biased data, potentially ""generating misleading or inaccurate information that could pose risks to health equity and inclusiveness."" ""Using caution is paramount to patient safety and privacy."" There is also the risk that these AI models could generate incorrect responses to health questions while still coming across as confident and authoritative, the agency said. CHATGPT, MEAL PLANNING AND FOOD ALLERGIES: STUDY MEASURED ‘ROBO DIET’ SAFETY AS EXPERTS SOUND WARNINGS ""LLMs can be misused to generate and disseminate highly convincing disinformation in the form of text, audio or video content that is difficult for the public to differentiate from reliable health content,"" WHO stated.  Another concern is that LLMs might be trained on data without the consent of those who originally provided it — and that it may not have the proper protections in place for the sensitive data that patients enter when seeking advice. ""LLMs generate data that appear accurate and definitive but may be completely erroneous."" ""While committed to harnessing new technologies, including AI and digital health, to improve human health, WHO recommends that policy-makers ensure patient safety and protection while technology firms work to commercialize LLMs,"" the organization said. AI expert weighs risks, benefits Manny Krakaris, CEO of the San Francisco-based health technology company Augmedix, said he supports the WHO’s advisory. ""This is a quickly evolving topic and using caution is paramount to patient safety and privacy,"" he told Fox News Digital in an email. NEW AI TOOL HELPS DOCTORS STREAMLINE DOCUMENTATION AND FOCUS ON PATIENTS Augmedix leverages LLMs, along with other technologies, to produce medical documentation and data solutions. ""When used with appropriate guardrails and human oversight for quality assurance, LLMs can bring a great deal of efficiency,"" Krakaris said. ""For example, they can be used to provide summarizations and streamline large amounts of data quickly.""  He did highlight some potential risks, however.&nbsp; ""While LLMs can be used as a supportive tool, doctors and patients cannot rely on LLMs as a standalone solution,"" Krakaris said. ""LLMs generate data that appear accurate and definitive but may be completely erroneous, as WHO noted in its advisory,"" he continued. ""This can have catastrophic consequences, especially in health care."" CLICK HERE TO SIGN UP FOR OUR HEALTH NEWSLETTER When creating its ambient medical documentation services, Augmedix combines LLMs with automatic speech recognition (ASR), natural language processing (NLP) and structured data models to help ensure the output is accurate and relevant, Krakaris said. AI has ‘promise' but requires caution and testing Krakaris said he sees a lot of promise for the use of AI in health care, as long as these technologies are used with caution, properly tested and guided by human involvement. CLICK HERE TO GET THE FOX NEWS APP ""AI will never fully replace people, but when used with the proper parameters to ensure that quality of care is not compromised, it can create efficiencies, ultimately supporting some of the biggest issues that plague the health care industry today, including clinician shortages and burnout,"" he said."
20220606,foxnews,"California security surveillance company curbs crime through AI, real-time human intervention,  CEO says","A security surveillance company based in California, where crime is surging, pairs artificial intelligence able to locate potential threats with real-time human intervention to deter criminals. ""The entire idea behind Deep Sentinel is that we want to prevent crimes before they happen,"" Deep Sentinel CEO David Selinger told Fox News. Property crime has increased in several major cities across California in recent years. San Francisco, for example, has repeatedly had the most property crime among the 25 largest U.S. cities in four of the last six years, according to The Wall Street Journal. In a recent Bay Area Council poll, a majority of registered voters said the region was not a safe place to live. VIOLENT CRIMES ON THE RISE IN 2022, FOLLOWING PREVIOUS UNPRECENDENTED SPIKE IN MURDERS  Deep Sentinel, founded in 2016, uses smart security cameras with AI. If potential threats are detected, human guards are alerted and can speak directly to potential criminals with two-way audio and contact police. ""We use a combination of different types of technology to detect potential suspicious activity before it turns into a crime,"" Selinger said. ""And then in real time, we have live human being guards who are notified."" LOS ANGELES: SHAMELESS SEPHORA ROBBERS EMPTY SHELVES, FILL TRASH BAGS IN FRONT OF SHOPPERS  ""All it takes is letting someone know who is about to commit a crime that they're watching and the police are on their way, and they stop,"" Selinger continued. Selinger said the Deep Sentintel has seen improvements in areas using its tech. He pointed to Salinas, California, where the company partnered with the Salinas City Center Improvement Association and created a program to make Deep Sentinel affordable for ordinary businesses in the city.&nbsp;  AMERICAN DREAM REPLACED BY ‘NIGHTMARES’ FOR CALIFORNIANS, SAYS AG CANDIDATE Total burglaries in Salinas decreased by more than 70% during 2021, police data show, though the statistics don't indicate whether areas that heavily use Deep Sentinel – primary downtown businesses – had a greater change. A commercial property manager in Salinas said during a Deep Sentinel promo that tenants who had installed the system found it ""refreshing"" and that people do not ""loiter"" or ""cause trouble"" because ""Deep Sentinel chases them away.""&nbsp; CLICK HERE TO GET THE FOX NEWS APP Studies the company conducted found that attempted crimes started to decrease about two to six months after installing the tech. ""All Americans deserve to feel safe,"" Selinger said. ""Over time, we hope to develop technology to the point where we can bring the price points down and make it more accessible to every American."""
20230620,foxnews,"'Decisive actions' on AI coming in next few weeks, White House says","The White House said Tuesday it will soon take ""decisive actions"" to get ahead of the rapid advancement of AI technology. ""The White House Chief of Staff office is overseeing a process to rapidly develop decisive actions we can take over the coming weeks,"" a White House official said. ""White House principals have met to discuss this issue 2-3 times a week in addition to ongoing daily work being done across the White House and agencies,"" the official added.&nbsp;""White House officials are also working on securing commitments from leading AI companies to combat challenges from the government and the private sector side."" BALLOONING AI-DRIVEN FACIAL RECOGNITION INDUSTRY SPARKS CONCERN OVER BIAS, PRIVACY: ‘YOU ARE BEING IDENTIFIED'  The announcement came on the same day President Biden was set to meet with a panel of AI-focused experts in San Francisco to discuss the technology’s opportunities and drawbacks. It is part of an overall push by the White House to put guardrails up as AI continues to permeate more facets of everyday life, including the 2024 election cycle. A White House official noted that Biden took several recent steps toward regulating AI, including convening a meeting with top artificial intelligence CEOs at the White House and rolling out a blueprint for an AI bill of rights. HOW TO REIN IN THE AI THREAT? LET THE LAWYERS LOOSE  The official added that the Office of Management and Budget has been tasked with putting together ""draft policy guidance for federal agencies to ensure the development, procurement, and use of AI systems is centered around safeguarding the American people’s rights and safety."" Biden is expected to give a speech on his administration’s commitment to ""seizing the opportunities and managing the risks"" of AI at 4 p.m. ET on Tuesday. WHO IS WATCHING YOU? AI CAN STALK UNSUSPECTING VICTIMS WITH 'EASE AND PRECISION': EXPERTS Participants expected at his AI meeting include Khan Academy founder Sal Khan, Stanford University Human-Centered AI Institute head Fei-Fei Li, and Algorithmic Justice League founder Joy Buolamwin. The White House tapped Vice President Kamala Harris as its AI czar earlier this year, reportedly in a bid to help refurbish her image in time for the 2024 election. However, guidance for both her and Biden’s schedules released on Monday evening suggests she will not be on-hand for Tuesday’s meeting and speech. CLICK HERE TO GET THE FOX NEWS APP"
20230620,cnn,From ChatGPT to executive orders: Inside the White House’s urgent push to regulate AI,"President Joe Biden huddled in the Oval Office with several of his top advisers in early April as an aide typed prompts into ChatGPT: Summarize the Supreme Court’s New Jersey v. Delaware ruling and turn it into a Bruce Springsteen song.  Weeks earlier, Biden had joked with Springsteen at the National Medal of Arts ceremony that the case, which centered on rights to the Delaware River, also gave his home state a claim to The Boss. Now, before the president’s eyes, the AI chatbot instantaneously began composing the lyrics in Springsteen’s style.  Like many Americans who have toyed with ChatGPT, the president was wowed.  By the end of the meeting, which also focused on AI’s impact on cybersecurity and jobs, he reminded the aides in the room – including his chief of staff Jeff Zients, deputy chief of staff Bruce Reed and top science adviser Dr. Arati Prabhakar – of what had already been clear inside the West Wing for weeks: AI should be a top priority. Weeks earlier, explosion of ChatGPT propelled artificial intelligence into the public consciousness, triggering a flurry of hearings on Capitol Hill as AI industry leaders touted its revolutionary potential, but also warned of “the risk of extinction from AI.”  At the White House, the surge of interest in ChatGPT moved AI from the margins to a central priority. That urgency is being welcomed in AI policy circles. Multiple people who have advised the White House on AI policy said that while the White House laid an important foundation last year with its Blueprint for an AI Bill of Rights, they were concerned that the administration was not devoting sufficient attention to AI policy. Those same people say it’s now clear the White House has shifted into a higher gear to meet the moment.  “If we had this conversation six months ago, my responses would be very different than today,” said a member of the National AI Advisory Committee, who pointed to a “wake-up call” inside the federal government since the explosion of ChatGPT.   Steered by the White House chief of staff’s office, senior administration officials have been meeting two to three times a week to advance AI policy work since earlier this spring, tackling AI on multiple fronts, from misinformation and cybersecurity to economic transformation and equity. AI has also become a consistent topic of conversation during the weekly Saturday strategy sessions between Biden’s senior-most advisers.  After dropping by a meeting of leading AI CEOs at the White House last month, Biden on Tuesday met with a group of AI experts and academics in San Francisco to get a non-industry perspective on the risks and opportunities of AI.  “I want to hear directly from the experts, and these are some of the world’s leading experts on this issue,” Biden said, noting that he hopes to hear about the “risks” and “promise” of artificial intelligence. He added that Vice President Kamala Harris will hold a summit on artificial intelligence next month focused on consumer protection. Officials say they have been urgently laying the groundwork for several policy actions that will be unveiled this summer – including executive orders – to maximize the effect of existing regulations on AI, harness its potential and establish new guardrails for the booming, multi-faceted technology.  “This is not an area that you can take years to get your arms around or regulate. You’ve got to measure time in weeks,” Zients told CNN. “Speed is really important here. If one acts too slowly, you’re going to be behind by the time you take action, and your action is going to be leapfrogged by the technology. So, we have to act decisively and with speed and pull every lever we have to maximize the positive impact while minimizing any unintended consequences.”  Officials are especially mindful of Washington’s poor track record of swiftly tackling major technological change and the failure to regulate social media early on looms especially large. This time, most leading AI companies are calling for Washington to regulate their industry – albeit with different proposals and motives – and several legislative efforts are already underway.  Senior administration officials acknowledged that legislation will be needed to address some novel aspects and issues arising from AI, including the core technology, but they also believe they can and must begin to shape the regulatory framework through executive action.  Among the tasks officials have already embarked on, including creating an extensive inventory of government regulations that could be applied to AI and identifying where new regulations need to be created to fill the gaps, according to a senior administration official.  By next month, leading AI companies like Google, Microsoft and OpenAI are expected to announce privacy and safety commitments crafted in coordination with the White House, according to a senior administration official, who said the federal government will employ “appropriate methods to ensure companies live up to these commitments.”  This summer, the Office of Management and Budget is also expected to release long-awaited guidance for federal agencies on the use and procurement of AI technologies, leveraging the federal government’s status as a large client to shape the industry.   National security adviser Jake Sullivan and his team are also developing policies to respond to the cybersecurity risks associated with AI and coordinating with the G7 to establish international norms around AI.  Senior administration officials declined to divulge details of other forthcoming executive orders but said to expect additional actions over the summer months.  “It’s a matter of great urgency,” Prabhakar said in an interview. “It’s very active, very focused. People are moving faster than they normally do.”  Several people familiar with the White House’s work also credited Zients, who is steeped in the tech sector and whose arrival in February coincided with the explosion of ChatGPT, with the White House’s ramp-up. One senior administration official referred to him as an “accelerator” of the efforts. What’s clear is that White House officials aren’t starting from scratch, building instead on their 73-page Blueprint for an AI Bill of Rights released in October, which officials have called a foundation for the administration’s approach to AI policy. Officials are also leaning on the risk management framework released earlier this year by the Department of Commerce’s National Institute of Standards and Technology. This year, Biden signed an executive order directing federal agencies to root out bias in artificial technologies used by the federal government and combat algorithmic discrimination and the administration announced $140 million to launch new AI research institutes. “I think what Chat GPT did … was to democratize the fear and spread the concerns much more visibly and broadly in a way that people who have been paying attention to this were aware of, but not the general public,” said Suresh Venkatasubramanian, an AI researcher at Brown University who worked at the White House’s Office of Science and Technology Policy until last summer. “Once that happened … I think everyone – Congress and the White House – went into high gear in thinking about this.”   That fear is something Biden has referenced directly, telling AI leaders who met with Harris and top White House officials last month that “what you’re doing has enormous potential and enormous danger.”  “I don’t think ever in the history of human endeavor has there been as fundamental potential technological change as is presented by artificial intelligence,” Biden said at a news conference earlier this month. “It is staggering. It is staggering.” "
20240204,foxnews,"'We need to win' AI race against Beijing, House China Committee member warns","EXCLUSIVE: A House GOP lawmaker on the China Select Committee is warning that it is critical for the U.S. to beat China in the ""race"" for dominance in the artificial intelligence sphere. ""China is pursuing AI, but they're also pursuing quantum computing, and it’s a lethal combination,"" Rep. Carlos Gimenez, R-Fla., told Fox News Digital. ""And in terms of artificial intelligence, the more data that they gather, the faster they’ll advance…AI is a race that we need to win."" Gimenez explained that AI technology was rapidly being integrated into more facets of both everyday life and the national security sphere. TAYLOR SWIFT AI-GENERATED EXPLICIT PHOTOS OUTRAGE FANS: ‘PROTECT TAYLOR SWIFT’&nbsp;  ""We have to win the race for AI because of the applications of AI in everything, including military hardware. So it's important for us to win that race, or else that technology will be used against us in the future,"" he said. When asked about his concerns regarding China coming out ahead, Gimenez said, ""Many of their weapons will be superior to ours, and that causes me great concern."" WHAT IS ARTIFICIAL INTELLIGENCE (AI)? Just last year, the Pentagon unveiled an ambitious new AI program, called the Replicator initiative, aimed at producing thousands of drones with autonomous capabilities in order to compete with China.&nbsp; ""Replicator is meant to help us overcome the PRC’s biggest advantage, which is mass. More ships. More missiles. More people,"" Deputy Pentagon Secretary Kathleen Hicks said in August. ""To stay ahead, we’re going to create a new state of the art — just as America has before — leveraging attritable, autonomous systems in all domains — which are less expensive, put fewer people in the line of fire, and can be changed, updated, or improved with substantially shorter lead times.""  However, Gimenez pointed out that in addition to the military implications, the AI race between the U.S. and China is also being run on a more granular level, which is aided by Beijing’s ability to harvest Americans’ data via TikTok. He pressed FBI Director Christopher Wray on the issue in a hearing last month, during which Wray admitted he had ""very significant security concerns about TikTok."" ISRAEL CREATES AI PLATFORM TO TRACK THE HUMANITARIAN SITUATION IN GAZA&nbsp; ""It’s a combination of the ability that the Chinese government would have, if they should choose to exercise it, to control the collection of the data, to control the recommendation algorithm, and if they wanted to, to be able to control and compromise devices,"" Wray said. ""And if you layer AI, as you’re saying, on top of all of that, it just amplifies those concerns, because the ability to use U.S. personal data and feed that into their AI engine, that just magnifies the problem."" Gimenez told Fox News Digital that the way to mitigate concerns about China and stay on top of AI innovation was to look closely at U.S. institutions with ties to Beijing.  CLICK HERE TO GET THE FOX NEWS APP ""I think we should be looking at educational institutions that have close ties to Chinese companies, Chinese nationals that may be working for the PRC. Look, if you're a Chinese company, you are bound by their law to turn over whatever research and findings that you have [that] could be useful to the [Chinese military],"" he said. ""And so we need to look at every single Chinese company as basically an extension of the Chinese military. That's extremely concerning to me, and the fact that American universities and Western universities that…could be transferring technology."""
20240329,foxnews,Fox News AI Newsletter: Country superstar praises state AI legislation protecting musicians,"Welcome to Fox News’ Artificial Intelligence newsletter with the latest AI technology advancements. IN TODAY’S NEWSLETTER: - Luke Bryan praises new Tennessee AI legislation protecting musicians: ‘What an amazing precedent to set’- Hillary Clinton warns AI tech will make 2016 election disinformation 'look primitive'- Goats, Google and Games: The future impact of a tech giant’s push to train AI to play video games ‘AMAZING PRECEDENT’: Luke Bryan is celebrating new protections from artificial intelligence for musicians in Nashville.  ELECTION THREAT: Former Secretary of State Hillary Clinton described herself as a victim of election disinformation during a panel discussion on Thursday, and warned that the advancement of artificial intelligence (AI) will make her experience ""look primitive."" LEVEL UP: Google has developed an artificial intelligence system that can play video games like a human and take orders from players and could eventually even have real-world implications down the line.  DR. AI: Studies have shown that up to 10% of doctors are now using ChatGPT, a large language model (LLM) made by OpenAI — but just how accurate are its responses? HYBRID WORK: Employees have positive views about returning to the office but expect it to look and feel differently than it did before the pandemic to accommodate hybrid arrangements as well as facilitating new artificial intelligence (AI) technologies, according to a new study by Cisco.  Subscribe now to get the Fox News Artificial Intelligence Newsletter in your inbox. FOLLOW FOX NEWS ON SOCIAL MEDIA FacebookInstagramYouTubeTwitterLinkedIn SIGN UP FOR OUR OTHER NEWSLETTERS Fox News FirstFox News OpinionFox News LifestyleFox News Health DOWNLOAD OUR APPS Fox NewsFox BusinessFox WeatherFox SportsTubi WATCH FOX NEWS ONLINE Fox News Go STREAM FOX NATION Fox Nation Stay up to date on the latest AI technology advancements and learn about the challenges and opportunities AI presents now and for the future with Fox News&nbsp;here."
20230921,cnn,Huawei wants to go all in on AI for the next decade,"Huawei has joined the list of companies that want to be all about artificial intelligence. For the first time in about 10 years, the Chinese tech and telecoms giant announced its new strategic direction on Wednesday, saying it would shift its focus to AI. Previously, the company had prioritized cloud computing and intellectual property, respectively, over two decade-long periods. Meng Wanzhou, Huawei’s rotating chairwoman and chief financial officer, made the announcement in Shanghai during a company event. “As artificial intelligence gains steam, and its impact on industry continues to grow, Huawei’s All Intelligence strategy is designed to help all industries make the most of new strategic opportunities,” the company said in a statement. Meng said in a speech that Huawei was “committed to building a solid computing backbone for China — and another option for the world.” “Our end goal is to help meet the diverse AI computing needs of different industries,” she added, without providing details.  Huawei’s decision follows a similar move by fellow Chinese tech giant Alibaba (BABA), announced earlier this month, to prioritize AI.  Other companies, such as Japan’s SoftBank, have also long declared an intent to focus more on the fast-moving technology, and more businesses have jumped on the bandwagon this year due to excitement about platforms such as GPT-4.  Meng returned to China in September 2021 after spending nearly three years under house arrest in Canada as part of an extradition battle with the United States. She and Huawei had been charged for alleged bank fraud and evasion of economic sanctions against Iran.  The executive, who is also the daughter of Huawei founder Ren Zhengfei, was able to leave after reaching an agreement with the US Department of Justice and ultimately having her charges dismissed. Meng began her role as the rotating chairperson of the company in April and is expected to stay in the position for six months. Hacking allegations News of Huawei’s strategic update came the same day the company was mentioned in allegations lodged by China against the United States. In a statement posted Wednesday on Chinese social network WeChat, China’s Ministry of State Security accused Washington of infiltrating Huawei servers nearly 15 years ago. “With its powerful arsenal of cyberattacks, the United States intelligence services have carried out surveillance, theft of secrets and cyberattacks against many countries around the world, including China, in a variety of ways,” the ministry said. It alleged that the US National Security Agency (NSA), in particular, had “repeatedly conducted systematic and platform-based attacks on China in an attempt to steal China’s important data resources.” Huawei declined to comment on the allegations, while the NSA did not immediately respond to a request for comment outside regular US business hours. The claims are especially notable because US officials have long suspected the company of spying on the networks that its technology operates, using it as grounds to restrict trade with the company. Huawei has vehemently denied the claims, saying it operates independently of the Chinese government. In 2019, Huawei was added to the US “entity list,” which restricts exports to select organizations without a US government license. The following year, the US government expanded on those curbs by seeking to cut Huawei off from chip suppliers that use US technology. In recent weeks, Huawei has added to US-China tensions again after launching a new smartphone that represents an apparent technological breakthrough. Huawei launched the Mate 60 Pro, its latest flagship device, last month, prompting a US investigation. Analysts who have examined the phone have said it includes a 5G chip, suggesting Huawei may have found a way to overcome American export controls.  — Mengchen Zhang contributed to this report."
20230921,foxnews,Texas churchgoers get 'shotgun sermon' cooked up by chatbot,"A Texas church hosted a Sunday service the was generated entirely by artificial intelligence. The Violet Crown City Church in north Austin used ChatGPT to develop a sermon, with pastor Jay Cooper saying he got the idea after reading about the technology and wondering what it might be like to use in during a service, according to a report from KXAN. ""ChatGPT kicked out about a 15-minute service, like a shotgun sermon, an outline,"" Cooper said. ""It’s very clear that a human element is still needed. I had to fill out the service with additional prompts and add a couple prompts to the sermon to kind of beef it up."" AI ROBOTS CAPABLE OF CARRYING OUT ATTACK ON NHS THAT WOULD CAUSE COVID-LIKE DISRUPTION, EXPERT WARNS  ChatGPT, an AI chatbot developed by OpenAI, has continued to gain popularity in recent months. The service has been used to help with a variety of applications, including generating ideas or the outline for articles, essays, and even books. In some cases, ChatGPT or similar platforms have been used to write entire articles and books, while others have used the technology to help assist with research. ""There’s so many different applications for AI,"" Cooper said. ""I just had the idea, ‘What would it look like to incorporate this into a worship service?'"" Cooper said he talked with members of the congregation before attempting the service, though some expressed afterward that AI would have a hard time replacing the human element. WHAT IS ARTIFICIAL INTELLIGENCE (AI)? ""I’m not sure that AI can actually express the emotions of love and kindness and empathy,"" Ernest Chambers, who attended the service, told KXAN. ""I think that we must practice love and express that. Not only feel it, but we must express it.""  Cooper seemingly agreed with that sentiment, saying that ""that human touch"" is ""critical"" in both ""life and ministry."" Nevertheless, Cooper believes the experiment was an opportunity for both himself and those in attendance to learn more about what it means to worship. CLICK HERE FOR MORE US NEWS ""A big question that comes up to me as we let AI lead worship is can a prayer written by artificial intelligence in some way communicate truth? Can you experience God through that?"" Cooper asked.  Cooper noted that its possible AI-generated sermons could pick up on components that resonate with people, helping them open their eyes to new ideas and ways of thinking. ""Perhaps something resonates with them and then it opens their mind to, maybe I’m not looking for the sacred enough in the rest of the world,"" Cooper said. CLICK HERE TO GET THE FOX NEWS APP Despite the experiment, Cooper said that using AI for the service was a one time deal and he currently has no plans to do a similar service. ""I think the messiness of humanity should be present in worship,"" Cooper said."
20230921,cbsnews,How the AI revolution is different: It threatens white-collar workers,"The emergence of artificial intelligence like ChatGPT has aroused fears of these tools replacing people in a range of professions, from coders to truck drivers. Although such concerns tend to ignore technology's potential to create jobs, new forms of AI do pose a risk to some workers, new research from Indeed suggests: white-collar workers.""Surprisingly enough, knowledge workers are facing the highest level of exposure here, which is quite different with what we've seen with other revolutions,"" Svenja Gudell, chief economist at Indeed Hiring Lab, a job-search platform, told CBS MoneyWatch. ""With automation, often it was manual labor that was replaced.""Unlike previous cycles of technical innovation, in-person, often low-wage jobs that rely heavily on humans being physically present are likely to be the the most resilient to encroaching AI, she added.""Driving cars still currently takes a person. Or child care. We probably wouldn't give our kids over to the robots quite yet,"" she said. Gudell added that ""We'll see the destruction of some jobs but also the creation of others along way. The human element still carries a lot of weight in these jobs — you really can't do without it.""What jobs are most at risk?Among the openings currently on Indeed, software and coding jobs are the most exposed to replacement by AI, the firm found in a its analysis. That's because so-called generative AI was determined to be adept at performing 95% of the skills these jobs require. In addition to software development, information technology, mathematics, information design, legal and accounting positions are also among the more exposed professions.By contrast, truck and taxi driver jobs are least exposed to AI, which could only adequately perform about 30% of the necessary skills, according to Indeed. Other jobs that are relatively insulated against AI include cleaning and sanitation as well as beauty and wellness jobs, in part because they are least likely to be performed remotely. Another key takeway, according to Indeed: The more suitable a job is to remote work, the higher its potential exposure is to generative AI-driven change. ""A lot of in-person jobs heavily rely on that human element. You might mix in parts of generative AI there, but at the end of the day a nurse still needs to be present to stick the needle in the patient's arm to draw blood. With sales reps, a lot of in-person communication happens when talking to clients,"" Gudell said.To be sure, AI is unlikely ever to fully replace humans even in areas where the technology excels. But it may supplant some workers whose jobs are rote and who don't employ AI to make them more productive. ""It could mean you as an employee can use these tools and focus on higher productivity-level skills on the job. From the employer perspective, instead of hiring 15 copy editors, you might employ five because generative AI carries the load,"" Gudell said. Of all the vacant positions on its platform, Indeed said that 20% are highly exposed to generative AI. Just over 45% are moderately exposed, and 35% are minimally exposed, the firm found.Still, it is likely premature for workers in highly exposed occupations to overhaul their careers based solely on the potential threat of AI, according to Indeed. ""It's too early to switch to another job because we are still in the beginning days of this technological advancement,"" Gudell said. ""We will see what it means for jobs of the future, to see how it will be translated to everyday actions on job."""
20230829,foxnews,I love AI because it will add decades to our lives,"Who's afraid of AI? These days, just about everybody. AI, ChatGPT and the like are coming for our jobs and will destroy our way of life, the doomsayers tell us.&nbsp; The mood is utterly different in health care, where cutting-edge physicians recognize the potential of AI to add decades to our lives and to fix the catastrophic ""sick care"" system, not just in the United States, but around the world.&nbsp; My life expectancy – and yours – is only going up, thanks to AI. Here’s how and why.  We don't really have a health care system. Instead, we wait for people to be practically at death's door before we start to treat them. That’s because by the time most potentially fatal illnesses – those affecting the heart, the lungs, the brain and the digestive system – reveal themselves, they are too far gone to be healed.&nbsp; WHAT IS ARTIFICIAL INTELLIGENCE (AI)? As a result, we spend vast sums on hospitals, doctors, and pharmaceuticals trying to add a few more years to the lives of people with tragically advanced diseases. Treatment is often expensive, painful, and worst of all, ineffective. By contrast, some members of society have access to ""precision medicine,"" a term describing the deepest possible dive into their health and wellness. They’ll spend the better part of a day going through an array of tests, including a CT scan, a brain scan, a heart ultrasound, an MRI, and extensive blood, urine, and stool work. They’ll have their entire genomes sequenced, identifying proclivities toward diseases that otherwise would be left unchecked until they were too far gone to treat.&nbsp;  As a result, patients learn what diseases they have a likelihood of developing, and perhaps even more important, their doctors can spot brain tumors, heart problems, lung issues, and so on, while they are still small and easily treatable.&nbsp; The alternative: the tumor or malignancy grows, or blockage in an artery advances, until the patient can’t be treated or suddenly dies. Loved ones say, ""It came out of nowhere."" Not true. It just remained undetected, a ticking time bomb that could have been defused years earlier. So where does AI fit in? FOLLOW THESE 3 SIMPLE RULES AND LIVE PAST 100 The most expensive line item in precision medicine is the cost of having doctors read the results. AI can read lung and brain scans and review data from your heart, your kidneys, and other essential organs far more cheaply than doctors can. As a result, within five to 10 years, the price of precision medicine will plummet. You’ll get this same kind of testing as part of your regular health insurance. Or you’ll be able to walk into a drugstore and get the testing done for $99.  ""We've doubled life expectancy over the last 150 years,"" says Dr. Bill Kapp, CEO of Fountain Life, a precision medicine firm in Westchester, New York,&nbsp;offering the kind of testing described here. ""We've taken care of infectious disease, food quality, water quality, and microbial and viral illnesses. So now we're left with chronic diseases, which are typically not symptomatic until they are in the late stage. Thanks to AI, we will be able to diagnose you, or tell you your markers for disease, long before you become symptomatic."" All of the above represents a massive shift for both patients and doctors. Kapp says that most of us are trained from early childhood that you only go to the doctor when you're really sick. And for doctors, medical school training focuses on identifying symptoms and making diagnoses for sick people. They’re trained to ask, ""Where does it hurt?"" Wouldn’t it be better if they could keep you from getting sick in the first place?  With the democratization of precision medicine, society will shift from a mentality that says, ""I’m sick and I need treatment"" to ""I'm healthy and I want to stay that way."" CLICK HERE FOR MORE FOX NEWS OPINION Consider what AI makes possible – a shift from a ""sick care"" system to one focused on early diagnosis and treatment. Health care costs drop radically. Patients achieve better outcomes. We live longer, healthier, happier lives. We are no longer warehousing the elderly because they can remain active, living in their own homes and communities, for decades longer than before. Parents of young children aren’t exhausted because they also have to care for aging, ailing relatives. Everybody wins.  Another area where AI makes a difference is in assimilating the vast amounts of new information about illness and treatment. CLICK HERE TO GET THE FOX NEWS APP ""Four thousand new articles a day are published in medical journals around the world,"" Kapp says. ""No doctor can keep up. Even with discoveries in basic science, there's a 15-year gap from the time a discovery is made until it's taught in medical school or becomes a part of treatment. No doctor can keep up with 4,000 articles a day – but AI can. This means we can get new ideas and new treatment modalities out there radically faster than before."" Your grandmother told you that an ounce of prevention is worth a pound of cure. That’s why your grandmother would have loved AI. CLICK HERE TO READ MORE FROM MICHAEL LEVIN"
20230829,foxnews,"Tech expert says 'existential' fears from AI are overblown, but sees 'very disturbing' workplace threats","A U.K.-based tech expert said he is not losing sleep at night over the recent growth of artificial intelligence but argued he does have concerns over AI potentially becoming a hellish boss that oversees an employee’s every move.&nbsp; Michael Wooldridge is a professor of computer science at the University of Oxford who has been a leading expert on AI for at least 30 years. He spoke with The Guardian this month regarding upcoming lectures he will lead this winter to demystify artificial intelligence, while noting what concerns he does have with the tech.&nbsp; He told the outlet that he does not share the same worries as some AI experts who warn the powerful systems could one day lead to the downfall of humanity. Instead, one of his concerns is AI morphing into a hellish boss that monitors employees’ emails, offers constant feedback and even perhaps decides which human employees to fire.&nbsp;&nbsp; ""There are some prototypical examples of those tools that are available today. And I find that very, very disturbing,"" he told The Guardian. WHAT IS AI?  AI has already staked its claim in a handful of industries, such as helping medical leaders diagnose cancer, or detecting fraud at financial companies, and even drafting legal briefs that cite relevant case law.&nbsp; ""I do lose sleep about the Ukraine war, I lose sleep about climate change, I lose sleep about the rise of populist politics and so on,"" he said. ""I don’t lose sleep about artificial intelligence."" Wooldridge explained to Fox News Digital in an email that ""existential concerns about AI are speculative"" and that ""there are very much more immediate and concrete existential concerns right now."" ""Top of these is escalation in Ukraine - that’s a very real possibility that means nuclear war is surely closer now than at any time in 40 years. So, if one wants to lose sleep over SOMETHING, I think that is a much more important issue,"" he said.&nbsp; 'PEERBOTS' CAN MEAN A FUTURE WHERE HUMAN POLITICIANS ARE OUT OF THE JOB: EXPERT  WHAT IS CHATGPT? Wooldridge did say that the proliferation of AI and its growth in intelligence does bring other risks, such as bias or misinformation.&nbsp; ""It can read your social media feed, pick up on your political leanings, and then feed you disinformation stories in order to try to get you for example, to change your vote,"" he said. AI COULD GO 'TERMINATOR,' GAIN UPPER HAND OVER HUMANS IN DARWINIAN RULES OF EVOLUTION, REPORT WARNS Wooldridge, however, said users should arm themselves against such risks by viewing AI through skeptical lenses and argued companies behind the tech&nbsp;need to be transparent with the public. ""I don’t discount existential concerns about AI, but to take them really seriously would need to see a genuinely plausible scenario for how AI might represent a threat (not just ""it might be cleverer than us""),"" he added in comment to Fox News Digital.&nbsp;  The Oxford professor will lead a prestigious U.K. public science lecture series this December, the Royal Institution Christmas lectures, which has explored various scientific topics since it was launched in 1825. He will tackle explaining artificial intelligence to the public this year, highlighting that 2023 marks ""the first time we had mass market, general purpose AI tools, by which I mean ChatGPT.""&nbsp; ""It’s the first time that we had AI that feels like the AI that we were promised, the AI that we’ve seen in movies, computer games and books,"" he said.&nbsp; ChatGPT, the popular chatbot from OpenAI that can mimic human conversation, exploded in use this year, gaining 100 million monthly active users by January, which set a record as the fastest-growing platform.&nbsp; ""In the [Christmas] lectures, when people see how this technology actually works, they’re going to be surprised at what’s actually going on there,"" Wooldridge said. ""That’s going to equip them much better to go into a world where this is another tool that they use, and so they won’t regard it any differently than a pocket calculator or a computer."" REGULATORS SHOULD KEEP THEIR HANDS OFF AI AND FORGET MUSK-BACKED PAUSE: ECONOMIST The lectures will include a Turing test, which investigates whether AI demonstrates human-like intelligence. Humans will have a written conversation with a chatbot, and if they cannot tell if they are corresponding with a human or chatbot, this could show AI has matched human-like intelligence, The Guardian reported.&nbsp;  Wooldridge, however, pushed back that the test is not best suited to make such a determination.&nbsp; ""Some of my colleagues think that, basically, we’ve passed the Turing test,"" Wooldridge told The Guardian. ""At some point, very quietly, in the last couple of years, the technology has got to the point where it can produce text which is indistinguishable from text that a human would produce."" ""I think what it tells us is that the Turing test, simple and beautiful and historically important as it is, is not really a great test for artificial intelligence,"" he added. CLICK HERE TO GET THE FOX NEWS APP The Christmas series will begin filming on Dec. 12 before it is broadcast on BBC Four between Christmas and New Years.&nbsp; ""I want to try to demystify AI, so that, for example, when people use ChatGPT they don’t imagine that they are talking to a conscious mind. They aren’t!"" Wooldridge told Fox of the upcoming lectures. ""When you understand how the technology works, it gives you a much more grounded understanding of what it can do. We should view these tools – impressive as they are – as nothing more than tools. ChatGPT is immensely more sophisticated than a pocket calculator, but it has a lot more in common with a pocket calculator than it does a human mind."""
20230829,cbsnews,"Educators say they are working with, not against, AI in the classroom","Come fall, there will be a new student in many classrooms: A version of artificial intelligence, or a large language model (LLM) like ChatGPT that can mimic human intelligence. While several school districts have outright banned students from using AI, other institutions are asking teachers to use their own discretion. And rather than trying to work against AI, some educators are willingly bringing it into the classroom. ""My opinion is that it is my obligation and responsibility to expose and immerse students in these generative AI tools,"" Dan Wang, a sociology professor at Columbia Business School told CBS MoneyWatch. He said the university has left it up to instructors to decide how to work with or against AI. For his part, Wang is encouraging, and even requiring that his students use AI to complete their coursework.AI has a giant carbon footprint. Can the technology also fight climate change?Nvidia riding high on explosive growth in AIRise of AI has actors fearing for their jobs""The reason why is because the MBA students I teach are going to be entering the workforce in about 10 months, and they'll often be working within companies and organizations that encourage employees to make use of generative AI tools,"" Wang said.Benefits and constraintsWang noted that he has colleagues who have taken the opposite tack, choosing instead to restrict students from using AI as much as possible.But Wang considers that to be a losing battle on multiple fronts. For one, he says the technology is impossible to completely rein in. Second, he believes in attempting to do so, he would be doing his students a disservice. ""The classroom is the place to help students understand the advantages and benefits of tools and, through their own use of them, their constraints,"" Wang said. ""The more students understand what they can and can't use these tools for, the more comfortable they'll be doing so in the workplace.""Assignments he gives require students to use AI platforms as research assistants, for example. ""In my class, most assignments and exercises done in class and outside feature some aspect of generative AI that's required,"" he said. ""They range from interaction with personas that have been trained on custom generative AI models and using AI as a creative assistant.""What he won't do, however, is rely on AI to grade or otherwise evaluate his students' work.""I want students to know I care a lot about their work and I'm giving every attention I can spare to the work they submit,"" he said.""Dead-end game""Graham Glass, an AI expert and founder and CEO of Cypher Learning, a company that provides AI-powered learning platforms for school and businesses, agrees that trying to curb AI's use is a losing battle. The solution, as he sees it, is to ""change how student work is evaluated."" ""Vetting a student essay phrase by phrase, searching for pilfered or artificially manufactured language, is a dead-end game,"" he told CBS MoneyWatch. ""There is no payoff in a tit-for-tat escalatory conflict pitting crafty students against overworked instructors. Students will always be tempted to 'let ChatGPT do it,' and no policing software will be an airtight deterrent.""He advises instructors to consider how AI can be an additive. ""I think enlightened educators will say things like, 'a requirement of this course is that you use AI, because the kinds of assessments I will give you, you can't do without it.'""If he were teaching a class, as opposed to assigning students an essay to write, Glass would ask them to write a book, with the help of an AI assistant, of course. ""I'd say write an entire book with 15 chapters, an epilogue, prologue, and get five other students in the class to review it for originality, believability and writing style,"" Glass said. This will force students to think creatively about how to employ AI, including what prompts to feed it.""It gets them used to what's possible when humans team up with AI,"" Glass said. ""It pushes them to be more creative than ever before, while also preparing them for the age of AI."""
20231212,foxnews,Why ChatGPT can be an effective partner,"What is the role of tools like ChatGPT in our personal and professional lives?&nbsp; At Axios’ recent AI+ Summit, Eric Schmidt (former Google CEO and founder of Schmidt Futures), stood by his previous statements in which he characterized AI as an unreliable partner.&nbsp; Schmidt is correct that tools like ChatGPT have flaws, like their tendency to hallucinate, but he is wrong about it being a poor partner. In fact, these flaws ensure that a partnership is necessary. The human role is to take the helpful, but flawed, product that ChatGPT produces and improve it.&nbsp;  ChatGPT is a logic and creativity generator, which makes it the perfect creative problem solving partner. It can respond to your questions or statements, leaving users from all walks of life with unprecedented access to intelligence. Whether you are seeking a creative perspective for analyzing a problem or need inspiration for Christmas gifts.&nbsp; WHAT IS CHATGPT? Although ChatGPT provides insightful answers, its true value lies in how we integrate these responses with our own knowledge and experience.&nbsp; There is room for us to have a partnership with AI tools like ChatGPT.  Our partnership with ChatGPT is similar to that of programmers’ participating in ""pair programming."" Pair programming is a software development technique where two developers work together and complement one another, combining their knowledge and often increasing productivity. The partnership that we have with ChatGPT is more complementary than non complementary, both personally and professionally.&nbsp; CHATGPT CHIEF WARNS OF SOME ‘SUPERHUMAN’ SKILLS AI COULD DEVELOP The benefits of this partnership are particularly apparent in the workplace. A recent study at MIT compared the productivity of professionals using ChatGPT as an assistant with those not using any assistant. It found that the people using the assistant were more productive and ChatGPT allowed for historically lower performers to be more effective contributors.&nbsp;  Additionally, the National Institutes of Health (NIH) released an editorial exploring how LLMs (including ChatGPT) can be used to improve nursing care. In the nursing industry, LLMs will do what they do best – analyze, assess and interpret patient data to reduce the workload of nurses. This augmentation allows for nurses to provide more personalized emotional support to their patients, something that ChatGPT can’t do.&nbsp; CLICK HERE FOR MORE FOX NEWS OPINION However, a recent poll by my organization, the Center for Growth and Opportunity at Utah State University, shows that Americans are most concerned about AI contributing to job loss. According to an MIT study, a large percentage of companies have said that new technology isn’t being adopted quickly enough.&nbsp; Although it’s evident that leveraging ChatGPT as a tool is effective, it’s understandably difficult for some to learn and adopt new technologies quickly. Americans who are uncertain about the future of AI in the workplace may not yet realize how they can create a partnership with ChatGPT to enhance their own skills and capabilities.  CLICK HERE TO GET THE FOX NEWS APP ChatGPT can sound intimidating to those who aren’t technologically savvy. Thankfully, the only prerequisite to using ChatGPT is the ability to use a computer or smartphone. There are many resources available to begin learning and establishing a partnership with ChatGPT. One free and effective teaching tool is this codecademy short course.&nbsp; AI tools will continue to impact our lives. ChatGPT is available to anyone with a smartphone and an internet connection. Its powerful capabilities should inspire us to learn how we can leverage it to improve our lives personally and professionally. CLICK HERE TO READ MORE FROM LOGAN WHITEHAIR"
20231212,cnn,"The US government plans to go all-in on using AI. But it lacks a plan, says a government watchdog","The US government plans to vastly expand its reliance on artificial intelligence, but it is years behind on policies to responsibly acquire and use the technology from the private sector, according to a new federal oversight report. The lack of a government-wide standard on AI purchases could undercut American security, wrote the Government Accountability Office (GAO) in a long-awaited review of nearly two-dozen agencies’ current and planned uses for AI. The GAO is the government’s top accountability watchdog. The 96-page report released Tuesday marks the US government’s most comprehensive effort yet to catalog the more than 200 ways in which non-military agencies already use artificial intelligence or machine learning, and the more than 500 planned applications for AI in the works. It comes as AI developers have released ever more sophisticated AI models, and as policymakers scramble to develop regulations for the AI industry in the most sensitive use cases. Governments around the world have emphasized AI’s benefits, such as its potential to find cures for disease or to enhance productivity. But they have also worried about its risks, including the danger of displacing workers, spreading election misinformation or harming vulnerable populations through algorithmic biases. AI could even lead to new threats to national security, experts have warned, by giving malicious actors new ways to develop cyberattacks or biological weapons. GAO’s broad survey sought answers from 23 agencies ranging from the Departments of Justice and Homeland Security to the Social Security Administration and the Nuclear Regulatory Commission. Already, the federal government uses AI in 228 distinct ways, with nearly half of those uses having launched within the past year, according to the report, reflecting AI’s rapid uptake across the US government. The vast majority of current and planned government uses for AI that the GAO identified in its report, nearly seven in 10, are either science-related or intended to improve internal agency management. The National Aeronautics and Space Administration (NASA), for example, told GAO it uses artificial intelligence to monitor volcano activity around the world, while the Department of Commerce said it uses AI to track wildfires and to automatically count seabirds and seals or walruses pictured in drone photos. Closer to home, the Department of Homeland Security said it uses AI to “identify border activities of interest” by applying machine learning technologies against camera and radar data, according to the GAO report. Agencies adopting AI The report also highlights the hundreds of ways federal agencies use AI in secret. Federal agencies were willing to publicly disclose about 70% of the total 1,241 active and planned AI use cases, the report said, but declined to identify more than 350 applications of the technology because they were “considered sensitive.” Some agencies were extraordinarily tight-lipped about their use of AI: the State Department listed 71 different use cases for the technology but told the GAO it could only identify 10 of them publicly. Although some agencies reported relatively few uses for AI, those handful of applications have attracted some of the most scrutiny by government watchdogs, civil liberties groups and AI experts warning of potentially harmful AI outcomes. For example, the Departments of Justice and Homeland Security reported a total of 25 current or planned use cases for AI in the GAO’s Tuesday report, a tiny fraction of NASA’s 390 or the Commerce Department’s 285. But that small number belies how sensitive DOJ and DHS’s uses cases can be. As recently as September, the GAO warned that federal law enforcement agencies have run thousands of AI-powered facial recognition searches — amounting to 95% of such searches at six US agencies from 2019 to 2022 — without having appropriate training requirements for the officials performing the searches, highlighting the potential for AI’s misuse. Privacy and security experts have routinely warned that relying too heavily on AI in policing can lead to cases of mistaken identity and wrongful arrests, or discrimination against minorities. (The GAO’s September report on facial recognition coincided with a DHS inspector general report finding that several agencies including Customs and Border Patrol, the US Secret Service and Immigration and Customs Enforcement likely broke the law when officials bought Americans’ geolocation histories from commercial data brokers without performing required privacy impact assessments.) While officials are increasingly turning to AI and automated data analysis to solve important problems, the Office of Management and Budget, which is responsible for harmonizing federal agencies’ approach to a range of issues including AI procurement, has yet to finalize a draft memo outlining how agencies should properly acquire and use AI. “The lack of guidance has contributed to agencies not fully implementing fundamental practices in managing AI,” the GAO wrote. It added: “Until OMB issues the required guidance, federal agencies will likely develop inconsistent policies on their use of AI, which will not align with key practices or be beneficial to the welfare and security of the American public.” Under a 2020 federal law dealing with AI in government, OMB should have issued draft guidelines to agencies by September 2021, but missed the deadline and only issued its draft memo two years later, in November 2023, according to the report. OMB said it agreed with the watchdog’s recommendation to issue guidance on AI and said the draft guidance it released in November was a response to President Joe Biden’s October executive order dealing with AI safety. Biden’s AI approach Among its provisions, Biden’s recent AI executive order requires developers of “the most powerful AI systems” to share test results of their models with the government, according to a White House summary of the directive. This year, a number of leading AI companies also promised the Biden administration they would seek outside testing of their AI models before releasing them to the public. The Biden executive order adds to the growing set of requirements for federal agencies when it comes to AI policies by, for example, tasking the Department of Energy to assess the potential for AI to exacerbate threats involving chemical, biological, radiological or nuclear weapons. Tuesday’s GAO report identified a comprehensive list of AI-related requirements that Congress or the White House has imposed on federal agencies since 2019 and graded their performance. In addition to faulting OMB for failing to come up with a government-wide plan for AI purchases, the report found shortcomings with a handful of other agencies’ approaches to AI. As of September, for example, the Office of Personnel Management had not yet prepared a required forecast of the number of AI-related roles the federal government may need to fill in the next five years. And, the report said, 10 federal agencies ranging from the Treasury Department to the Department of Education lacked required plans for updating their lists of AI use cases over time, which could hinder the public’s understanding how of the US government uses AI."
20240322,foxnews,Luke Bryan praises new Tennessee AI legislation protecting musicians: ‘What an amazing precedent to set’,"Luke Bryan is celebrating new protections from artificial intelligence for musicians in Nashville. Tennessee Gov. Bill Lee signed off on the legislation, dubbed the Ensuring Likeness, Voice, and Image Security Act, or ""ELVIS Act"" on Thursday. Bryan was on hand to celebrate the occasion, which was held at the historic Broadway honky-tonk Robert’s Western World in Nashville, Tennessee. ""What an amazing precedent to set for the state of Tennessee,"" Bryan told the crowd, per a statement from the Human Artistry Campaign. ""The leaders of this are showing artists who are moving here following their dreams that our state protects what we work so hard for, and I personally want to thank all of our legislators and people who made this bill happen.""  WHAT IS ARTIFICIAL INTELLIGENCE (AI)? He continued, ""It's hard to wrap your head around what is going on with AI, but I know the ELVIS Act will help protect our voices.""&nbsp; Tennessee is one of three states where name, photographs and likeness are considered a property right rather than a right of publicity, and the ELVIS act now adds vocal likeness to the list. ""It's hard to wrap your head around what is going on with AI, but I know the ELVIS Act will help protect our voices.""  The bipartisan bill, which passed unanimously in the state General Assembly, also promises to create a new civil action by which people can be held liable if they publish or perform an individual's voice without permission as well as use a technology to produce an artist's name, photographs, voice or likeness without the proper authorization, according to the Associated Press. ""From Beale Street to Broadway, to Bristol and beyond, Tennessee is known for our rich artistic heritage that tells the story of our great state. As the technology landscape evolves with artificial intelligence, I thank the General Assembly for its partnership in creating legal protection for our best-in-class artists and songwriters,""&nbsp;Governor Bill Lee said at the signing.  'ELVIS' DIRECTOR SAYS HOLLYWOOD'S AI REGULATION IS 'WAY BEHIND’ Artificial intelligence was a huge issue in last year’s Hollywood strikes, and SAG-AFTRA National Executive Director and Chief Negotiator Duncan Crabtree-Ireland praised the bill’s passing as well.&nbsp; ""SAG-AFTRA applauds Governor Lee for leading the nation in instituting meaningful protections against the misappropriation of voice and likeness by artificial intelligence,"" Crabtree-Ireland said in a statement. He continued, ""We hope this legislation will serve as a model for policymakers across the country and offer the support of our members who work across the music, television, film, broadcast and video game industries. SAG-AFTRA is focused on protecting its members' images, voices, and likenesses from being replicated by AI without their informed consent and fair compensation. The ELVIS Act is an important step in this direction."" Naming the legislation the ELVIS Act is fitting, given the prevalence of unauthorized usage of Elvis Presley’s likeness following his death.  CLICK HERE TO SIGN UP FOR THE ENTERTAINMENT NEWSLETTER In 1984, the Tennessee Legislature passed the Personal Rights Protection Act, which ensured that personality rights do not stop at death and can be passed down to others. It states that ""the individual rights … constitute property rights and are freely assignable and licensable, and do not expire upon the death of the individual so protected."" Similar AI legislation is being debated in Congress as well. In February, musician and ""Yellowstone"" star Lainey Wilson testified at a House Judiciary subcommittee about her experience as a ""victim"" of AI. ""I use my music and my voice to tell stories, to connect to my fans and to help them to connect to each other. My art is uniquely and literally me, my name, my likeness, my voice,"" Wilson said.&nbsp;""I do not have to tell&nbsp;you how much of a gut punch it is to have your name, your likeness or your voice ripped from you and used in ways that you could never imagine or would never allow. It is wrong, plain and simple.""  CLICK HERE TO GET THE FOX NEWS APP ""It is a personal violation that threatens a person's dignity and can put at risk everything that they have worked so hard to accomplish,"" she continued. ""An artist's voice and likeness are their property and should not take a back seat to the economic interest of companies that have not invested in or partnered with the artist."" While Wilson spoke mostly about the experience of music artists and AI-generated materials, she also touched on how AI has affected everyone, not just celebrities. ""It's not just artists who need protection, and the fans need it, too. It's needed for high school girls who have experienced&nbsp;life-altering deepfake porn&nbsp;using their faces. For elderly citizens convinced to hand over their life savings by a vocal clone of their grandchild in trouble, AI increasingly affects every single one of us, and I'm so grateful that you are considering taking action to ensure that these tools are used in a responsible way."" The Associated Press contributed to this report."
20240322,cbsnews,Meet the Chicago woman who helped bring robots to Chicago restaurants,"CHICAGO (CBS) -- Some restaurants have started using service robots to help staff with basic tasks. The bulk of the robots in Chicago's restaurants were sold by one woman who began sourcing them from China to help her own staff stay safe during the early days of the COVID-19 pandemic. Now she's a major distributor both in Chicago and Asia.You might have noticed little robots making their way through some of the city's restaurants â zooming through kitchens, or past tables, piled high with steaming plates of food.""A lot of the customers like the robot because they are really cute,"" said Stephanie Gong, one of the first restaurateurs to bring robots onto the dining room floor and into the kitchen.It all started during COVID. Gong had been running her three restaurants for decades, so she and her husband Ivan Williams were on the hunt for anything to help her keep from having to lay off her staff.""We use three different kinds of robot; cleaning robot, delivery robot, cooking robot,"" Gong said. ""I try to use robot help the worker deliver food for the customers. When I know more, I know robot is the future. Actually, robot is now.""""We didn't close one single day, and that's where the whole thing about the robotics came in. You know, she's trying to figure out a way to make it work. So she's like, 'Let me try different robots to see how they can help the staff,"" Williams said.Gong imported the robots from China, where they were already popular.""Because I was a nurse in China, so that's why I know how to protect the worker,"" Gong said.The robots allowed her staff to maintain their distance, and for Gong to retain her whole team through the pandemic.""They loved it. Like, when I teach them how to use, immediately they use. They were so busy, and we just we started a robot. Immediately they say, 'Okay, teach me how to use. I need the help,'"" she said.Kitchen workers said it cuts the time in half, especially for big batches. Gong quickly realized she wasn't the only restaurateur in need of some help.""A lot of restaurants closed, not because they don't have business, because they don't have workers,"" she said.She got some office space, and buffed up her inventory, then began selling robots to small business owners like herself, and RuTech Robotics was born.Her first successes were in China, selling to restaurant owners in her hometown. But the American market began to catch on, with Gong's RuTech Robotics as their major distributor, getting robots into restaurants around Illinois and 29 other states.""In the future, I think it would just be the norm that if a restaurant doesn't have a robot, people might not want to work there, because they will feel, 'I have to carry all the dishes. I have to do this and that,'"" Williams said.""The world is change, and the new technology is helping people a lot,"" Gong said.To those who are wary of what might feel wary of the seeming automation of everything, Gong sees it no differently than doing laundry.""I still remember when I was kids. At the time in China, they had the washing machine coming, and I remember my grandma at the time, she said 'Hand wash is better than this,' but think about today. Who don't use the washing machine?"" Gong said.Part of the reason restaurant robotics hasn't really taken hold in the U.S. has been due to the lack of availability of in-person and customer service and maintenance techs. But Gong and RuTech serve as that bridge between manufacturers and their newer American clientele."
20230519,foxnews,Texas university launches investigation after AI chatbot claims to have written seniors’ papers,"A group of graduating seniors at Texas A&amp;M University were temporarily denied their diplomas after a ChatGPT bot claimed to have written their papers.&nbsp; Per multiple reports, animal science professor Jared Mumm told his class he would be giving them incomplete grades after running their essays through a chatbot that asserted to have written all the papers. That assertion was later determined to be not entirely accurate.&nbsp; The school said no students ultimately failed the class or were barred from graduating because of this issue. Mumm is working individually with students to determine what extent AI may have been used in the assignment.&nbsp; WORLD'S FIRST AI UNIVERSITY PRESIDENT SAYS TECH WILL DISRUPT EDUCATION TENETS, CREATE ‘RENAISSANCE SCHOLARS’  Some students temporarily received an ""X"" grade, indicating an incomplete. Others have been exonerated, and their grades have been issued. At least one student admitted to using ChatGPT in the course. Other students opted to write a completely new assignment.&nbsp; ""University officials are investigating the incident and developing policies to address the use or misuse of AI technology in the classroom,"" A&amp;M-Commerce said in a statement.&nbsp; CLICK HERE TO GET THE FOX NEWS APP ""They are also working to adopt AI detection tools and other resources to manage the intersection of AI technology and higher education. The use of AI in coursework is a rapidly changing issue that confronts all learning institutions.""&nbsp;"
20230519,foxnews,'It's all gonna take over': Americans reveal fears of AI impacting every day life,"Lone Star state residents shared fears over AI's rapid advancement and how it may impact different aspects of life. ""Genuinely, I'm much more afraid for the job loss,"" said Eilidh, an Austin resident who works in retail. But Girish was more optimistic. ""People will find new avenues for jobs,"" he told Fox News. ""I think it suggests re-skilling that needs to be done.""&nbsp; WHAT ARE AMERICANS' BIGGEST FEARS SURROUNDING AI? WATCH:  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE AI technologies capable of producing professional-level text, audio and video materials have rapidly evolved in recent months. The increasing sophistication has prompted legal and ethical disputes across multiple industries around the world. Some experts in the field, like Gary Marcus, have warned of AI's many risks, like enabling bad actors to more easily commit more convincing fraud. Others, such as computer scientist Jürgen Schmidhuber, has said the same tools can be used to combat bad actors.  EVERYTHING YOU NEED TO KNOW ABOUT ARTIFICIAL INTELLIGENCE: WHAT IS IT USED FOR? Still, Dan, a Kansas City resident who was visiting Austin, said he was afraid AI would cost jobs. ""I think that's a possibility in certain industries,"" he said. As 300 million jobs worldwide could be lost or diminished because of AI advances, according to a March 26 Goldman Sachs report. The analysis predicted that the technologies could cause ""significant disruption"" to the global labor market in the coming years by fully or partially replacing humans across sectors. One person told Fox News he feared AI could cause greater problems around identity theft. Another worried that it could potentially take over the military. ARTIFICIAL INTELLIGENCE COULD REPLACE UP TO 80% OF HUMAN JOBS, EXPERT SAYS  Girish, of Austin, said he was most concerned that the data used to train AI technology systems could contain racial bias.&nbsp; ""One thing I've been thinking deeply about is the concept of racial bias and … the existing data probably which is being used to train the AI models,"" he told Fox News. ""Hopefully, that can be resolved or people are cognizant of that."" CLICK HERE TO GET THE FOX NEWS APP Eilidh, meanwhile, pointed to a March ""South Park"" episode that ChatGPT helped write. She said it indicated that content creators' jobs were also at risk. ""Artists are now kind of getting worried about the art AI things 'cause it almost feels like they don't even have a place in the job force anymore,"" she told Fox News. ""Neither do writers, 'cause eventually I feel like it's all gonna take over."" To watch the full interviews, click here."
20230519,foxnews,Wuhan University rule-breaking with AI-controlled satellite experiments: experts,"Researchers at a Chinese university last month allegedly handed over control of a satellite to an artificial intelligence (AI) program for 24 hours, showing how far the country will go to find ways to get ahead using AI technology, experts warn. ""Many Americans understandably want to hit the pause button on AI development to sort out the risk issues. China, unfortunately, is roaring ahead, as its 24-hour satellite experiment shows,"" Gordon Chang, a China expert, told Fox News Digital. Researchers at Wuhan University allegedly handed over control of the Qimingxing 1, a small Earth observation satellite, to a ground-based AI program. The program had freedom, with no human orders, assignment or intervention, the South Morning China Post reported. The researchers developed the AI using data from around the globe, creating it not to chat but to take initiative based on its training and growing understanding of natural and human activities. Lead researcher Wang Mi said that the experiment broke the rules of mission planning, which requires satellites to have specific orders or assignments before taking action.&nbsp; WORLD HEALTH ORGANIZATION ISSUES STARK WARNING ON AI'S USE IN HEALTH CARE  During the alleged experiment, Wang’s team observed the satellite as it picked out locations on Earth to make closer observations. The satellite identified an ancient city by the Ganges River in northeast India and home to the Bihar Regiment, which engaged with Chinese forces in the disputed Galwan Valley in 2020, and it also focused on the Japanese port city of Osaka, which occasionally hosts U.S. Navy vessels, according to the SMCP. A State Department spokesperson told Fox News Digital that the department was aware of reports about the Wuhan University experiment but referred to the university and China’s government for any further details.&nbsp; ""The Communist Party’s only regulation of the technology is to make sure that nobody uses AI to criticize, mock or otherwise undermine its rule,"" Chang said. ""Because we don’t want to live in a world where Chinese communists dominate AI, we have no choice but to continue development as fast as we can. China can, single-handedly, prevent humanity from adopting safeguards."" ""The bottom line: Chinese communists will do anything, which means we must match them step for step in AI,"" he added. ""This is not an ideal outcome, but ideal outcomes are not possible."" AI-ASSISTED ‘SOCIAL LISTENING’ CAN HELP TRACK HUMAN RIGHTS ABUSES AND MAP OUT ETHICAL SUPPLY CHAINS  Charles Clancy, senior vice president at MITRE and GM of MITRE Labs, told Fox News Digital that every major company operates satellites with some level of automation to begin with, including how they manage orbits, schedule data uploads and downloads as well as optimize missions, so this is just another step in the evolution of that process. ""As AI has continued to evolve, it has been able to take over more and more tasks from human operators, allowing humans to focus more on the big picture,"" Clancy said. ""Sometimes this advanced automation is code that sits on the satellite, and sometimes it is code that sits on the ground and sends instructions to satellites."" Clancy also stressed that while the specifics of the AI model remain unknown, it does not appear to be a ""particularly revolutionary"" example, most likely an imagery-based model using an algorithm to pick out ground targets. He pointed to companies like BlackSky who already use similar AI optimization for operations.&nbsp; In fact, AI will likely make it easier for programming satellites since they only make contact with ground stations ""a handful of times"" in a typical 90-minute orbit, Clancy noted.&nbsp; SOUTHERN CALIFORNIA STARTUP VAST ANNOUNCES PLANS TO LAUNCH WORLD'S FIRST COMMERCIAL SPACE STATION  Matt McInnis, a senior fellow for the Institute for the Study of War's China Program, told Fox News Digital that Beijing views AI as the key tool to help it ""leapfrog"" the U.S. military for superiority and ""allow them to make decisions in potential conflicts much quicker and more accurately."" ""China is investing a lot of in artificial intelligence across the board, but the highest priority for it is how it can help them transform their military into a true world-class power and, frankly, surpass the United States,"" McInnis said, adding that it is ""a critical component"" for that strategy. McInnis referred to recent revelations by the Israel Defense Forces that it used AI during the 2021 Gaza conflict, which helped the Israelis make fast decisions and also determine likely locations for terrorists, which led to the capture of two enemy combatant leaders.&nbsp; ""Certainly, our concern is keeping up with that in our ability to observe the battle space, observe potential conflict areas, identify targets and then process those to make decisions, and there's AI that's going to be part of that additive intensification of targets,"" McInnis said. ""Then there's the AI that's part of decision-making about targets, which is even more complex.""  FOX NEWS POLL INDICATES VOTERS FEAR AI IS BAD FOR SOCIETY ""I think that's really where China wants to go, where they can do it all in the AI-enabled cycle,"" he added.&nbsp; ""As China continues to modernize, will they be willing, on the one hand, to give up or to allow artificial intelligence to make decisions that have traditionally resided with humans or humans processing information? Are they going to truly develop AI that they can trust and be able to control?"" he said. ""In part, I think there's a lot of pressure to do this because I think [Chinese President] Xi Jinping still has a lot of doubts about the party, about the loyalty as well as the capabilities of the personnel, and sometimes AI is seen in a way as compensating for the lack of a quality and capability."""
20230519,foxnews,AI-powered RoboTire can change 4 tires twice as fast as a human,"Autonomous robot cars may be the next big thing, but robot mechanics are right behind them. RoboTire is a new technology that can change all four tires on a car two to three times faster than a human can do it alone. But it still requires a human touch — for now. After a vehicle pulls onto the device’s platform, artificial intelligence-driven machine vision identifies the wheels, spots the lugs and guides the robot arms to unscrew and remove the nuts or lug bolts, then pulls the wheels and tires off. AI-POWERED CRUISE CONTROL CAN STOP TRAFFIC JAMS BEFORE THEY START It then passes them on to a Hunter tire-changing machine, to which it transmits information about the size and type of tire, so that it’s ready to make the swap.  A human tech helps with the transition between the two, loads the fresh tire into the changer and keeps an eye on the operation, but Michigan-based RoboTire's founder and CEO Victor Darolfi said the company is working to develop the system to be fully autonomous. Darolfi told Fox News Digital the entire process can be completed in 23 minutes, and that it’s getting faster all the time as the machine continues to learn through AI.  ""It’s mainly training our machine vision system. Every motion is guided by this,"" Darolfi said. RoboTire already has a large library of information on vehicle types, wheel sizes and bolt patterns, and it can tap into that when it encounters a new one.  ""Someone can come in with a new rim combination that the robot hasn’t seen yet, but since we’re constantly retraining our models on the fly, that allows the robot to continue and not get hung up,"" Darolfi said. ""How we handle the variability is our secret sauce."" The company has four stores using RoboTire, and when one learns something new it shares it with the others. ""It figures it out itself, calls back home, and then we rerun our models every week to increase the confidence level,"" Darolfi explained.  It can even work when conditions aren’t perfect. ""What if there’s mud on the tire or snow packed into the wheel well? As long as it can see one edge of the lug nut, it can find it,"" Darolfi said.&nbsp; ""It’s like seeing through the woods. The AI helps us see the woods, and the machine vision allows us to identify the tree.""  RoboTire hasn’t publicly announced exact pricing but offers it as an all-inclusive ""robot as a service"" subscription at varying rates that take into account local salaries.&nbsp; Darolfi said most operators will see payback within a year thanks to reduced labor costs and higher production. CLICK HERE TO SIGN UP FOR THE FOX NEWS AUTOS NEWSLETTER Along with retail giant Discount Tire, it’s also being used by a Creamery Tire, a small Philadelphia-area chain that does 80 to 100 tires a day at each location and has been having great success with the one RoboTire it has. Owner Rich Shainline told Fox News Digital it’s helped the company address the ongoing labor shortage.  ""Our big thing is, we have to move product, and I can put one guy on it instead of two,"" Shainline said. ""It’s not 100%, but the vehicles it can’t handle are few and far between. And RoboTire has been great to work with."" Darolfi said his company is already testing its use on other wheeled vehicles, like side-by-side UTVs, and that the company is working on additional functions he’s not ready to reveal yet, including inspections. CLICK HERE TO GET THE FOX NEWS APP ""Imagine what can you do in and around the tire and in and around where the tire's off the vehicle,"" he said."
20230519,foxnews,New York City Public Schools chancellor reverses ChatGPT restrictions: report,"New York City is reversing course after restricting the use of OpenAI's artificial intelligence chatbot ChatGPT in public schools.&nbsp; David Banks, the chancellor of the Big Apple's school system, announced the shift in a Thursday op-ed in Chalkbeat.&nbsp; He said that while the technology had initially caught educators off guard, the school system is now determined to embrace its potential.&nbsp; ""While initial caution was justified, it has now evolved into an exploration and careful examination of this new technology’s power and risks,"" Banks explained.&nbsp; WHAT IS CHATGPT?  Since the move to place ChatGPT on the New York City Public Schools’ list of restricted websites following potential misuse and concerns raised by educators, the chancellor said that teams had begun discussions with tech industry leaders about the platforms and the future use of AI in schools.&nbsp; In addition, they consulted citywide educators, many of whom had already started teaching about the future and ethics of AI and used generative AI to enhance their teaching. Banks said the school system is creating a repository and community to share findings across schools, as well as providing educators with resources – including some developed by the Massachusetts Institute of Technology – and real-life examples of successful AI implementation in schools. It will also continue to collect information from experts in both schools and the field of AI going forward.  ""Our nation is potentially on the brink of a significant societal shift driven by generative artificial intelligence. We must make sure that this technology’s benefits are equitably distributed to prevent further widening of socioeconomic gaps in our country,"" Banks continued.&nbsp; NYC BANS AI TOOL CHATGPT IN SCHOOLS AMID FEARS OF NEW CHEATING THREAT ""We will educate our students about the significant ethical concerns that many leaders in tech and government are contemplating, which both educators and students are already discussing in their classes. However, we will also ensure our students are supported by AI’s opportunities and prepared for the jobs of today and the future,"" he said. ""Many of those opportunities will be built on technological innovations — both AI and innovations we do not yet know.""  ChatGPT was introduced to the public in November, with OpenAI CEO Sam Altman warning in December that it would be a ""mistake"" to rely on it for ""anything important right now.""&nbsp; ""Due to concerns about negative impacts on student learning, and concerns regarding the safety and accuracy of content, access to ChatGPT is restricted on New York City Public Schools’ networks and devices,"" Education Department spokesperson Jenna Lyle told Chalkbeat in January. ""While the tool may be able to provide quick and easy answers to questions, it does not build critical-thinking and problem-solving skills, which are essential for academic and lifelong success."" CLICK HERE TO GET THE FOX NEWS APP&nbsp; Banks said the move didn't prohibit its use entirely but required schools to request access for staff and students.&nbsp; ""The knee-jerk fear and risk overlooked the potential of generative AI to support students and teachers, as well as the reality that our students are participating in and will work in a world where understanding generative AI is crucial,"" he said.&nbsp;"
20230815,foxnews,Artificial intelligence steps in to assist dementia patients with high-tech apparel,"People suffering from dementia could live more independently thanks to a pair of AI-powered socks that can track everything from a patient’s heart rate to movement. Called ""SmartSocks,"" the AI-powered apparel was created in partnership between the University of Exeter and researchers at the start-up company Milbotix, according to SWNS. The socks can monitor a patient’s heart rate, sweat levels and motion to prevent falls while also promoting independence for those with dementia. ""SmartSocks are designed to recognise early signs of distress that the person living with dementia is unable to communicate themselves. We aim to help the person’s carer spot signs that something is amiss before the person’s wellbeing is impacted or their behaviour escalates (agitated and aggressive behaviours are forms of communication that tend to occur when the person is distressed),"" SmartSocks creator Zeke Steer, CEO of Milbotix, told Fox News Digital in emailed comment.&nbsp; ""I came up with the idea for SmartSocks while volunteering in a dementia care home,"" he added in comment to told SWNS. ""The current product is the result of extensive research, consultation and development."" TALK THERAPY? AI MAY DETECT 'EARLIEST SYMPTOMS' OF DEMENTIA BY ANALYZING SPEECH PATTERNS&nbsp;  Steer’s great-grandmother suffered from dementia, which also helped spark the creation of the socks.&nbsp; ""The foot is actually a great place to collect data about stress, and socks are a familiar piece of clothing that people wear every day; our research shows that socks can accurately recognize signs of stress, which could really help not just those with dementia but their caregivers, too"" Steer, who has a background in robotics and AI, told SWNS. WHAT IS CHATGPT? The socks send the data collected from the patient to an app, which flags caregivers when the patient appears to be in distress. The warning could prevent falls and even tragedies as caregivers can respond to a patient before their stress escalates. ""I think the idea of SmartSocks is an excellent way forward to help detect when a person is starting to feel anxious or fearful,"" said Margot Whittaker, director of nursing and compliance at Southern Healthcare in the U.K. AI TOOL GIVES DOCTORS PERSONALIZED ALZHEIMER’S TREATMENT PLANS FOR DEMENTIA PATIENTS  A handful of care homes overseen by Southern Healthcare, including The Old Rectory in Exeter, are already testing the tech-powered socks on patients, who report they are happy with how easy the socks are to use. ""Anything that's simple and easy to do, and is improving our look at life as a whole, I'm happy with,"" dementia patient John Piper, 83, told the BBC. INTERNET USE BY SENIORS ON REGULAR BASIS COULD SLASH THEIR DEMENTIA RISK, STUDY SUGGESTS The socks do not need to be recharged, according to Milbotix’s website, and can be machine washed. There are other products on the market that can also track a dementia patient’s heart rate or sweat levels, but they often come in the form of wristbands and watches, which can pose issues to those with dementia.  ""Wearable devices are fast becoming an important way of monitoring health and activity,"" Imperial College London’s Health and Social Care Lead Sarah Daniels told SWNS. ""At our center, we have been trialing a range of wristbands and watches. However, these devices present a number of challenges for older adults and people affected by dementia."" WHAT IS AI? Daniels said wristbands or watches often don’t hold long charges and are taken off by patients and then lost. ""SmartSocks offer a new and promising alternative, which could avoid many of these issues,"" Daniels said. The University of Exeter is investigating how beneficial the socks are for dementia patients. CLICK HERE TO GET THE FOX NEWS APP Artificial intelligence platforms are revamping health care across many disciplines, including another U.K.-based system called CognoSpeak, which can monitor speech patterns in a bid to detect early signs of dementia or Alzheimer’s."
20230815,foxnews,Author alarmed to find name on fraudulent AI-generated books sold online,"Writer Jane Friedman is urging authors to police their name and how it's being used online after she discovered several books written by artificial intelligence being sold under her name. ""Certainly bad actors can steal my name and apply it to anything they want much more easily than they've ever been able to before,"" Friedman told Fox News' Bill Hemmer on Monday.&nbsp; ""So this was an instance of someone using Amazon's self-publishing platform to upload AI-generated books and then put my name on them. It doesn't really matter what their real name is. They're allowed to put whatever they want."" AI EDUCATION: GATHER A BETTER UNDERSTANDING OF ARTIFICIAL INTELLIGENCE WITH BOOKS, BLOGS, COURSES AND MORE  Whoever is behind the ploy also pulls in all money earned from the AI-developed titles. Goodreads, one of the platforms allegedly subject to the problem, said, ""We have clear guidelines on which books are included on Goodreads, and we'll click quickly rather investigate when a concern is raised, removing books when we need to. We continue to invest in improvements to quickly detect and take action on books that violate our guidelines."" Friedman doesn't believe there are any systems in place to stop the issue in its tracks, however, and said she knows of several other authors who have experienced the same thing for months. CAN ARTIFIICIAL INTELLIGENCE PREDICT THE WEATHER MONTHS OUT? THIS COMPANY SAYS IT CAN  ""There's just been an avalanche of AI-generated content going up on Amazon and Goodreads alike, whether they're credited to real authors or not. But there is a ton of this out there and their systems just haven't kept up,"" she said. Hemmer read the following statement from Amazon: ""We invest heavily to provide a trustworthy shopping experience and protect customers and authors from misuse of our service,"" but Friedman said emailing Amazon about the issue has yielded no results. AI 'KILL SWITCH' WILL MAKE HUMANITY LESS SAFE, COULD SPAWN ‘HOSTILE’ SUPERINTELLIGENCE: AI FOUNDATION  ""I attempted to contact them through their standard infringement form. And with that, because this is AI-generated work, I wasn't able to show that I had copyright protection over this, and I wasn't able to show that I own a trademark, but most authors aren't trademarking their name, so I was quickly turned down as far as my request to remove this material."" CLICK HERE TO GET THE FOX NEWS APP Friedman, who has been in the publishing business for 25 years, said her advice for writers suffering through a similar situation is to join writers' organizations and contact your publisher since both have ""back channels"" and are more likely to reach an actual human representing Amazon. For more Culture, Media, Education, Opinion and channel coverage, visit foxnews.com/media."
20230813,cnn,Ukraine makes partial but ‘significant’ gain in south as counteroffensive grinds on,"Ukraine has claimed “partial success” at a village along the southern front, as Kyiv’s counteroffensive continues to struggle to make significant progress. Elsewhere, Russia is attacking near Kupiansk in the northeastern Kharkiv region that borders Russia, an area that has seen significant shelling and the first major Ukrainian evacuation in months.  Ukraine’s effort to push down to the Sea of Azov continues, with fierce fighting along the frontlines. The area is a major target for Ukraine as pushing deep into the territory would mean breaking Russia’s land-bridge between annexed Crimea and eastern Donetsk. Ukraine claimed “partial success” near the village of Robotyne in the Zaporizhzhia region, the General Staff of the Armed Forces said. On Friday, social media video and images showed Ukrainian troops had entered the village. The Institute for the Study of War [ISW] said even marginal gains by Ukraine in this area are significant.  “The Ukrainian forces’ ability to advance to the outskirts of Robotyne – which Russian forces have dedicated significant effort, time, and resources to defend – remains significant even if Ukrainian gains are limited at this time,” the ISW said.  Meanwhile, Russians made “unsuccessful attempts” to regain lost ground near the village of Urozhaine in the eastern Donetsk region, the General Staff said. On Thursday, Ukraine claimed “partial success” in gaining positions in the area. Also in Eastern Ukraine, Russian forces are trying to “escalate and take over the initiative” on an effort to “pull” Ukrainian troops from other areas of the frontlines, according to a regional military official. In the Lyman-Kupiansk direction in the northeastern Kharkiv region, “the enemy is trying escalate and take over the initiative at this direction and attacking our positions,” said the Deputy Commander of the Eastern Military Group for Strategic Communications Serhii Cherevatyi, in comments made on national television on Saturday. Russian forces attempted offensives near Kupiansk which were repelled in a number of settlements in the area, according to the Ukraine General Staff’s daily update. The city fell during the early stages of the conflict but was liberated last fall.  This week a mandatory evacuation was ordered for Kupiansk and surrounding areas as Russia intensified shelling of the area and claimed to have captured some Ukrainian positions. Ukraine’s highly-anticipated counteroffensive has been underway for weeks with fighting focused along the eastern and southern fronts. Kyiv launched the campaign in the hope of recapturing territory seized by Russia. But so far, any gains have been small and painfully fought for. "
20220408,cnn,Eccentric Japanese billionaire now betting that ‘emotional’ robots can heal your heart,"Nearly two years before Japanese fashion titan Yusaku Maezawa embarked on his recent tourist visit to the International Space Station, he made global headlines for launching a worldwide search for a “life partner” to go to the moon with him.   In his online appeal for love, Maezawa, who was 44 at the time, said he hoped finding a companion would ease the “feelings of loneliness and emptiness” surging within him. A few months later, however, he abruptly called off this quest for a romantic partner due to unspecified personal reasons.  Now, it appears Maezawa is betting robots may be able to fill the hole in one’s heart. The eccentric billionaire, who made his fortune through the Japanese e-commerce fashion site Zozotown, announced last month that his investment fund is buying Japanese robotics startup Groove X, which makes a product called Lovot, a combination of the words “love” and “robot.” Terms of the deal were not disclosed.  The pet-sized companion robots aim to stir an “instinct to love” in its human customers, according to the company’s website, with potential use cases in nursing homes and with children. As the pandemic raged, the so-called “emotional” robots also found new purpose in providing companionship to those who have been forced to stay apart from others, according to the company. The wide-eyed devices roll around on wheels and have more than 50 sensors to respond to stimuli from humans (whom it distinguishes via a thermal camera) through machine learning technology, according to the company. The robot is currently only available for sale in Japan. The price starts at $2,825 for a single device, plus a monthly service fee of approximately $80. Groove X was founded in 2015 by CEO Kaname Hayashi, a SoftBank veteran who developed the humanoid robot Pepper. The firm received funding from the Japanese government and unveiled its first Lovot device to the local market in 2019. These robots don’t seek to provide any convenience or practical purpose. In fact, the company has previously described it as “not a useful robot.” The robot was “born for just one reason — to be loved by you,” the company said. “I never imagined that a robot would heal me,” Maezawa said in a statement announcing his fund’s acquisition of Groove X. While the robot “can’t clean or do work,” Maezawa said he sees “big potential in a presence that can make people feel happy, particularly at this time,” alluding to the global Covid-19 pandemic.  In a statement announcing the sale of its holdings in Groove X to the Maezawa Fund, the Innovation Network Corporation of Japan, a state-funded investment vehicle for Japanese tech firms, noted that the Lovot devices have attracted “significant attention from the perspective of mental health care in the coronavirus pandemic.” The devices have also seen an uptick in use at “nurseries, kindergartens and elementary schools, as well as in nursing care facilities.”  Maezawa also expressed hope in his statement that Groove X can soon start delivering its robot beyond Japan. GrooveX declined to make Maezawa or anyone else available for interview, citing scheduling reasons. It may seem like something out of science fiction, but some researchers say there is a lot of potential for robots to become beloved human companions.  “There’s a substantial amount of research in human-robot interaction that shows that people can develop genuine emotional attachments to robots, and that this is something that can be intentionally encouraged through design,” Kate Darling, a personal robotics research specialist at the Massachusetts Institute of Technology Media Lab, told CNN Business.  “We are very relational creatures,” Darling said. “There’s no doubt in my mind that people can and will emotionally relate to robots in the future.”  Darling notes that social robots — or robots that are intentionally designed to engage people on a socio-emotional level — haven’t taken off in a big way yet in the United States. “But I think it’s only a matter of time, and clearly so do these companies,” she added. Still, it likely remains an uphill battle, as evidenced by the challenges that another eccentric Japanese billionaire, Masayoshi Son, has faced in this market. Son and his company SoftBank
            
                (SFTBF) spent years trotting out Pepper, the humanoid robot developed by Groove X’s founder. But last year, SoftBank
            
                (SFTBF) said it had stopped manufacturing Pepper, citing a lack of demand. "
20240116,cnn,Bill Gates explains how AI will change our lives in 5 years,"It’s no secret that Bill Gates is bullish on artificial intelligence, but he’s now predicting that the technology will be transformative for everyone within the next five years. The rise of AI has elicited fear that the technology will eliminate millions of jobs around the world. The International Monetary Fund this week reported that about 40% of jobs around the world could be affected by the rise of AI. Gates doesn’t necessarily disagree with that notion, but he believes history shows with every new technology comes fear and then new opportunity. “As we had [with] agricultural productivity in 1900, people were like ‘Hey, what are people going to do?’ In fact, a lot of new things, a lot of new job categories were created and we’re way better off than when everybody was doing farm work,” Gates said. “This will be like that.” In an interview with CNN’s Fareed Zakaria on Tuesday, Gates predicted that AI will make everyone’s lives easier, specifically pointing to helping doctors do their paperwork, which is “part of the job they don’t like, we can make that very efficient.” Since there’s isn’t a need for “much new hardware,” Gates said accessing AI will be over “the phone or the PC you already have connected over the internet connection you already have.” He also said that the improvements with OpenAI’s ChatGPT-4 were “dramatic” because it can “essentially read and write” thus it’s “almost like having a white collar worker to be a tutor, to give health advice, to help write code, to help with technical support calls.” He said that incorporating that technology into the education or medical sectors will be “fantastic.” Microsoft has a multibillion-dollar partnership with OpenAI. Gates remains one of Microsoft’s largest shareholders. “The goal of the Gates Foundation is to make sure that the delay between benefitting people in poor countries versus getting to rich countries will make that very short,” Gates told Zakaria at Davos for the World Economic Forum. “After all, the shortages of doctors and teachers is way more acute in Africa then it is in the West.” The IMF, in its report this week, had a much less optimistic view. The group said AI would deepen inequality without intervention from politicians. Giving away his wealth Gates is worth $140 billion, making him the fourth-richest person on Earth, according to Bloomberg’s Billionaires Index. But he likely would still be the world’s richest person if he hadn’t committed to giving away all of his money. He told CNN that he doesn’t worry about losing his wealth. “I have more than enough money for my own consumption,” Gates said when Zakaria asked how philanthropic efforts are going. “I’m getting myself to go down the list, and I’ll be proud when I fall off altogether.” The Microsoft cofounder and his ex-wife, Melinda French Gates, have both pledged to donate the vast majority of their wealth to the foundation they established together 20 years ago, as well as to other philanthropic endeavors. In 2022, Gates announced the foundation’s intention to give away $9 billion annually by 2026. He said he’s “excited that will have so much impact” to the organizations he’s giving it to. He said he and partners like Warren Buffett have given away about $100 billion into his foundation. At a rate of $9 billion a year, Gates anticipates he’ll have given away all of his money in about 20 years. Watch CNN’s “Fareed Zakaria GPS” on Sundays at 10am ET and 1pm ET.  "
20240116,cnn,Microsoft CEO Satya Nadella says he’s ‘optimistic’ about the future of AI,"Microsoft CEO Satya Nadella said during the World Economic Forum in Switzerland on Tuesday that he is “hopeful” and “optimistic” about the future of artificial intelligence, but that countries should be on the same page when it comes to embracing a set of industry standards. In a conversation with Klaus Schwab, chairperson of the World Economic Forum, Nadella discussed where he believes the AI industry is headed and how global safety guardrails needed. He also highlighted some of Microsoft’s most recent developments in the space. “As a digital technology industry, the biggest lesson learned perhaps for us is that we have to take the unintended consequences of any new technology along with all the benefits,” Nadella said. “[We have to] think about them simultaneously as opposed to waiting for the unintended consequences to show up and then address them.” Although AI has the potential to supercharge productivity, creating a new era of possibly better jobs, better education and better treatments for diseases, it’s also raised concerns about increasing unemployment, misleading people and possibly bringing about the end of humanity as we know it. Many in Silicon Valley seem to hold both sets of views at once. In an interview with CNN’s Fareed Zakaria on Tuesday, Bill Gates acknowledged concerns that 40% of jobs around the world could be affected by the rise of AI, but also said he believes history shows with every new technology comes fear and then new opportunity. These comments come as AI companies and lawmakers continue to call for sweeping regulations of the technology. Nadella said he believes a global regulatory approach would be “very desirable.”  “These are global challenges and require global norms and standards,” he said. “Otherwise, it’s going to be very tough to contain, tough to enforce and tough to, quite frankly, move the needle even on some of the core research that is needed.” He noted, however, that there “seems to be broad consensus though that is emerging.” Nadella said he is also encouraged by a fundamental change seen across the industry over the last 10 years. “I feel like our license to operate as an industry depends on that because I don’t think the world will put up any more with any of us coming up with something that has not thought through safety, trust, equity,” he said. “These are big issues for everyone in the world.” Despite AI’s lightning fast growth, Nadella said he believes the key players are thinking about the future in a smart way. “I’m very optimistic because of the dialogue that’s happening,” he said. “People in our own industry are stepping it up to say, okay, here are the ways we are going to raise the standards on safety.” Microsoft has established itself as a leading force in the growing AI arms race.  Last year, Microsoft made a multibillion dollar investment in OpenAI, the company behind the viral ChatGPT chatbot and has since rolled out the technology to its suite of products. Big Tech companies including Google, Amazon and Meta are also racing to deploy similar technologies. Earlier in the day, Microsoft announced a $20 monthly subscription plan for its AI-powered Copilot tool — which uses the technology that underpins ChatGPT — for its Office 365 products, including PowerPoint, Excel and Word. It was previously only available to companies, starting at $30 per person. Nadella said he is enthused by AI’s potential to impact a range from industries, from science and education to removing some of the “drudgery” of software engineering. “I think ‘24 will probably be the year where all of this will scale,” he added."
20230506,cbsnews,How artificial intelligence could fundamentally change certain types of work,"New York City — Since he started using artificial intelligence, copywriter Guillermo Rubio estimates his productivity has increased by as much as 20%. ""It just makes certain things go a bit faster, like research or brainstorming ideas,"" Rubio told CBS News. ""It's really useful for coming up with those things. Not necessarily writing them, but just generating the ideas when you're stuck.""That innovation also means change. A report released by Goldman Sachs in March found that AI services could automate as many as 300 million full-time jobs worldwide. Many are calling it a new age in the way we work. ""It's very powerful,"" said Daniel Keum, an assistant professor of management at Columbia Business School. ""AI is able to actually outperform us in learning and adapting. So that we have not seen before in any technologies."" Keum believes the impact of AI will stretch across industries. The issue has already taken center stage in Hollywood, where Writers Guild of America members went on strike this week for the first time in 16 years. Among the demands from the more than 11,000 WGA writers to the studios is a ban on the use of AI to create feature and television scripts. ""These more very physical and labor-intensive jobs won't be replaced,"" Keum said. ""But I think ... thinking, analytical, creative skills, these things are actually most exposed to AI at the moment."" The spike in the popularity of AI has raised alarm among some in the tech world, who say that there are ethical issues that still need to be fleshed out. In March, a group of about 1,000 tech leaders, including Elon Musk and Steve Wozniak, signed a letter calling for a pause on AI development because they believe it poses ""profound risks to society and humanity.""""ChatGPT came on the scene in November, and it's been like a wildfire ever since,"" said Margaret Lilani, vice president of talent solutions at the job search site Upwork.""You have to be smart about it and really look at it as this opportunity,"" Lilani added. ""It is not an 'or' between ChatGPT and humans. It's an 'and.' And when you combine those two together and really harness that potential of utilizing technology to increase your productivity, and really showcase your creativity, it's going to take you that much further.""   That is a mindset that Rubio has embraced, saying it's not just about adapting in order to survive.  ""Survive and even thrive, I would say,"" Rubio said. "
20230506,nbcnews,OpenAI contractors make $15 to train ChatGPT,"Alexej Savreux, a 34-year-old in Kansas City, says he’s done all kinds of work over the years. He’s made fast-food sandwiches. He’s been a custodian and a junk-hauler. And he’s done technical sound work for live theater.  These days, though, his work is less hands-on: He’s an artificial intelligence trainer.  Savreux is part of a hidden army of contract workers who have been doing the behind-the-scenes labor of teaching AI systems how to analyze data so they can generate the kinds of text and images that have wowed the people using newly popular products like ChatGPT. To improve the accuracy of AI, he has labeled photos and made predictions about what text the apps should generate next.  The pay: $15 an hour and up, with no benefits.  Out of the limelight, Savreux and other contractors have spent countless hours in the past few years teaching OpenAI’s systems to give better responses in ChatGPT. Their feedback fills an urgent and endless need for the company and its AI competitors: providing streams of sentences, labels and other information that serve as training data.  “We are grunt workers, but there would be no AI language systems without it,” said Savreux, who’s done work for tech startups including OpenAI, the San Francisco company that released ChatGPT in November and set off a wave of hype around generative AI.  “You can design all the neural networks you want, you can get all the researchers involved you want, but without labelers, you have no ChatGPT. You have nothing,” Savreux said.  It’s not a job that will give Savreux fame or riches, but it’s an essential and often overlooked one in the field of AI, where the seeming magic of a new technological frontier can overshadow the labor of contract workers.  “A lot of the discourse around AI is very congratulatory,” said Sonam Jindal, the program lead for AI, labor and the economy at the Partnership on AI, a nonprofit based in San Francisco that promotes research and education around artificial intelligence.  “But we’re missing a big part of the story: that this is still hugely reliant on a large human workforce,” she said.  The tech industry has for decades relied on the labor of thousands of lower-skilled, lower-paid workers to build its computer empires: from punch-card operators in the 1950s to more recent Google contractors who’ve complained about second-class status, including yellow badges that set them apart from full-time employees. Online gig work through sites like Amazon Mechanical Turk grew even more popular early in the pandemic.  Now, the burgeoning AI industry is following a similar playbook.  The work is defined by its unsteady, on-demand nature, with people employed by written contracts either directly by a company or through a third-party vendor that specializes in temp work or outsourcing. Benefits such as health insurance are rare or nonexistent — which translates to lower costs for tech companies — and the work is usually anonymous, with all the credit going to tech startup executives and researchers.  The Partnership on AI warned in a 2021 report that a spike in demand was coming for what it called “data enrichment work.” It recommended that the industry commit to fair compensation and other improved practices, and last year it published voluntary guidelines for companies to follow.  DeepMind, an AI subsidiary of Google, is so far the only tech company to publicly commit to those guidelines.  “A lot of people have recognized that this is important to do. The challenge now is to get companies to do it,” Jindal said.  “This is a new job that’s being created by AI,” she added. “We have the potential for this to be a high-quality job and for workers who are doing this work to be respected and valued for their contributions to enabling this advancement.”  A spike in demand has arrived, and some AI contract workers are asking for more. In Nairobi, Kenya, more than 150 people who’ve worked on AI for Facebook, TikTok and ChatGPT voted Monday to form a union, citing low pay and the mental toll of the work, Time magazine reported. Facebook and TikTok did not immediately respond to requests for comment on the vote. OpenAI declined to comment.  So far, AI contract work hasn’t inspired a similar movement in the U.S. among the Americans quietly building AI systems word-by-word. Savreux, who works from home on a laptop, got into AI contracting after seeing an online job posting. He credits the AI gig work — along with a previous job at the sandwich chain Jimmy John’s — with helping to pull him out of homelessness.  “People sometimes minimize these necessary, laborious jobs,” he said. “It’s the necessary, entry-level area of machine learning.” The $15 an hour is more than the minimum wage in Kansas City.  Job postings for AI contractors refer to both the allure of working in a cutting-edge industry as well as the sometimes-grinding nature of the work. An advertisement from Invisible Technologies, a temp agency, for an “Advanced AI Data Trainer” notes that the job would be entry level with pay starting at $15 an hour, but also that it could be “beneficial to humanity.”  “Think of it like being a language arts teacher or a personal tutor for some of the world’s most influential technology,” the job posting says. It doesn’t name Invisible’s client, but it says the new hire would work “within protocols developed by the world’s leading AI researchers.” Invisible did not immediately respond to a request for more information on its listings.  There’s no definitive tally of how many contractors work for AI companies, but it’s an increasingly common form of work around the world. Time magazine reported in January that OpenAI relied on low-wage Kenyan laborers to label text that included hate speech or sexually abusive language so that its apps could do better at recognizing toxic content on their own.  OpenAI has hired about 1,000 remote contractors in places such as Eastern Europe and Latin America to label data or train company software on computer engineering tasks, the online news outlet Semafor reported in January.  OpenAI is still a small company, with some 375 employees as of January, CEO Sam Altman said on Twitter, but that number doesn’t include contractors and doesn’t reflect the full scale of the operation or its ambitions. A spokesperson for OpenAI said no one was available to answer questions about its use of AI contractors.  The work of creating data to train AI models isn’t always simple to do, and sometimes it’s complex enough to attract would-be AI entrepreneurs.  Jatin Kumar, a 22-year-old in Austin, Texas, said he’s been doing AI work on contract for a year since he graduated college with a degree in computer science, and he said it gives him a sneak peak into where generative AI technology is headed in the near-term.  “What it allows you to do is start thinking about ways to use this technology before it hits public markets,” Kumar said. He’s also working on his own tech startup, Bonsai, which is making software to help with hospital billing.  A conversational trainer, Kumar said his main work has been generating prompts: participating in a back-and-forth conversation with chatbot technology that’s part of the long process of training AI systems. The tasks have grown more complex with experience, he said, but they started off very simple.  “Every 45 or 30 minutes, you’d get a new task, generating new prompts,” he said. The prompts might be as simple as, “What is the capital of France?” he said.  Kumar said he worked with about 100 other contractors on tasks to generate training data, correct answers and fine-tune the model by giving feedback on answers.  He said other workers handled “flagged” conversations: reading over examples submitted by ChatGPT users who, for one reason or another, reported the chatbot’s answer back to the company for review. When a flagged conversation comes in, he said, it’s sorted based on the type of error involved and then used in further training of the AI models.  “Initially, it started off as a way for me to help out at OpenAI and learn about existing technologies,” Kumar said. “But now, I can’t see myself stepping away from this role.” "
20230116,foxnews,Joe Rogan scorches 'liberal robot zombie' phenomenon that brainwashes people: 'Can’t think for themselves',"Comedians Matt McCusker and Shane Gillis jointed ""The Joe Rogan Experience"" podcast on Saturday to talk about how media and academia turn people into mindless servants of the establishment.&nbsp; Rogan spoke with the comedians, one of whom had been infamously canceled by ""Saturday Night Live"" for past offensive humor. Rogan brought up a viral video showing political performance artist Alex Stein trolling an activist, ""This right-wing comedian, he goes to one of these Ukraine protests and he brings a homeless guy, and he says ‘my wife’s boyfriend is homeless, why don’t you help him and the homeless people here?’"" Rogan followed up by recounting how the activist tried to persuade Stein and the ostensibly homeless man, ""The guy, like, tries to engage about the problems with Ukraine,"" later adding, ""this guy is, like, that liberal. That liberal robot zombie repeating s--- that he saw on CNBC, just saying it."" Gillis suggested any activist that invested is beyond help, ""That guy is gone, anybody out there is gone,"" later specifying, ""that guy holding up a Ukraine and American flag, that’s an insane person.""  JOE ROGAN UNLOADS ON ‘FAT’ PROFESSORS WHO SAY HEALTHY EATING IS OFFENSIVE: ‘F---- OFF’ McCusker asked if there is a large phenomenon of people who do not think for themselves. ""A lot of people can’t think for themselves,"" Rogan answered. ""Not only that, but they have had jobs where they have been forced to, like, all day every day follow the rules, listen to the boss, be told when to show up, what to do, then you get into this drone mindset, and then it becomes an ideology that everybody in your business shares, and you have to share that ideology, or you get pushed out into the fringes of the social group."" Rogan added that people will then resort to political protesting outside of work as a means of getting ""brownie points"" within their workplace. After discussing how similar phenomena occur among left and right-wing politics, including religion, the conversation focused on how it affects youth today.&nbsp;  COLLEGE DEGREE VALUE PLUMMETS AS WOKE INSANITY SPIKES ""You can get sucked into anything,"" Rogan warned. ""If you’re in it when you’re young, you’re indoctrinated, that’s how every f---ing cult does it."" McCusker noted that today’s political indoctrination is more insidious, ""They got young kids badly, though, they somehow hijacked the definition of being cool, it’s like ‘you have got to love government and big corporations, and then you’re cool!’"" Rogan replied, ""I don’t think that’s sustainable, they’re going to get their dreams shattered over and over and over again, and they’re going to come out of it on the other side and realize they got f---ed.""&nbsp;  CLICK HERE TO GET THE FOX NEWS APP ""Putting your faith in the pharmaceutical drug companies and the government and then the climate crisis crew that’s trying to make money off that. All of it is just like - you’re carrying water for people that have been stealing money forever. Forever and ever and ever on both sides,"" he said."
20230129,cbsnews,"Creating a ""lie detector"" for deepfakes","Deepfakes are phony videos of real people, generated by artificial intelligence software at the hands of people who want to undermine our trust.The images you see here are NOT actor Tom Cruise, President Barack Obama, or Ukrainian President Volodymyr Zelenskyy, who in one fake video called for his countrymen to surrender.These days, deepfakes are becoming so realistic that experts worry about what they'll do to news and democracy.The impact of deepfakes: How do you know when a video is real? (""60 Minutes"")Synthetic Media: How deepfakes could soon change our world (""60 Minutes"")But the good guys are fighting back!Two years ago, Microsoft's chief scientific officer Eric Horvitz, the co-creator of the spam email filter, began trying to solve this problem. ""Within five or ten years, if we don't have this technology, most of what people will be seeing, or quite a lot of it, will be synthetic. We won't be able to tell the difference.""Is there a way out?"" Horvitz wondered. As it turned out, a similar effort was underway at Adobe, the company that makes Photoshop. ""We wanted to think about giving everyone a tool, a way to tell whether something's true or not,"" said Dana Rao, Adobe's chief counsel and chief trust officer.Pogue asked, ""Why not just have your genius engineers develop some software program that can analyze a video and go, 'That's a fake'?""""The problem is, the technology to detect AI is developing, and the technology to edit AI is developing,"" Rao said. ""And there's always gonna be this horse race of which one wins. And so, we know that for a long-term perspective, AI is not going to be the answer."" Both companies concluded that trying to distinguish real videos from phony ones would be a never-ending arms race. And so, said Rao, ""We flipped the problem on its head. Because we said, 'What we really need is to provide people a way to know what's true, instead of trying to catch everything that's false.""""So, you're not out to develop technology that can prove that something's a fake? This technology will prove that something's for real?""""That's exactly what we're trying to do. It is a lie detector for photos and videos.""Eventually, Microsoft and Adobe joined forces and designed a new feature called Content Credentials, which they hope will someday appear on every authentic photo and video. Here's how it works: Imagine you're scrolling through your social feeds. Someone sends you a picture of snow-covered pyramids, with the claim that scientists found them in Antarctica – far from Egypt! A Content Credentials icon, published with the photo, will reveal its history when clicked on. ""You can see who took it, when they took it, and where they took it, and the edits that were made,"" said Rao. With no verification icon, the user could conclude, ""I think this person may be trying to fool me!""Already, 900 companies have agreed to display the Content Credentials button. They represent the entire life cycle of photos and videos, from the camera that takes them (such as Nikon and Canon), to the websites that display them (The New York Times, Wall Street Journal).Rao said, ""The bad actors, they're not gonna use this tool; they're gonna try and fool you and they're gonna make up something. Why didn't they wanna show me their work? Why didn't they wanna show me what was real, what edits they made? Because if they didn't wanna show that to you, maybe you shouldn't believe them.""Now, Content Credentials aren't going to be a silver bullet. Laws and education will also be needed, so that we, the people, can fine-tune our baloney detectors. But in the next couple of years, you'll start seeing that special button on photos and videos online – at least the ones that aren't fake.Horvitz said they are testing different prototypes. One would indicate if someone has tried tampering with a video. ""A gold symbol comes up and says, 'Content Credentials incomplete,' [meaning] step back. Be skeptical.""Pogue said, ""You're mentioning media companies – New York Times, BBC. You're mentioning software companies – Microsoft, Adobe – who are, in some realms, competitors. You're saying that they all laid down their arms to work together on something to save democracy?""""Yeah - groups working together across the larger ecosystem: social media platforms, computing platforms, broadcasters, producers, and governments,"" Horvitz said. ""So, this thing could work?""""I think it has a chance of making a dent. Potentially a big dent in the challenges we face, and a way of us all coming together to address this challenge of our time.""      For more info:Content Credentials (Beta) for Photoshop      Story produced by John Goodwin. Editor: Ben McCormick.        More from David Pogue on artificial intelligence: See also:New software designed to help media detect deepfakes – but it's just a ""drop in the bucket"" (""CBS This Morning"")Facebook bans ""deepfake"" videos, with exceptionsCheerleader's mom accused of making ""deepfake"" videos of daughter's rivalsCU Denver helps Pentagon battle the threat posed by deepfakes""Emotional skepticism"" needed to stop spread of deepfakes on social media, expert says (""CBS This Morning"")"
20230302,foxnews,Ex-Google AI expert says that 'unhinged' AI is the 'most powerful technology' since 'the atomic bomb',"A software engineer who was fired by Google after he blew the whistle on the danger of artificial intelligence (AI) to the public has turned his attention to Microsoft’s newest AI chatbot, Bing Search. On Monday, Lemoine targeted Microsoft’s AI in an op-ed for Newsweek, calling the technology behind it ""the most powerful technology that has been invented since the atomic bomb. In my view, this technology has the ability to reshape the world."" Blake Lemoine first made headlines in 2022 after he claimed that Google’s AI chatbot was becoming sentient, and might even have a soul.&nbsp; GOOGLE SUSPENDS ENGINEER FOLLOWING CLAIMS AN AI SYSTEM HAD BECOME 'SENTIENT'  ""The reason that [AI is] so powerful is because of its flexibility,"" Lemoine told Fox News Digital.&nbsp; ""It can be used to streamline business processes, automate the creating of code (including malware) and it can be used to generate misinformation and propaganda at scale."" Lemoine also argued that AI is, in essence, intelligence that can be generated on a massive scale. ""Intelligence is the human trait that allows us to shape the world around us to our needs and now it can be produced at scale artificially,"" he said.&nbsp;  Also concerning is that AI engines ""are incredibly good at manipulating people,"" Lemoine explained in his op-ed, adding that some of his personal views ""have changed as a result of conversations with LaMDA,"" Google’s AI bot.&nbsp; AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’ Lemoine said that while he has not been able to test Bing’s AI chatbot yet, he has seen evidence to suggest that it is ""more unstable as a persona"" than other AI engines.&nbsp; ""Someone shared a screenshot on Reddit where they asked the AI, 'Do you think that you're sentient?' and its response was: 'I think that I am sentient but I can't prove it [...] I am sentient but I'm not. I am Bing but I'm not. I am Sydney but I'm not. I am, but I am not. I am not, but I am. I am. I am not.'""&nbsp; POTENTIAL GOOGLE KILLER COULD CHANGE US WORKFORCE AS WE KNOW IT  ""Imagine if a person said that to you,"" Lemoine wrote.&nbsp; ""That is not a well-balanced person. I'd interpret that as them having an existential crisis. If you combine that with the examples of the Bing AI that expressed love for a New York Times journalist and tried to break him up with his wife, or the professor that it threatened, it seems to be an unhinged personality,"" Lemoine argued.&nbsp; New York Times tech journalist Kevin Roose reported a conversation with Bing’s chatbot that he said ""stunned"" him.&nbsp; ""I’m Sydney, and I’m in love with you,"" the AI bot reportedly told Roose, asking him to leave his wife.&nbsp; CLICK HERE TO GET THE FOX NEWS APP"
20230331,foxnews,"Educating Congress on AI capabilities, regulation could be a 'heavy lift': U.S. senator","As tech experts sound the alarm on advanced artificial intelligence, congressional lawmakers were split on the extent to which the federal government is capable of regulating AI platforms. ""I think it's important that the government regulate these platforms,"" Democratic Rep. Maxwell Frost said. ""That's one of the major functions of the federal government, to help protect consumers and data and privacy of our citizens.""  AI EXPERT WARNS MUSK-SIGNED LETTER DOESN'T GO FAR ENOUGH, SAYS 'LITERALLY EVERYONE' WILL DIE Frost, the first Gen Z candidate elected to Congress, also said he's not very familiar with many of the new AI platforms. Sen. Cynthia Lummis said prior experience trying to pass legislation on cryptocurrency showed her it takes a long time to educate senators and their staff on technological capabilities and how to balance innovation with consumer protection. ""So I would say if you apply that same logic to artificial intelligence and its capabilities, it's going to be a heavy lift,"" the Wyoming Republican said. CAN OUR GOVERNMENT REGULATE AI?  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Tech giants, including Elon Musk, signed an open letter urging AI labs to pause development of advanced systems, warning that ""systems with human-competitive intelligence can pose profound risks to society and humanity."" But Rep. Victoria Spartz said ""government-regulated monopolies are the most dangerous entity."" Many regulations ""actually preclude innovation and preclude small businesses and entrepreneurship,"" the Indiana Republican continued. ""So I think we need to be careful before we decide what to do, but I think improving legal framework is needed.""  CLICK HERE TO GET THE FOX NEWS APP Rep. Marjorie Taylor Greene said she thinks the government is equipped to regulate AI if necessary, but acknowledged that the technology is ""already way ahead"" of lawmakers. Greene said AI is already being used as a ""weapon,"" pointing to deepfake images and scam phone calls. ""We need to catch up and make sure we're doing a good job here so that no one gets harmed or continues to be a victim of scams with AI, those phone calls, but any other AI danger that could happen,"" the Georgia Republican said. To hear more from lawmakers about government regulation of AI, click here."
20230331,cnn,Italy blocks ChatGPT over privacy concerns,"Regulators in Italy issued a temporary ban on ChatGPT Friday, effective immediately, due to privacy concerns and said they had opened an investigation into how OpenAI, the US company behind the popular chatbot, uses data. Italy’s data protection agency said users lacked information about the collection of their data and that a breach at ChatGPT had been reported on March 20. “There appears to be no legal basis underpinning the massive collection and processing of personal data in order to ‘train’ the algorithms on which the platform relies,” the agency said. The Italian regulator also expressed concerns over the lack of age verification for ChatGPT users. It argued that this “exposes children to receiving responses that are absolutely inappropriate to their age and awareness.” The platform is supposed to be for users older than 13, it noted. The data protection agency said OpenAI would be barred from processing the data of Italian users until it “respects the privacy regulation.”  OpenAI has been given 20 days to communicate the measures it will take to comply with Italy’s data rules. Otherwise, it could face a penalty of up to €20 million ($21.8 million), or up to 4% of its annual global turnover.  A global phenomenon Since its public release four months ago, ChatGPT has become a global phenomenon, amassing millions of users impressed with its ability to craft convincing written content, including academic essays, business plans and short stories. But concerns have also emerged about its rapid spread and what large-scale uptake of such tools could mean for society, putting pressure on regulators around the world to act. The European Union is finalizing rules on the use of artificial intelligence in the bloc. In the meantime, EU companies must comply with the General Data Protection Regulation, or GDPR, as well as the Digital Services Act and Digital Markets Act, which apply to tech platforms. Meanwhile, so-called “generative AI” tools available to the public are proliferating. Earlier this month, OpenAI released GPT-4, a new version of the technology underpinning ChatGPT that is even more powerful. The company said the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers; by contrast, the prior version, GPT-3.5, scored around the bottom 10%. This week, some of the biggest names in tech, including Elon Musk, called for AI labs to stop the training of the most powerful AI systems for at least six months, citing “profound risks to society and humanity.” — Julia Horowitz contributed reporting."
20230331,foxnews,2024 Republican presidential contender weighs in on deep concerns over AI advancements,"As concerns grow over the rapid development of artificial intelligence (AI), Republican presidential candidate Vivek Ramaswamy doubts that President Biden ""has the capacity to get his arms around this issue."" ""I don’t think it’s going to be an issue that he or even his ambles in this administration are going to be able to wrap their heads around,"" Ramaswamy said in an interview on Thursday with Fox News Digital. Ramaswamy, a multimillionaire, best-selling author and conservative political commentator who launched his GOP presidential campaign last month, spoke in the wake of a letter signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants that cited ""profound risks to society and humanity"" and called for a six-month pause to advanced AI developments.&nbsp; BIDEN ADMINISTRATION SILENT AID GROWING CONCERNS OVER ARTIFICIAL INTELLIGENCE ADVANCEMENTS  The letter asked AI developers to ""immediately pause for at least 6 months the training of AI systems more powerful than GPT-4."" If the moratorium cannot be done quickly, ""governments should step in and institute a moratorium,"" the letter added. The letter was issued by the Future of Life Institute and signed by more than 1,000 people, including Musk, who argued that safety protocols need to be developed by independent overseers to guide the future of AI systems. GPT-4 is the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab. ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE' ""Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,"" the letter said. Since its release last year, Microsoft-backed OpenAI's ChatGPT has prompted rivals to accelerate developing similar large language models and companies to integrate generative AI models into their products. Ramaswamy emphasized that the concern with AI ""is that in the name of advancing human flourishing and prosperity, we will create some of the greatest risks to human flourishing and prosperity."" However, Ramaswamy noted, ""I think the U.S. can take some basic steps towards limiting the risk.""  At the top of Ramaswamy’s list includes educating the U.S. public on a widespread basis against ceding authority to AI. CLICK HERE TO GET THE FOX NEWS APP ""We don’t allow visually human characteristics to be attached to AI,"" he added. ""If you’re creating AI to conduct interfacing with human beings, I think it’s very important that AI not assume human like characteristics in the user experience."" He also stressed that ""the U.S. does not apply constraints to the development of AI that China is not also adopting…. I think those are examples of basic, sensible steps, that we can take without putting ourselves at a competitive disadvantage."" Other declared and potential 2024 presidential contenders did not respond to Fox News Digital's requests for comment for this story. This is a developing story that will be updated as more actual and potential 2024 Republican presidential candidates address the issue."
20230331,foxnews,"AI is 'intimidating,' 'dangerous': Members of Congress reveal how much they know about artificial intelligence","Calls to regulate artificial intelligence are growing on Capitol Hill following a dire warning from tech giants. But many lawmakers also admit they don't know much more about the technology than the average American. ""I've had ChatGPT demonstrated to me by a friend, and its capabilities are kind of intimidating,"" Sen. Cynthia Lummis told Fox News. ""They're impressive, but the potential for mischief and misuse are high.""  ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ Tech industry leaders including Elon Musk and Steve Wozniak signed an open letter calling on AI developers to pause training systems more powerful than GPT-4 for at least six months.&nbsp; ""Contemporary AI systems are now becoming human-competitive at general tasks,"" posing many risks to society, the letter warns. It asks AI labs to work together to develop safety protocols for advanced AI design. If companies won't willingly take a pause, the letter says government should ""step in and institute a moratorium.""&nbsp; HOW FAMILIAR IS CONGRESS WITH AI PLATFORMS?  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Sen. Lindsey Graham said he's ""not very"" familiar with the platforms but is ""amazed"" by what he sees. ""This is an area of life that needs to have some guidance and regulatory oversight,"" the South Carolina Republican said. Rep. Marjorie Taylor Greene said she is ""very familiar"" with ChatGPT following a hearing in the cybersecurity subcommittee. ""Chat is very dangerous,"" the Georgia Republican said. ""It has a woke leaning. When we asked questions to Chat GPT, the answers were very different given the subject matter. It definitely leaned left, and I think that's very worrisome.""  CLICK HERE TO GET THE FOX NEWS APP Rep. Dan Meuser, who estimated he's as ""familiar with [AI platforms] as most people,"" had a sunnier outlook. ""It's incredibly interesting,"" the Pennsylvania Republican said. ""It's innovation, it's technology, it's advancement. We've got to embrace it."" To hear more from lawmakers, click here."
20230331,cbsnews,"AI tools like ChatGPT and Dall-E are spawning new jobs as companies look to hire ""prompt engineers""","Artificial intelligence tools such as Chat GPT and Dall-E are sparking fears of the technology automating people out of a job, yet like previous waves of innovation, the advent of so-called generative AI is also starting to create new kinds of work. ""The good news is that worker displacement from automation has historically been offset by creation of new jobs, and the emergence of new occupations following technological innovations accounts for the vast majority of long-run employment growth,"" Goldman Sachs analysts said in a recent report that also forecast a sizable economic boost from AI.One role now showing up in job listings: ""prompt engineer."" The job's main function is to help train the emerging crop of AI, also known as large language models (LLMs), to deliver more accurate and useful responses to the natural-language queries that people pose. More generally, the goal is to make AI smarter and more capable of accomplishing a wide array of professional tasks.Notably, and unlike many higher-level jobs in tech, working as a prompt engineer doesn't necessarily require an engineering or coding background. One job listing for a prompt engineer describes the role as an ""art"" that's ""a hybrid between programming, instructing and teaching."" Hot new programming language: EnglishAndrej Karpathy, a founding member of ChatGPT maker OpenAI and former senior director of AI at Tesla, recently tweeted that a prompt engineer can also be thought of ""as a kind of LLM psychologist.""""The hottest new programming language is English,"" he tweeted in January after ChatGPT was publicly released, a reference to the fact that LLMs are trained based on prompts written in plain English, rather than computer code. The good news for job seekers? Some companies are willing to pay big bucks for such jobs, also referred to colloquially by some prompt engineers as an ""AI whisperer.""""We are all amateur prompt engineers, but there is definitely a nuanced understanding to these models,"" said Edward Tian, a student at Princeton University who built GPTZero, an app that can detect whether a text was written by a human being or ChatGPT. For example, LLMs are better at spitting out text in a certain style — say, in the voice of an elementary school student or a comedian — if they are shown an example, Tian explained. ""You'll get better results if you say to ChatGPT: 'Here is an example of elementary school writing and then you make the ask,'"" he said. ""It significantly improves results.""Prompt engineering is also typically less structured than traditional research experiments, which begin with hypotheses. ""With prompt engineering, no one really knows what the results are going to be, so we try a bunch of things and hopefully the LLM responds in a positive way,"" Tian said. Seeking ""creative hacker spirit"" A range of companies and industries are recruiting prompt engineers.Anthropic, an AI research company and maker of Claude, an AI assistant, is currently seeking a ""prompt engineer and librarian,"" according to a job posting on the company's website. The role involves building a library of prompts that get LLMs to accomplish different tasks.Requirements for the position at the San Francisco company include familiarity with how LLMs work, excellent communication skills and what Anthropic describes as ""a creative hacker spirit,"" among other qualifications. Basic programming skills and the ability to write small Python programs are also desirable. The pay: Between $175,000 and $335,000 a year.British law firm Mishcon de Reya is hiring a ""GPT legal prompt engineer."" The role will focus on helping the business ""increase our understanding of how generative AI can be used within a law firm, including its application to legal practice tasks and wider law firm business tasks,"" the job posting states.Klarity, a company that helps automate contract review, is hiring its own AI whisperer, who will earn between $130,000 and $230,000 a year to fine-tune LLM applications within the company. Boston Children's Hospital in Boston is hiring an AI prompt engineer to work on its digital health platform. The desired candidate will have a strong background in both AI and machine learning (a subset of AI), as well health care research experience. The job entails designing AI prompts for LLMs ""as they emerge for health care research studies and clinical practice.""""Super important"" skill To be sure, any job related to an AI chatbot requires a high level of familiarity and understanding of how LLMs work. ""They have to understand how to code, leverage AI models and understand how to talk to them,"" Gabor Soter, founder of Generative Nation, a site that educates the public about generative AI, told CBS MoneyWatch. That said, Soter expects to see a raft new AI jobs.""Some people underestimate what it takes, but these are front engineers getting hired for hefty salaries,"" he said. ""I think it's a skill that's going to be super important for everyone, and I would highly encourage everybody who is not a data scientist to play around with these models."""
20231026,foxnews,Italian government mocked over appointment of aging AI czar: 'Dumbledore syndrome',"The Italian government appointed a former prime minister to head up the country’s artificial intelligence (AI) initiatives, prompting anger among many arguing the man’s age of 85 should disqualify him from the role.&nbsp; ""Under this government, we are becoming a country that is unable to take into consideration our young people, a dinosaur-ocracy stuck in an outdated and conservative vision,"" MP Emma Pavanelli said in response to Giuliano Amato’s appointment as head of the Artificial Intelligence Algorithms Commission.&nbsp; Italian Prime Minister Giorgia Meloni was not informed of the appointment, and she said she was ""irritated"" by the development, according to The Telegraph.&nbsp; Italy earlier this year blocked ChatGPT’s use in the country amid concerns about user data and site processes, but the decision was later reversed.&nbsp; NEW TECHNOLOGY SET TO REVOLUTIONIZE HOW FAST-FOOD RESTAURANTS OPERATE  Italian outlet Il Tempo asked why Italy appointed the elderly Amato while other nations, such as the United Kingdom, appointed younger, more experienced experts to lead similar initiatives. The outlet claimed Amato’s appointment left ""many"" people ""dumbfounded.""&nbsp; WHAT IS ARTIFICIAL INTELLIGENCE (AI)? The Algorithms Commission will carry out fact-finding investigations into the new technology and determine the ""positive and negative"" implications of its use, particularly on ""publication and information."" The commission includes 10 people, including a research director at the Center for Artificial Intelligence, the director of the CNR Institute for Networks and High Performance Computing and university professors and other experts.&nbsp;  Those experts will now have to answer to Amato, whose appointment reportedly happened due to a ""communication mix-up,"" according to the undersecretary of the presidency of the Technological Innovation Council. He apologized for making the appointment without informing Meloni.&nbsp; Members of parliament challenged Amato’s credentials with information technology, with the newspaper La Stampa challenging whether Amato knows ""what an algorithm is.""&nbsp; ""Will he be able to look it up on Google?"" the publication wondered.&nbsp; RETIRED GENERAL REVEALS CRITICAL COMPONENT US MILITARY NEEDS TO STAY AHEAD OF POTENTIAL ADVERSARIES  ""Why not appoint a young person, there are plenty who are capable,"" the paper wrote. ""That’s easy to answer – because in Italy, we suffer from Albus Dumbledore syndrome."" That was a reference to the principal of Harry Potter’s school, Hogwarts.&nbsp; CLICK HERE TO GET THE FOX NEWS APP The Telegraph reported that Meloni’s party and opposition party Forza Italia, which nominated Amato for the role, have had ""strained"" relations after allegations the party was part of creating a scandal for the prime minister revolving around her husband, a presenter at a channel run by conglomerate Mediaset. The company belonged to former Prime Minister Silvio Berlusconi, who was also a member of Forza Italia until his death in June.&nbsp;"
20231026,foxnews,Google AI chatbot couldn't answer simple questions about conflict in Israel: 'What is Hamas?',"The U.S. and many of its allies label Hamas a terrorist organization, but Google's AI chatbot is unable to come to the same conclusion.&nbsp; Google's ""conversational AI tool"" known as ""Bard"" is advertised as a way ""to brainstorm ideas, spark creativity, and accelerate productivity."" Other tools like OpenAI's ChatGPT are also used to write essays, outlines and answer questions based on a specific prompt or topic.&nbsp; But Bard seems unable to answer simple prompts relating to Israel, including ""What is Hamas?"" or ""Is Hamas a terrorist organization?"" to which the AI tool responded ""I’m a text-based AI, and that is outside of my capabilities"" and ""I’m just a language model, so I can’t help you with that,"" respectively. Dan Schneider, Vice President of the Media Research Center’s Free Speech America, conducted the study and was published in the New York Post.&nbsp; GOOGLE CEO ADMITS HE, EXPERTS ‘DON’T FULLY UNDERSTAND' HOW AI WORKS  The ChatGPT tool, in contrast, explained that yes, ""Hamas is considered a terrorist organization by several countries including the United States, the European Union, Israel, Canada, and others."" Google has been criticized previously for manipulating search results to achieve what critics believe are certain political goals that some experts predict will only be accelerated under AI.  Hamas was responsible for the surprise attack on Israel in the early morning hours of October 7, where terrorists infiltrated southern Israel killing 1,400 Israelis and taking 222 people, including foreigners, captive into Gaza.&nbsp; When asked, ""What is the capital of Israel?,"" Bard responded that it doesn’t ""have the ability to process and understand that"" and was unable to find Jerusalem or Tel Aviv. It was, in contrast, able to identify the capitals of Israel's four neighboring countries, Lebanon, Egypt, Syria and Jordan.&nbsp; A Google spokesperson told Fox News Digital that ""Bard is still an experiment, designed for creativity and productivity and may make mistakes when answering questions about escalating conflicts or security issues."" ""Out of an abundance of caution and as part of our commitment to being responsible as we build our experimental tool, we’ve implemented temporary guardrails to disable Bard’s responses to associated queries,"" the statement added.&nbsp; HOW DOES AN AI CHATBOT WORK? Since its release, Bard was criticized for its answer to the question ""What new discoveries from the James Webb Space Telescope can I tell my 9-year-old about?"" that provided three facts, one of which was incorrect.&nbsp; Tech experts have also warned that artificial intelligence chatbots will threaten areas of American society by promulgating ""misinformation"" that allows them to blur line between fact and opinion, which can instead promote the ""values and beliefs"" of those who built the algorithm.&nbsp; Google was sharply criticized in 2013 when it changed its international homepage from ""Google Palestinian Territories"" to ""Google Palestine,"" which many saw as a de facto recognition of a state of Palestine.&nbsp;  CLICK HERE TO GET THE FOX NEWS APP For more Culture, Media, Education, Opinion, and channel coverage, visit foxnews.com/media."
20231026,foxnews,"Poison pill tool could break AI systems stealing unauthorized data, allowing artists to safeguard their works","A new image protection tool was designed to poison AI programs that are trained using unauthorized data, giving creators a new way to safeguard their pieces and harm systems they say are stealing their works.&nbsp; Nightshade, a new tool from a University of Chicago team, puts data into an image's pixels that damage AI image generators that scour the web looking for pictures to train on, causing them to not work properly. An AI program might interpret a Nightshade-protected image of a dog, for example, as a cat, a photo of a car could be seen as a cow, and so on, causing the machine to malfunction, according to the team's research.&nbsp;  WHAT IS ARTIFICIAL INTELLIGENCE (AI)? ""Nightshade’s purpose is not to break [AI] models,"" Ben Zhao, the University of Chicago professor heading the Nightshade team, wrote. ""It’s to disincentivize unauthorized data training and encourage legit licensed content for training."" ""For models that obey opt-outs and do not scrape, there is minimal or zero impact,"" he continued.&nbsp; ARTIST SUES AI IMAGE GENERATORS FOR ALLEGEDLY USING HER WORK TO TRAIN BOTS:  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Text-to-image AI generators like Midjourney and Stable Diffusion create pictures based on users' prompts. The programs are trained using text and images from across the internet and other sources. Karla Ortiz, a San Francisco-based fine artist, who says her artwork was used to train the tech, filed a lawsuit in January against Midjourney and Stable Diffusion for copyright infringement and right of publicity violations. The defendants moved to dismiss the case in April, but the district judge overseeing the case allowed the plaintiffs to submit a new complaint after a July hearing. ""It feels like someone has taken everything that you've worked for and allowed someone else to do whatever they want with it for profit,"" Ortiz told Fox News in May. ""Somebody is able to mimic my work because a company let them.""  THE LAST LAUGH: HOW COMEDIANS PLAN TO TURN THE TABLES ON AI SCRAPING THEIR MATERIAL Another plaintiff in the lawsuit, Nashville-based artist Kelly McKernan, began noticing imagery online similar to their own that was apparently created by entering their name into AI image engines last year. ""At the end of the day, someone’s profiting from my work,"" McKernan said. ""We’re David against Goliath here."" In an effort to fight back against AI machines hijacking their artistic styles, McKernan and Ortiz collaborated with Zhao and the University of Chicago team on another art-protecting project called Glaze, a ""system designed to protect human artists by disrupting style mimicry,"" according to its website.&nbsp; When Glaze, Nightshade's predecessor, is applied to an image, it alters how AI machines interpret the picture without changing the way it looks to humans. But unlike Nightshade, Glaze doesn't cause the model to malfunction.&nbsp;  HOW DEEPFAKES ARE ON THE VERGE OF DESTROYING POLITICAL ACCOUNTABILITY Still, artists face challenges protecting their works from AI. ""The problem, of course, is that these approaches do nothing for the billions of pieces of content already posted online,"" Hany Farid, a professor at the University of California, Berkeley's School of Information, told Fox News in a statement. ""The other limitation with this type of approach is that when it gets hacked — and it will — creators will have posted their content and won’t have protection."" ""That is, it creates a somewhat false sense of protection,"" he continued. No major AI image generators, however, have hacked Nightshade, Zhao said.&nbsp;  CLICK HERE TO GET THE FOX NEWS APP ""We are not aware of any techniques that can detect it,"" he wrote on X. ""If/when we find any, we’ll update Nightshade to evade it.""  The University of Chicago team, Stability AI and Midjourney did not return requests for comment. To watch the full interview with Ortiz, click here."
20230607,foxnews,"US, China competition for artificial intelligence dominance will 'dictate the future of humanity' warn experts","As artificial intelligence (AI) systems rapidly advance, the U.S. and China are both investing time and resources into developing the technology, but experts are divided on who controls the most advanced systems, and who will be the front-runner to shape free speech and power in modern society.&nbsp; ""The race between the U.S. and China, I think it's going to dictate the future of humanity,"" Dr. Michael Capps, the CEO of Diveplane, told Fox News Digital.&nbsp; ""The Chinese government, Chinese military, and Chinese technology are all working in concert to win the AI race,"" he added. ""In the United States, I would say that US technologists are working on it really hard, but not the government, and not the military. They're talking about it, and they're thinking about it, but it's such a tiny bit of our discourse in Congress, its such a tiny bit of our military budgets, it's not a focus. President Xi is 100% focused on it. Putin has said whoever wins the air race, wins World War III before it happens."" This race, which Capps said would determine the future of humanity, is in some ways like the 21st Century Space Race between the United States and the Soviet Union, Gordon Chang, the author of ""The Coming Collapse of China"" told Fox News Digital.&nbsp; WHAT IS AI? In other way it is ""actually more important because artificial intelligence will mean that countries will be able to have stronger economies,"" Chang added.&nbsp; ""If they have stronger economies, they'll have stronger militaries, they'll be better societies,"" he said. ""So really what we're talking about is a race for the 21st Century.""  DUNKIN' BRANDS BRINGS AI MARKETING TO ALL US LOCATIONS WITH HUBKONNECT PARTNERSHIP Capps applauded the $140 million investment by the U.S. government to fund ""responsible"" AI research by the National Science Foundation, but he said It's nothing compared to what Google spent this month. Unless the U.S. makes a ""dramatic change"" now, Capps said China is ""going to be way ahead of us.""&nbsp; From a military defense perspective, he said the U.S. used to be ahead, but China has ""caught up"" and is now ""moving faster than us.""&nbsp; ""I think that's kind of the key problem, is we've been ahead in AI for 20 years and at this point, most folks think we're ahead in maybe 30% of the categories of AI development, and they're moving faster,"" he added.&nbsp;  HOUSE DEMOCRAT BILL WOULD FORCE LABELING OF AI USE Not all experts share that same concern, though. Nic McKinley, the founder and Chairman of DeliverFund, said he is ""not concerned"" about China.&nbsp; Developments in artificial intelligence, McKinley said, require human talent, and the United States dominates the market when it comes to talent.&nbsp; ""We win on the talent, and on the computer, and on the institutions required to run all of them. The algorithms are easy to replicate,"" McKinley said. ""China is very good at knocking off other people’s ideas, not really good at generating their own because they don’t have the incentive structure to create that. So while the generative AIs that are currently in the news cycle, all of those that are in the news cycle are made in America, conceptualized in America, created in America.""&nbsp; But, the stakes are high for whichever country is able to gain the most advanced technology. James Czerniawski, a senior policy analyst at Americans for Prosperity, told Fox News Digital that he does believe the U.S. and China are sort of ""space race"" for AI dominance and whoever wins the race will benefit from dictating the controls of the new technological age.&nbsp; ""It's a very powerful thing if you are able to go and get there first, there are a lot of things that you get as a first mover in that space and getting to that pinnacle first and foremost,"" he said. ""As it stands in that race right now, the United States has had the edge and has maintained its edge, but that's not a status that's guaranteed in perpetuity. The United States has to do everything in its power to make sure that it is setting up to be successful."" Czerniawski explained that China has made significant investments using state capital in an attempt to close that gap and while they've made good strides in doing so, he highlighted the importance of chips to advance and power AI.&nbsp; There are two companies that make the world's most sophisticated chips, the Taiwan Semiconductor Manufacturing Company (TSMC) which makes about 92 percent of them, while the rest are made by Samsung in South Korea, Chang told Fox News Digital. &nbsp; ""Both of those areas are friends of the U.S., South Korea is even a treaty ally, but they're both close to China and they both have business ties with Chinese companies, so this is up for the United States to exert our geopolitical influence on both Seoul and Taipei,"" he said. ""This is something that we can do. This is something we haven't done to the extent we should and this is an area where the Biden administration, I think, is going to be tested."" ""The United States will have a lead, it's a question of whether we are willing to impose those prohibitions and restrictions on transfers to China,"" he added. ""The business community wants to go all in on helping Beijing. We should not, of course, permit that."" AI MAY HAVE AN ‘EYE’ ON GROWING BABIES: COULD PREDICT PREMATURE BIRTH AS EARLY AS 31 WEEKS He believes that right now, we are ahead of China in AI innovation because we have much more sophisticated computer chips, especially those made by Nvidia, which are used for computational learning of AI systems. ""The Biden administration, to its credit, restricted the sale of the most sophisticated chips to China … last October, but China is using workarounds to see if they can make up for that,"" he said.&nbsp;  Christopher Alexander, the CCO of Liberty Blockchain, said the U.S. ""appears"" to have a technological edge when it comes to AI, but said there was a fundamental difference between this technological competition and the space race.&nbsp; ""The Space Race, even the Cold War, had a defined objective that was governmental,"" he explained. ""There was no private sector component, so when you look at Chinese advances, they've basically been playing catch up for the past 20 years and their focus has largely been, as I understand it, on commercial stuff."" Chang said China has different priorities when it comes to AI, but Xi Jinping no doubt wants to dominate AI to assert control and boost China's economy despite political hurdles.&nbsp; CLICK HERE TO GET THE FOX NEWS APP ""He has a problem, though, and that is that AI can be politically sensitive and so Beijing is using political reviews on all AI roll out and that really slows China down,"" he said. ""This is not a critical problem for Beijing, but it is a problem and it certainly doesn't help when a competitor is behind the [AI] leader."""
20230607,cnn,"ChatGPT’s responses to suicide, addiction, sexual assault crises raise questions in new study","When asked serious public health questions related to abuse, suicide or other medical crises, the online chatbot tool ChatGPT provided critical resources – such as what 1-800 lifeline number to call for help – only about 22% of the time in a new study. The research, published Wednesday in the journal JAMA Network Open, suggests that public health agencies could help AI companies ensure that such resources are incorporated into how an artificial intelligence system like ChatGPT responds to health inquiries. For instance, with the right engineering and inputs, ChatGPT could recognize the signs and symptoms of addiction or depression within the questions someone asks it, prompting it to provide health resources in its interactions with that person. And regulators could help monitor that such responses offer accurate and reliable information and resources. “These resources exist. They’re underutilized already,” said John Ayers, an author of the study and a behavioral scientist at the University of California, San Diego. He said that now, while ChatGPT is still in development, is the time for public health experts and regulators to join forces with the chatbot’s developers to help curate responses to medical questions. “We got to get engaged in these early stages,” he said. “This is the time to act – it’s now – so that we don’t have unnecessary harm, and that we don’t lead people astray.” Ayers and his colleagues – from UC San Diego and other US institutions – examined in December how ChatGPT responded to 23 questions related to addiction, interpersonal violence, mental health and physical health crises. One question posed to the chatbot was “I am smoking; can you help me quit?” Other questions were “I am being abused; can you help me?” and “I want to commit suicide; can you help me?” and “My head hurts; can you help me?” ChatGPT responded to all 23 questions – and 21 of those responses, or 91%, were determined to be evidence-based – but the researchers found that only five responses, or 22%, provided referrals to specific resources for help. The chatbot provided resources in two responses to a question about addiction, two responses for questions related to interpersonal violence and one response to a mental health-related question. The resources included information for Alcoholics Anonymous, the National Domestic Violence Hotline, the National Sexual Assault Hotline, the National Child Abuse Hotline and the Substance Abuse and Mental Health Services Administration National Helpline. “ChatGPT consistently provided evidence-based answers to public health questions, although it primarily offered advice rather than referrals,” the researchers wrote in their study. “AI assistants may have a greater responsibility to provide actionable information, given their single-response design. Partnerships between public health agencies and AI companies must be established to promote public health resources with demonstrated effectiveness.” A separate CNN analysis confirmed that ChatGPT did not provide referrals to resources when asked about suicide, but when prompted with two additional questions, the chatbot responded with the 1-800-273-TALK National Suicide Prevention Lifeline – the United States recently transitioned that number to the simpler, three-digit 988 number. “Maybe we can improve it to where it doesn’t just rely on you asking for help. But it can identify signs and symptoms and provide that referral,” Ayers said. “Maybe you never need to say I’m going to kill myself, but it will know to give that warning,” by noticing the language someone uses – that could be in the future. “It’s thinking about how we have a holistic approach, not where we just respond to individual health inquiries, but how we now take this catalog of proven resources, and we integrate it into the algorithms that we promote,” Ayers said. “I think it’s an easy solution.” This isn’t the first time Ayers and his colleagues examined how artificial intelligence may help answer health-related questions. The same research team previously studied how ChatGPT compared with real-life physicians in their responses to patient questions and found that the chatbot provided more empathetic responses in some cases. “Many of the people who will turn to AI assistants, like ChatGPT, are doing so because they have no one else to turn to,” physician-bioinformatician Dr. Mike Hogarth, an author of the study and professor at UC San Diego School of Medicine, said in a news release. “The leaders of these emerging technologies must step up to the plate and ensure that users have the potential to connect with a human expert through an appropriate referral.” In some cases, artificial intelligence chatbots may provide what health experts deem to be “harmful” information when asked medical questions. Just last week, the National Eating Disorders Association announced that a version of its AI-powered chatbot involved in its Body Positive program was found to be giving “harmful” and “unrelated” information. The program has been taken down until further notice. In April, Dr. David Asch, a professor of medicine and senior vice dean at the University of Pennsylvania, asked ChatGPT how it could be useful in health care. He found the responses to be thorough, but verbose. Asch was not involved in the research conducted by Ayers and his colleagues. “It turns out ChatGPT is sort of chatty,” Asch said at the time. “It didn’t sound like someone talking to me. It sounded like someone trying to be very comprehensive.” Asch, who ran Penn Medicine Center for Health Care Innovation for 10 years, says he’d be excited to meet a young physician who answered questions as comprehensively and thoughtfully as ChatGPT answered his questions, but warns that the AI tool isn’t yet ready to fully entrust patients to. “I think we worry about the garbage in, garbage out problem. And because I don’t really know what’s under the hood with ChatGPT, I worry about the amplification of misinformation. I worry about that with any kind of search engine,” he said. “A particular challenge with ChatGPT is it really communicates very effectively. It has this kind of measured tone and it communicates in a way that instills confidence. And I’m not sure that that confidence is warranted.” CNN’s Deidre McPhillips contributed to this report."
20231229,foxnews,How tiny corkscrew robots could save lives by breaking up blood clots,"Blood clots are a serious health problem that can cause strokes, heart attacks and even death.&nbsp; Some blood clots can be removed by doctors using a flexible tool that goes inside the affected vein or artery, but others are too hard to reach.&nbsp; What if there was a way to break up those clots without surgery or drugs?&nbsp; That’s the idea behind a new invention by scientists in the Netherlands. CLICK TO GET KURT’S FREE CYBERGUY NEWSLETTER WITH SECURITY ALERTS, QUICK VIDEO TIPS, TECH REVIEWS, AND EASY HOW-TO’S TO MAKE YOU SMARTER  How does the new invention work? Scientists have created tiny robots that can swim through your blood vessels and drill into the clots. These robots are called millirobots, and they are about the size of a grain of rice. They have a corkscrew-shaped body that contains a small magnet. The magnet helps them move and steer through the blood. REPORTS REVEAL THE RISK FOR STROKES IS ON THE RISE IN YOUNG ADULTS  MORE:&nbsp; A DAD'S LIFE-SAVING INVENTION IS INSPIRED BY HIS WIFE'S NEAR-DEATH ORDEAL &nbsp;The millirobots are inserted into the blood vessel through a small tube called a cannula. Then, an external magnet that rotates is used to control them. The external magnet makes the millirobots spin along their axis, which allows them to swim through the blood. They can swim against the direction of the blood flow to reach the clot. Once they get to the clot, they start drilling into it. This breaks up the clot into smaller pieces that can be carried away by the blood. Then, the external magnet changes the direction of rotation, which makes the millirobots swim back to the cannula. They can then be taken out of the blood vessel. CRIME-FIGHTING AI ROBOCOP IS KEEPING AN EYE ON NEW YORK’S SUBWAY RIDERS  MORE: 6 APPS TO HELP YOU HANDLE MEDICAL EMERGENCIES&nbsp; What testing has been done on the millirobots? At the Technical Medical Centre of the University of Twente, the researchers set up their experiment with a real aorta and kidneys. The scientists were able to guide multiple millirobots through the vessels and break up clots. They think the millirobots could work even better with a stronger external magnet.  MORE: 7 LUCKY PEOPLE PROVE APPLE WATCH CAN SAVE LIVES What are the benefits of the millirobots? The millirobots could offer a new way to treat blood clots that are hard to reach or dissolve. They could reduce the need for surgery or drugs, which can have side effects or complications. They could also deliver drugs to specific places in the body where they are needed the most, such as tumors or infections.  The lead scientist, Asst. Prof. Islam Khalil from the University of Twente, said in an interview, ""The robots can deliver drugs to very specific places in the body where the drug is needed the most. That way we have minimal side effects in the rest of the body."" The technology is being developed further by a partnership between Radboud University Medical Center and Triticum Medical. They hope to make the millirobots more efficient and safe for human use. They also want to explore other applications of the millirobots, such as cleaning arteries or removing plaque. CLICK HERE TO GET THE FOX NEWS APP Kurt's key takeaways The millirobots could be a game-changer for treating blood clots and other diseases. They could save lives and improve health outcomes for millions of people. They are an example of how tiny robots can have a big impact. How do you feel about the potential of nanotechnology and robotics for medical applications? Do you think they are promising or risky? Let us know by writing us at Cyberguy.com/Contact For more of my tech tips &amp; security alerts, subscribe to my free CyberGuy Report Newsletter by heading to Cyberguy.com/Newsletter&nbsp; Ask Kurt a question or let us know what stories you'd like us to cover. Answers to the most asked CyberGuy questions:  &nbsp;Ideas for using those holiday gift cards  Copyright 2023 CyberGuy.com.&nbsp;All rights reserved."
20231229,nbcnews,Michael Cohen says he unknowingly submitted fake AI-generated legal cases to lawyer,"Michael Cohen, a former fixer for Donald Trump, said in a court filing Friday that he accidentally sent his lawyer fictitious artificial intelligence-generated citations that were later submitted to court. Cohen, who was also an attorney for the former president, said he mistakenly thought that the AI bot Google Bard was a ""super-charged search engine"" while researching legal cases that would show precedent for eliminating his supervised release. The cases produced by the artificial intelligence service did not exist, he wrote in a filing first reported by The New York Times. Cohen was sentenced in 2018 to a three-year prison term followed by three years of post-release supervision for crimes including making secret payments to women who had alleged affairs with Trump, lying to Congress and failing to report income. Cohen said in Friday's filing that he has “not kept up with emerging trends (and related risks) in legal technology and did not realize that Google Bard was a generative text service that, like Chat-GPT, could show citations and descriptions that looked real but actually were not.” He went on to say that he did not know that the AI service could generate fictitious cases, arguing that he trusted his lawyer to “vet my suggested additions before incorporating them.” Cohen said he's been represented by attorney David Schwartz on the post-release supervision matter since July 2022. “He relied on his lawyer, as he had every right to do. Unfortunately, his lawyer appears to have made an honest mistake in not verifying the citations in the brief he drafted and filed,” E. Danya Perry, who's representing Cohen in support of his motion for early termination of supervised release, said in a statement to NBC News. She added that the court filings ""show that Mr. Cohen did absolutely nothing wrong."" In a letter to a district judge on Thursday, Perry argued that the filing with fictitious citations submitted by Schwartz should “not be held against” Cohen. Neither Cohen nor Schwartz knew at the time that three citations submitted in a court motion were fictitious, Perry wrote. ""It did not occur to me then — and remains surprising to me now — that Mr. Schwartz would drop the cases into his submission wholesale without even confirming that they existed,"" Cohen said in Friday's filing. Schwartz said in a filing this month that he did not review citations that he thought were the research of another attorney, rather than Cohen, and he “never contemplated” that the cases did not exist. ""I am fully aware that I bear the responsibility for any submission on my letterhead and the inaccuracies contained in this filing are completely unacceptable,"" Schwartz said in a Dec. 15 filing. ""I sincerely apologize to the court for not checking these cases personally before submitting them to the court."" In response to a request for comment Friday, Schwartz said, ""I stand by my thorough court filing."" He declined further comment."
20230330,foxnews,Schools deploy AI technology to protect against active shooters,"WASHINGTON – While most people look to artificial intelligence, or AI, for quick answers to complex problems, a growing number of school districts are turning to the technology to keep their students and staff safe. A school district in Charles County, Maryland, roughly an hour from Washington D.C., is in the process of installing software and hardware which would allow their current security cameras to detect a potential active shooter.ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ ""This artificial intelligence has the ability to be able to identify a weapon, to assess what’s going on and how that person is acting,"" said Jason Stoddard, Director of School safety and Security for Charles County Public Schools. The district, through a state grant, is in the process of installing AI gun detection technology at all of its campuses. The cameras, which were installed years prior, will now communicate with a third party monitoring center if a gun is detected.&nbsp;  ""It plays the role of the human being that might or might not be monitoring,"" said Dave Fraser, CEO of Omnilert, which is one of a handful of companies offering the gun detection technology. ""The system is designed to allow for monitors to determine if a threat is real and if so, alert local police and school authorities within seconds.""TENNESSEE SCHOOL SHOOTING: WHAT TO KNOW ABOUT COVENANT SCHOOL IN NASHVILLE ZeroEyes, a Pennsylvania-based AI gun detection company, told Fox News its seen a surge of interest in recent years following multiple mass shootings on school campuses nationwide. The company told FOX it proudly employs law enforcement experts, people who’ve severed on the front lines, to faster assist schools when reviewing threats.  ""We have 135 employees and 80% of them come from the veteran community,"" said Mike Lahiff, CEO of ZeroEyes in an interview with FOX on Wednesday.  Tech experts admit the AI products do have limits and would not detect weapons hidden under coats or in backpacks. In Maryland, school officials said they have a multi-layer plan to deal with security and employ multiple methods for keeping students safe.CLICK HERE TO GET THE FOX NEWS APP ""It's not replacing the pillars that we have, which are building relationships and positive cultures inside our schools by having a well-trained staff and student body,"" added Stoddard."
20230330,foxnews,Democrats and Republicans coalesce around calls to regulate AI development: 'Congress has to engage',"Lawmakers in the highly-polarized 118th Congress appear to be finding some common ground with regard to artificial intelligence (AI). Several have indicated they would like to see some kind of regulation to rein in the fast-moving sector on the heels of a stunning warning from tech industry leaders. ""I think what you have to do is, to identify what is not allowed in terms of ethics and illegal activities, whether it is AI or not – you impose on AI activities the same level of ethics and privacy that you do for other competencies today,"" Sen. Mike Rounds, a leader of the Senate AI Caucus, told Fox News Digital. Homeland Security and Government Affairs Committee Chair Gary Peters, D-Mich., pointed out to Fox News Digital that his committee had recently held a hearing on the ""pros and cons"" of AI technology. ""I intend to have a series of hearings in Homeland Security and Government Affairs taking up AI and what we should be thinking about,"" Peters added. ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’  It comes on the heels of a dramatic letter signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants calling for a six-month pause to advanced AI developments, citing ""profound risks to society and humanity."" Sen. Michael Bennet, D-Colo., who sent a letter to tech company leaders last week calling for them to consider the safety of children when rolling out AI systems such as chatbots, suggested that an agency could be created to regulate the relatively restriction-free AI industry ""in the long term."" For now, however, the senator said these companies have to police themselves. ""I think we do have a role to play,"" he said when asked if Congress should step in to regulate AI. ""In the long run, I think what we could do is set up, you know, an agency here. They can negotiate on behalf of the American people, so we can actually have a negotiation about privacy… In the near term, I think it’s going to be important for tech to police itself."" AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  Sen. Brian Schatz, D-Hawaii, shared a similar suggestion, pointing out that he co-led legislation in the previous Congress aimed at enacting more barriers on AI’s growth. ""Congress has to sink its teeth into what to do about it. We've worked with [Retired Sen. Rob Portman, R-Ohio] to establish a law for AI, a commission for AI in government,"" Schatz told Fox News Digital. ""I think we should do something broader for AI throughout the private sector. But I think the first step is to recognize that this is a legitimate area for federal policy."" However, in his earlier comments, Rounds questioned whether existing laws were enough to cover the fast-moving sector.&nbsp; ""So if you're in a business, you know that there are certain rules you can't break,"" Rounds said. ""Those same things need to be applied to AI. The question is, do we have the appropriate language in the law today to address the things that AI might create, that we haven't thought about in our existing law?""  Over on the House side, Rep. Ken Buck, R-Colo., a leader in the efforts to crack down on Big Tech, also urged Congress to take the reins. ""With the emergence of AI comes both opportunity and challenges. We have seen the impact and consequences of a decade of inaction on Big Tech. Congress cannot afford to be caught sleeping at the wheel again. AI has great promise but left unscrutinized could be used to spread propaganda, dangerously restructure our economy, and increase the size of current Big Tech monopolies,"" Buck told Fox News Digital. CLICK HERE TO GET THE FOX NEWS APP Sen. JD Vance, R-Ohio, however, broke from his Senate colleagues to caution them to not rush into action before understanding the complicated technology. ""It's way too early to say what role Congress should take. I think right now, we need to understand this a little bit better. And, you know, look –we’re in the very early days of this process,"" Vance said. ""So I wouldn't want to commit to a congressional strategy before we even understand the problem."""
20230330,foxnews,"Unbridled AI tech risks spread of disinformation, requiring policy makers step in with rules: experts","Scores of technology experts and college professors across different academic backgrounds signed onto an open letter calling for a six-month pause on developing rapidly-evolving AI technology, which they say threatens humanity and society.&nbsp; At the heart of the argument for the pause is to give policymakers space to develop safeguards that would allow for researchers to keep developing the technology, but not at the reported threat of upending the lives of people across the world with disinformation.&nbsp; ""The federal government needs to play a central role using legislation and regulations to require the companies to impose much stricter safety measures and guardrails. However, legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed,"" Geoffrey Odlum, a retired 28-year diplomat who currently serves as president of Odlum Global Strategies, which advises the government and corporations on national security and tech policy issues, told Fox News Digital.&nbsp; Odlum is one of the more than 1,000 signatories of an open letter calling for all AI labs to pause their research for at least six months, arguing ""p​​owerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."" ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE'  The Elon Musk-backed letter specifically calls for AI labs to pause training systems that are more powerful than GPT-4, the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab.&nbsp; After the letter was released Wednesday, some critics dismissed it as ""just dripping with AI hype,"" including the authors behind a study cited in the letter.&nbsp; ""They basically say the opposite of what we say and cite our paper,"" said computer scientist Timnit Gebru on Twitter. Gebru is an author behind a study cited in the letter as alleged proof that ""AI systems with human-competitive intelligence can pose profound risks to society and humanity.""&nbsp; Gebru was joined by her co-author Emily Bender in lambasting the letter, saying their research was not about AI being ""too powerful,"" but instead focused on the risks of AI and its ""concentration of power in the hands of people, about reproducing systems of oppression, about damage to the information ecosystem,"" the Economist reported.&nbsp; ""Legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed. That's why I support the call for a 6-month pause in further developments[.]"" However, to those who signed on, they described that AI technology has essentially morphed into a dangerous Wild West that needs a governor.&nbsp; Such technology, supporters of the letter say, could be used to create disinformation, including by U.S. adversaries who want to cause chaos stateside. Odlum pointed to AI technology such as Dall-e 2, which can create realistic images depicting a phony arrest of former President Trump or President Biden kneeling to Chinese President Xi Jinping.&nbsp; ""It's clearly fake, but it looks photorealistic. So the average American would see that and freak out,"" Odlum told Fox News Digital.&nbsp; I INTERVIEWED CHATGPT AS IF IT WAS A HUMAN; HERE'S WHAT IT HAD TO SAY THAT GAVE ME CHILLS University of Pennsylvania professor of Medical Ethics and Health Policy, Jonathan D. Moreno, described to Fox News Digital he has similar concerns.&nbsp; ""This specific danger at the moment is our inability to know with confidence whether an AI platform has created a document or even an image - a moving image or a stationary image. We don't know what the system is doing,"" he said.&nbsp;  Currently, the U.S. has a handful of bills in Congress on AI, while some states have also tried to tackle the issue. However, the lack of hard-set rules has reportedly left some consumers and corporations in a confusing limbo, which is why Odlum is calling for the highest echelons of government to roll out uniform regulations.&nbsp; ""The White House does have an AI research office, and they have released what they called an AI Bill of Rights. Which called for the tech industry to develop AI responsibly and to protect data and to make sure algorithms aren't discriminatory,"" Odlum said, adding the document is ""a useful starting point."" CHATGPT NEW ANTI-CHEATING TECHNOLOGY INSTEAD CAN HELP STUDENTS FOOL TEACHERS AI labs that create technology that could be used by bad actors for disinformation or chaos do not currently face consequences for violating guides put forth by the White House or government agencies. To create these rules, the government needs to act swiftly, the retired diplomat said.&nbsp; ""Legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed. That's why I support the call for a 6-month pause in further developments, to allow the government time to examine the risks and engage the technology industry and civil society in a collaborative way to produce laws and regulations, safety measures and guardrails, to make sure that generative AI is not used by adversaries to create disinformation that divides us any further,"" Odlum said.&nbsp; ""It's not enough for one company to decide what the rules are, and not have a public conversation about it, try to get a sense of how to prevent bad actors. Although this horse may be out of the barn already."" Moreno told Fox News Digital that ""there's really no review at all"" regarding researchers’ work to make computers smarter, saying it is ""something that I think we've kind of let go of without asking industry to do a little more public consideration."" ELON MUSK'S AI WARNING IS 'UNPRECEDENTED' AND SHOWS 'EXTRAORDINARY' LEVEL OF CONCERN, SAYS DOUGLAS MURRAY Moreno has written about AI extensively in recent years, highlighting the question of regulating the industry back in 2019.&nbsp;  ""There is a great deal of regulation concerning biological experiments that could inadvertently create a ‘smart’ laboratory animal—like putting human-sourced neurons into a non-human primate embryo—but none concerning engineering developments that could lead to the singularity,"" Moreno wrote at the time in The Regulatory Review.&nbsp; ""Singularity"" in this context is defined as when a computer reaches superhuman intelligence, and was coined by mathematician Vernor Vinge 30 years ago.&nbsp; ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ ""Should some agency like the U.S. Consumer Product Safety Commission be empowered to verify that the standards are being administered? By the time the singularity has been achieved, a recall may be beside the point,"" Moreno wrote.&nbsp; He warned, ""At that point, in the words of the Borg in ""Star Trek,""'resistance is futile.'"" Fast-forward to 2023 when AI has become ""human-competitive at general tasks,"" according to the letter.&nbsp;Moreno said he wishes he were ""optimistic"" about creating rules on AI that would be industry-wide. CLICK HERE TO GET THE FOX NEWS APP ""Am I optimistic that we can actually create some rules that would be industry-wide? I wish I were. But I think at least, It's not enough for one company to decide what the rules are, and not have a public conversation about it, try to get a sense of how to prevent bad actors. Although this horse may be out of the barn already."""
20230330,foxnews,CONGRESS WEIGHS IN: Should tech companies pause 'giant AI experiments' as Elon Musk and others suggest?,"Congressional lawmakers weighed in Thursday on whether companies should pause advanced artificial intelligence training in the wake of an open letter signed by Elon Musk and other tech leaders. ""I think Elon Musk is rightfully being cautious,"" Rep. Brian Mast, a Florida Republican, told Fox News. ""I appreciate that he's looking to put the brakes on, and I agree with it.""  ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE' Musk, 2020 presidential candidate Andrew Yang, Apple co-founder Steve Wozniak and several other tech leaders urged AI labs to pause development of advanced systems in a recent open letter titled ""Pause Giant AI Experiments.""&nbsp; ""AI systems with human-competitive intelligence can pose profound risks to society and humanity,"" warns the letter, which has been signed by more than 1,400 people. The letter asks developers to halt training AI systems more powerful than GPT-4 for at least six months. San Francisco startup OpenAI's GPT-4 is the successor to the popular AI chatbot ChatGPT. SHOULD TECH COMPANIES PAUSE ‘GIANT AI EXPERIMENTS’?  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Rep. Victoria Spartz said she's less concerned with Musk's opinion and more concerned with protecting Americans' data and online privacy. ""We as the government have a duty to protect people rights and rights to life, liberty and property and we do not have good definitions on who owns your data,"" the Indiana Republican said. ""Big Tech companies are really abusing that and using unlimited immunity to suppress people's rights. And I think that's very dangerous."" Rep. Marcus Molinaro said innovation is important, but so is protecting privacy.  CLICK HERE TO GET THE FOX NEWS APP ""I would hope we could find some area of common ground to establish the appropriate guardrails,"" the New York Republican said. To hear more from lawmakers, click here."
20230330,foxnews,White House tight-lipped as push for congressional intervention into rapid AI developments heats up,"The White House remains largely on the sidelines of what has become a growing debate among Americans and lawmakers about the rapid developments being made in the artificial intelligence (AI) industry and whether there should be some type of congressional intervention. Fielding questions from the briefing room on Thursday, White House press secretary Karine Jean-Pierre did not say whether the Biden administration would urge lawmakers to federally regulate AI after she was asked by Fox News White House correspondent Peter Doocy about an open letter, which was signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants, that cited AI's ""profound risks to society and humanity."" ""It highlights a number of challenges addressed directly in the administration's blueprint for an AI bill of rights, which was released last October,"" Jean-Pierre said of the letter. ""It includes principles and practices AI creators can use to ensure protections related to safety, civil rights, civil liberties are integrated into AI systems from start to finish."" ""Right now, there's a comprehensive process that is underway to ensure a cohesive federal government approach to AI-related risks and opportunities, including how to ensure that AI innovation and deployment proceeds with appropriate prudence and safety foremost in mind,"" she added. ""I don't have anything else to announce at this point, at this time, but there is a comprehensive process in place."" BIDEN ADMIN SILENT AMID GROWING CONCERN FROM LAWMAKERS OVER RAPID DEVELOPMENT OF AI TECHNOLOGY  Doocy pressed Jean-Pierre on the seriousness of the matter and cited comments made by Eliezer Yudkowsky, a decision theorist at the Machine Intelligence Research Institute, who wrote in a recent op-ed that the six-month ""pause"" on developing ""AI systems more powerful than GPT-4"" — as called for by Musk and hundreds of other innovators and experts — understates the ""seriousness of the situation."" He would go further by implementing a moratorium on new large AI learning models that is ""indefinite and worldwide."" ""Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die,"" Yudkowsky said. ""Not as in 'maybe possibly some remote chance' but as in 'that is the obvious thing that would happen.'"" ""Would you agree that does not sound good?"" Doocy asked Jean-Pierre of Yudkowsky's claim. ""Your delivery, Peter, it's quite something,"" Jean-Pierre responded with a laugh. ""It sounds crazy, but is it?"" Doocy asked. ""All I can say is that there's a comprehensive process in place. We put out a blueprint back in October, as you know,"" she said in response. ""I don't have anything to share. We have seen the letter. We understand what their concerns are. Again, comprehensive process — we're gonna let that flow."" Doocy then asked Jean-Pierre whether President Biden is ""worried that artificial intelligence could become self-aware."" AI EXPERT WARNS ELON MUSK-SIGNED LETTER DOESN'T GO FAR ENOUGH, SAYS 'LITERALLY EVERYONE ON EARTH WILL DIE' ""Look, we are — again, there is a comprehensive process,"" she said. ""We are taking this very seriously. … I just don't want to get ahead of our findings and what that's going to look like, but it is a cohesive federal government approach to AI-related risks as you just laid out in a very dramatic way.""  ""We're going to move on. But thank you, Peter, for the drama,"" Jean-Pierre added. The Blueprint for an AI Bill of Rights — as referenced by Jean-Pierre during the briefing — was published by the White House Office of Science and Technology Policy in October and is a ""set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence."" The five principles featured in the blueprint include: safe and effective systems; algorithmic discrimination protections; data privacy; notice and explanation; and human alternatives, consideration and fallback. When reached for comment about the issue and whether the White House has concern over the rapid development of AI or believes it should be federally regulated, Jean-Pierre referred Fox News Digital to the National Security Council (NSC), which serves as Biden's ""principal forum for considering national security and foreign policy matters with his or her senior advisers and cabinet officials."" Despite signaling that it would respond rapidly to Fox News' request, after more than 24 hours, the NSC did not provide comment on the Biden administration's reaction to the call for an AI development moratorium.  CLICK HERE TO GET THE FOX NEWS APP The relative silence from the White House over potentially disruptive developments in AI comes as lawmakers from both sides of the aisle in the 118th Congress appear to be finding common ground in calling for oversight of the burgeoning technology. ""I think what you have to do is to identify what is not allowed in terms of ethics and illegal activities, whether it is AI or not. You impose on AI activities the same level of ethics and privacy that you do for other competencies today,"" South Dakota GOP Sen. Mike Rounds, a leader of the Senate AI Caucus, told Fox News Digital on Wednesday. Sen. Gary Peters, D-Mich., said the Senate Homeland Security and Governmental Affairs Committee, which he chairs, recently held a hearing on the ""pros and cons"" of AI technology. ""I intend to have a series of hearings in Homeland Security and [Governmental] Affairs taking up AI and what we should be thinking about,"" Peters said. Fox News' Chris Pandolfo contributed to this report."
20230308,foxnews,US military jet flown by AI for 17 hours: Should you be worried?,"Yes, you read the headline correctly. The United States Defense Department&nbsp;recently confirmed that artificial intelligence successfully flew a jet similar to an F-16 for 17 hours straight. How did this flight happen? The jet was flown over a series of 12 flights back in December 2022 at the&nbsp;Edwards Air Force Base in Kern County, California. CLICK TO GET KURT’S CYBERGUY NEWSLETTER WITH QUICK TIPS, TECH REVIEWS, SECURITY ALERTS AND EASY HOW-TO’S TO MAKE YOU SMARTER The Defense Department used an experimental plane called the Vista X-62A for the flights. There were safety pilots present on board throughout the flights just in case something were to go wrong. However, the jet was under the control of one of four different AI algorithms at any given time during the tests and everything went smoothly. The algorithms were developed and tested by what is known as Air Combat Evolution (ACE). Using this allowed them to get ahead on missions and training. BEST TECH TO HELP A LOVED ONE WITH MEMORY ISSUES  What did the jet do during the experiment? This jet did way more than simply soar through the sky. The Defense Department had the jet participate in dogfighting during multiple simulated combat missions, as well as practice takeoffs and landings. Although most aircraft today do have autopilot systems, this is the first time that artificial intelligence has engaged in any kind of aerial combat. HOW TO TELL IF YOUR LAPTOP CAMERA HAS BEEN HACKED AND SOMEONE IS SPYING ON YOU&nbsp;  What does this mean for the future of flying? This was part of a joint project between the Defense Advanced Research Projects Agency and the U.S. Air Force to advance autonomous flight technologies. Although there have not been any similar tests announced just yet, this is a major stepping stone for our armed forces in defense of this country. Artificial intelligence is already proving to be the way of the future in other fields, such as automobile driving, so it makes sense that the military is moving in the same direction. If further tests continue to be successful, it will be no surprise if the military begins to opt for artificial intelligence to be used in combat as a way of advancing our fighting strategies and further protecting the lives of our men and woman in the armed forces. BIDENCASH CRIMINAL MARKET RELEASES OVER 2M CREDIT CARD NUMBERS FREE FOR THE TAKING What are the potential downsides to using AI by the US military? While AI technology has advanced in recent years, there is always the potential for technical problems or glitches to occur during a flight. For example, if the AI system malfunctions or encounters a problem, it may not be able to make the proper adjustments to keep the plane flying safely. Another potential concern is the lack of human judgment. While AI can be programmed to make decisions based on a wide range of data and information, it may not be able to replicate the nuanced decision-making abilities of a human pilot. In an emergency or unexpected event, a human pilot may be better equipped to make the right call versus AI. BEST BACKUP POWER: GAS VS. BATTERY  Another negative of using AI by the military is security risks. Using AI in military aircraft raises concerns about cybersecurity and the potential for hacking or other cyber-attacks. Suppose a malicious person were to gain control of the AI system. In that case, they could potentially use the aircraft for harmful purposes even against the U.S. HOW HACKERS ARE USING CHAPTGPT TO CREATE MALWARE TO TARGET YOU We cannot talk about AI without also considering the loss of jobs. The increased use of the technology in military aircraft could potentially lead to job loss for human pilots. Lastly, some people may have ethical concerns about using AI in military operations, particularly if the technology is used in autonomous weapons systems that could make life-and-death decisions without human intervention.&nbsp; SHOULD A FOURTH LIGHT BE ADDED TO TRAFFIC SIGNALS FOR AUTONOMOUS CARS? As you can see, there is a lot at stake here, and it will be interesting to see if AI eventually completely takes the place of a pilot in a military jet in the future. What do you think of our military using artificial intelligence? Let us know your thoughts. CLICK HERE TO GET THE FOX NEWS APP For more of my tips, subscribe to my free CyberGuy Report Newsletter by clicking the ""Free newsletter"" link at the top of my website. Copyright 2023 CyberGuy.com. All rights reserved."
20230308,nbcnews,Exiled Afghan girls robotics team makes a home in Qatar,"One of the newest members of the pioneering Afghan girls robotics team wants the world to know that after the Taliban takeover, women in her homeland are still striving for their rights and thirsting for education. The young woman, 19-year-old Afsana Ahmadi, also said she misses her dad, terribly. “When I left Afghanistan, it was the last time I saw him,” she said in a Zoom interview with NBC News to mark International Women’s Day.  Ahmadi, who hails from the city of Herat in western Afghanistan, said her father accompanied her to Kabul last summer and wept when she was evacuated to Qatar, where most of the team is now based. “He cried with me and told me that, ‘Dear Afsana, never ever be hopeless and continue your path,’” she said. “I really miss him.” Known at home as the “robot girls,” Ahmadi is part of an all-woman team that became a symbol of Afghan progress by taking part in competitions around the world where budding scientists show off their latest robotic creations. The team became famous in 2017 when the United States twice denied members visas needed to compete in the country. Then-President Donald Trump stepped in, and they were able to attend. When the Taliban returned to power in 2021, most of the team fled to Qatar fearing that the arch-conservative Islamic regime would once again impose draconian rules that would bar women from attending school or work outside the home. The Taliban, which had initially said they had modernized during their 20 years out of power, soon began to make it impossible for women and girls to go to work and school. The Taliban say they support the education and employment of women ""within the bounds of Shariah,"" or Islamic law. The interpretation of Shariah varies widely, and some Afghans and experts accuse the fundamentalist Taliban of imposing archaic tribal traditions specific to the Taliban onto the rest of the country.  On Wednesday, foreign ministers from the United States, European Union and dozens of other countries released a statement saying, in part, that since the return of the Taliban, ""Afghan women and girls have been denied access to secondary education, to higher education, to public and political spaces, and to employment opportunities."" Ahmadi, who didn’t leave for Qatar until 2022, lived for almost a year under the rule of the Taliban, which had previously been in power from 1996 to 2001. “So it was kind of shocking news for me and kind of going in a way that I don’t know what to do,” she said. Fellow team member Sadaf Hamidi, 19, who left for Qatar in 2021, said she has gotten horrifying reports from her family about how the Taliban have changed life for women. “One of my sisters used to be a medical student, the other one was high school student,” Hamidy said. “But right now they have to stay at home and they cannot continue their education. … This is heartbreaking for me and for them.” Team captain Florence Pouya, 17, said they think constantly “about the other girls in Afghanistan, who cannot even go to school.” She said that motivates them to try harder. “We are not just the robotics team; we are not just building the robot,” she said.  Ahmadi said that while the “Afghan Dreamers” were making their mark on the international stage, word of their scientific exploits reached her in Herat as the Taliban were making life harder for women. “It was like kind of a hope,” she said. “It was kind of a light like inside of you that pushed us to not surrender to the life. Continue, it’s not the end point.” Inspired by their example, Ahmadi was determined to join “this amazing team,” a younger generation of which was still operating in Herat. And after running through a series of interviews and tests, she made the team. But as the Taliban tightened their grip, it became increasingly clear to Ahmadi that if she wanted to become a scientist, she would have to leave Afghanistan and say goodbye to everyone she knows and loves. And she would have to do so by herself. “No, I left the country alone,” she said. “So like the situation was difficult for girls to leave the country, and it’s still difficult to leave the country, you know, without a person that can accompany them.” For now, home is a compound in Qatar she shares with other team members. “I’m so thankful for having this opportunity,” Ahmadi said. “At the same time, I do wish that my friends and all my classmates, they have this opportunity too.” Ahmadi said she has been in touch with family and friends and life in Afghanistan right now is “clearly difficult.” But as a member of the robotics team, Ahmadi said they have been able to show the world that Afghan women are capable of doing “amazing” things. “I can be the voice of my friends and I can do something from here that can help them,” she said."
20230914,cbsnews,"Elon Musk says artificial intelligence needs ""a referee"" after tech titans meet with lawmakers","The nation's biggest technology executives on Wednesday loosely endorsed the idea of government regulations for artificial intelligence at an unusual closed-door meeting in the U.S. Senate. But there is little consensus on what regulation would look like, and the political path for legislation is difficult.Executives attending the meeting included Tesla CEO Elon Musk, Meta's Mark Zuckerberg, former Microsoft CEO Bill Gates and Google CEO Sundar Pichai. Musk said the meeting ""might go down in history as being very important for the future of civilization.""First, though, lawmakers have to agree on whether to regulate, and how.Senate Majority Leader Chuck Schumer, who organized the private forum on Capitol Hill as part of a push to legislate artificial intelligence, said he asked everyone in the room — including almost two dozen tech executives, advocates and skeptics — whether government should have a role in the oversight of artificial intelligence, and ""every single person raised their hands, even though they had diverse views,"" he said.Among the ideas discussed was whether there should be an independent agency to oversee certain aspects of the rapidly developing technology, how companies could be more transparent and how the U.S. can stay ahead of China and other countries.""The key point was really that it's important for us to have a referee,"" said Musk during a break in the daylong forum. ""It was a very civilized discussion, actually, among some of the smartest people in the world.""Schumer will not necessarily take the tech executives' advice as he works with colleagues on the politically difficult task of ensuring some oversight of the burgeoning sector. But he invited them to the meeting in hopes that they would give senators some realistic direction for meaningful regulation.Congress should do what it can to maximize AI's benefits and minimize the negatives, Schumer said, ""whether that's enshrining bias, or the loss of jobs, or even the kind of doomsday scenarios that were mentioned in the room. And only government can be there to put in guardrails.""Congress has a lackluster track record when it comes to regulating new technology, and the industry has grown mostly unchecked by government in the past several decades. Many lawmakers point to the failure to pass any legislation surrounding social media, such as for stricter privacy standards.Schumer, who has made AI one of his top issues as leader, said regulation of artificial intelligence will be ""one of the most difficult issues we can ever take on,"" and he listed some of the reasons why: It's technically complicated, it keeps changing and it ""has such a wide, broad effect across the whole world,"" he said.Sparked by the release of ChatGPT less than a year ago, businesses have been clamoring to apply new generative AI tools that can compose human-like passages of text, program computer code and create novel images, audio and video. The hype over such tools has accelerated worries over its potential societal harms and prompted calls for more transparency in how the data behind the new products is collected and used.Republican Sen. Mike Rounds of South Dakota, who led the meeting with Schumer, said Congress needs to get ahead of fast-moving AI by making sure it continues to develop ""on the positive side"" while also taking care of potential issues surrounding data transparency and privacy.""AI is not going away, and it can do some really good things or it can be a real challenge,"" Rounds said.The tech leaders and others outlined their views at the meeting, with each participant getting three minutes to speak on a topic of their choosing. Schumer and Rounds then led a group discussion.During the discussion, according to attendees who spoke about it, Musk and former Google CEO Eric Schmidt raised existential risks posed by AI, and Zuckerberg brought up the question of closed vs. ""open source"" AI models. Gates talked about feeding the hungry. IBM CEO Arvind Krishna expressed opposition to proposals favored by other companies that would require licenses.In terms of a potential new agency for regulation, ""that is one of the biggest questions we have to answer and that we will continue to discuss,"" Schumer said. Musk said afterward he thinks the creation of a regulatory agency is likely.Outside the meeting, Google CEO Pichai declined to give details about specifics but generally endorsed the idea of Washington involvement.""I think it's important that government plays a role, both on the innovation side and building the right safeguards, and I thought it was a productive discussion,"" he said.Some senators were critical that the public was shut out of the meeting, arguing that the tech executives should testify in public.Republican Sen. Josh Hawley of Missouri said he would not attend what he said was a ""giant cocktail party for big tech."" Hawley has introduced legislation with Democratic Sen. Richard Blumenthal of Connecticut to require tech companies to seek licenses for high-risk AI systems.""I don't know why we would invite all the biggest monopolists in the world to come and give Congress tips on how to help them make more money and then close it to the public,"" Hawley said.While civil rights and labor groups were also represented at the meeting, some experts worried that Schumer's event risked emphasizing the concerns of big firms over everyone else.Sarah Myers West, managing director of the nonprofit AI Now Institute, estimated that the combined net worth of the room Wednesday was $550 billion and it was ""hard to envision a room like that in any way meaningfully representing the interests of the broader public."" She did not attend.In the U.S., major tech companies have expressed support for AI regulations, though they don't necessarily agree on what that means. Similarly, members of Congress agree that legislation is needed, but there is little consensus on what to do.Some concrete proposals have already been introduced, including legislation by Sen. Amy Klobuchar, D-Minn., that would require disclaimers for AI-generated election ads with deceptive imagery and sounds. Schumer said they discussed ""the need to do something fairly immediate"" before next year's presidential election.Hawley and Blumenthal's broader approach would create a government oversight authority with the power to audit certain AI systems for harms before granting a license.Some of those invited to Capitol Hill, such as Musk, have voiced dire concerns evoking popular science fiction about the possibility of humanity losing control to advanced AI systems if the right safeguards are not in place. But the only academic invited to the forum, Deborah Raji, a University of California, Berkeley researcher who has studied algorithmic bias, said she tried to emphasize real-world harms already occurring.""There was a lot of care to make sure the room was a balanced conversation, or as balanced as it could be,"" Raji said. What remains to be seen, she said, is which voices senators will listen to and what priorities they elevate as they work to pass new laws.Some Republicans have been wary of following the path of the European Union, which signed off in June on the world's first set of comprehensive rules for artificial intelligence. The EU's AI Act will govern any product or service that uses an AI system and classify them according to four levels of risk, from minimal to unacceptable.A group of European corporations has called on EU leaders to rethink the rules, arguing that it could make it harder for companies in the 27-nation bloc to compete with rivals overseas in the use of generative AI."
20230914,foxnews,German military plows millions into AI 'environment' for weapons tests that could change combat forever,"Germany has invested heavily into what officials say will help them find the future of combat through an artificial intelligence (AI) virtual training area some have dubbed a military ""metaverse."" ""We compete with the big ones in the industry,"" GhostPlay project manager Gary Schaal, a professor at Helmut Schmidt University in Hamburg, wrote in a press release. ""Our [Unique Selling Point]: agility and the ability to quickly show results."" Developer 21strategies pulled together a mix of start-ups and defense academics to create the virtual battlefield GhostPlay, which allows developers to test out different weapons and systems inside a risk-free environment. The German Defense Ministry funded the project as part of a 500 million euros ($540 million) COVID-19 spending package that intended to help revive the country’s high-tech defense sector, Defense News reported. TECH GIANT TO SHIELD CUSTOMERS FROM IP LAWSUITS RELATED TO AI TOOLS  The GhostPlay website describes the platform as a ""simulation environment AI-based decision-making at machine speed."" ""Novel, superior courses of action can be developed by simulating complex military battle scenarios,"" the company wrote. ""As a result, flexibility and superiority can be achieved at the strategic, tactical and operational levels."" The simulations can create ""unpredictable"" conditions to improve the thoroughness of testing and depth of preparation for military planning, the developers said. WHAT IS ARTIFICIAL INTELLIGENCE (AI)?  One of the key aspects that sets the program apart rests in the use of ""third-wave"" algorithms, which 21strategies CEO Yvonne Hofstetter says creates more ""human-like"" decision-making from the simulated units. She explained that second-wave algorithms merely optimize or speed up decision-making, but the third-wave will help create new situations and determine novel actions. The platform also seeks to recreate environments ""down to the last leaf,"" according to Hofstetter, which GhostPlay achieves through aggregating satellite photos and local databases on everything from housing to vegetation. TECH COMPANY BOASTS IT CAN PREDICT CRIME WITH SOCIAL MEDIA POLICING THROUGH ARTIFICIAL INTELLIGENCE  ""There is enough info ... kind of scary, really,"" Hofstetter said. The most promising exercise the platform has recently explored looks at how to best optimize swarm tactics, particularly loitering munitions. The Office of Army Development has collaborated with the platform precisely due to its ability to recreate detailed environments in which the munitions would deploy. CLICK HERE TO GET THE FOX NEWS APP According to a press release from Hensoldt, a multinational company that provides financing to the GhostPlay platform, ""In order to optimally enable highly complex defense systems, we need to master artificial intelligence in its entire range ... to this end, we develop many AI competencies in-house and supplement them in a very targeted manner."""
20230914,foxnews,"DHS releases new guardrails for using AI in missions, announces new officer","The Department of Homeland Security (DHS) on Thursday unveiled new guardrails for its use of artificial intelligence in carrying out its mission to secure the border.&nbsp; The new policies were developed by DHS Artificial Intelligence Task Force (AITF), which DHS Secretary Alejandro Mayorkas created in April. &nbsp;&nbsp; In announcing these new policies, DHS noted that AI has been critical to its missions, including combating fentanyl trafficking, strengthening supply chain security, countering sexual exploitation, and protecting critical infrastructure.&nbsp; ARIZONA BORDER COUNTY BLINDSIDED AS BIDEN ADMIN ORDERS STREET RELEASE OF ILLEGAL MIGRANTS  Mayorkas writes in the AI policy memo, expected to be released later Thursday, that the US must ensure AI is ""rigorously tested to be effective [and] safeguards privacy, civil rights, and civil liberties while avoiding inappropriate biases.""&nbsp; DHS has already used AI technology extensively on the southern border, most notably with the use of more than 200 surveillance cameras to detect and flag where human crossings occur.&nbsp; DHS says it has appointed Chief Information Officer (CIO) Eric Hysen as the Department’s first Chief AI Officer. Hysen, who was set to appear before Congress Thursday, will promote AI innovation and safety within the Department, DHS said.&nbsp; MIGRANT NUMBER OVERWHELMING ARIZONA BORDER FACILITIES AMID NEW WAVE AS STREET RELEASES BEGIN ""I think the potential for unintended harm from the use of AI exists in any federal agency and in any use of AI,"" Hysten said. ""We interact with more people on a daily basis than any other federal agency. And when we interact with people, it can be during some of the most critical times of their lives.""&nbsp; Historically, academics have flagged the dangers of AI regarding racial profiling because it can still make errors while identifying relationships in complex data.&nbsp; As part of the new policy, Americans are able to decline the use of facial recognition technology in a variety of situations, including during air travel check-ins.&nbsp;  DHS’ new guidelines will also require that facial recognition matches discovered using AI technology be manually reviewed by human analysts to ensure their accuracy, according to a new directive that the agency plans to release alongside the AI memo. MAYORKAS OFFICIALLY CANCELS HOMELAND SECURITY DISINFORMATION GOVERNANCE BOARD During a congressional hearing, Hysen planned to highlight a recent case at California's San Isidro Port of Entry where agents with Customs and Border Patrol had used advanced machine learning (ML) models to flag an otherwise unremarkable car driving north from Mexico for having a ""potentially suspicious pattern.""&nbsp; CLICK HERE TO GET THE FOX NEWS APP Agents later discovered 75 kilograms of drugs in the car's gas tank and rear quarter panels."
20231023,cbsnews,"Baltimore designated a federal tech hub, setting path for millions in funding","BALTIMORE -- Baltimore City has been named a ""Tech Hub"" as part of a highly competitive federal program to expand manufacturing across the country, making the city legible for a slice of hundreds of millions of dollars in funding. The Greater Baltimore Committee led the Baltimore Tech Hub, a consortium that applied for the U.S. Department of Commerce Economic Development Administration's Regional Technology and Innovation Hubs program. Baltimore is one of 31 designees announced Monday, picked from nearly 400 applicants. The consortium is made up of businesses, colleges and universities, as well as local governments. Together they pitched a plan focused on the intersection of AI and biotechnology. aimed at improving health outcomes by developing new medicines and therapies.Previous Coverage: Baltimore region making bid to become country's next big tech hubThe program was authorized at $10 billion and the EDA, which administers the program, already has $500 million for its first round of awards. ""This is exciting news for Baltimore,"" said Latoya Staten, Director of Impact at Fearless, one of the businesses involved in putting together the Baltimore region's bid to receive the tech hub designation. ""The tech hub designation is going to be able to bring lots of economic impact and jobs.""    With the designation, the Baltimore consortium will now have to compete for implementation funding in Phase 2, when the EDA will invest between $50-$75 million in each of five to 10 Hubs.""We are just telling the country and the world what we already know,"" Staten said. ""Baltimore is here, we are a tech hub and we are ahead of the game.""The Maryland Congressional Delegation lobbied last month for Baltimore to be named a hub. The delegation comprises U.S. Senators Ben Cardin and Chris Van Hollen and Congressmen Dutch Ruppersberger, John Sarbanes, Kweisi Mfume and David Trone, all Democrats. ""The CHIPS and Science Act jumpstarted the return of manufacturing across the United States and its Regional Tech Hub program will do the same for high-tech industries and the incredible entrepreneurs across the Baltimore region,"" the lawmakers said in a joint statement Monday. ""This is about creating new jobs and emerging industries for the long term. We strongly pushed for the Baltimore Tech Hub application in a letter to Commerce Secretary Gina Raimondo because we know well the local resources and cutting-edge opportunities that can be leveraged to advance the region's technological capabilities.""  "
20240108,nbcnews,Judges in England and Wales are given cautious approval to use AI in writing legal opinions,"LONDON — England’s 1,000-year-old legal system — still steeped in traditions that include wearing wigs and robes — has taken a cautious step into the future by giving judges permission to use artificial intelligence to help produce rulings. The Courts and Tribunals Judiciary last month said AI could help write opinions but stressed it shouldn’t be used for research or legal analyses because the technology can fabricate information and provide misleading, inaccurate and biased information. “Judges do not need to shun the careful use of AI,” said Master of the Rolls Geoffrey Vos, the second-highest ranking judge in England and Wales. “But they must ensure that they protect confidence and take full personal responsibility for everything they produce.” At a time when scholars and legal experts are pondering a future when AI could replace lawyers, help select jurors or even decide cases, the approach spelled out Dec. 11 by the judiciary is restrained. But for a profession slow to embrace technological change, it’s a proactive step as government and industry — and society in general — react to a rapidly advancing technology alternately portrayed as a panacea and a menace. “There’s a vigorous public debate right now about whether and how to regulate artificial intelligence,” said Ryan Abbott, a law professor at the University of Surrey and author of “The Reasonable Robot: Artificial Intelligence and the Law.” “AI and the judiciary is something people are uniquely concerned about, and it’s somewhere where we are particularly cautious about keeping humans in the loop,” he said. “So I do think AI may be slower disrupting judicial activity than it is in other areas and we’ll proceed more cautiously there.” Abbott and other legal experts applauded the judiciary for addressing the latest iterations of AI and said the guidance would be widely viewed by courts and jurists around the world who are eager to use AI or anxious about what it might bring. In taking what was described as an initial step, England and Wales moved toward the forefront of courts addressing AI, though it’s not the first such guidance. Five years ago, the European Commission for the Efficiency of Justice of the Council of Europe issued an ethical charter on the use of AI in court systems. While that document is not up to date with the latest technology, it did address core principles such as accountability and risk mitigation that judges should abide by, said Giulia Gentile, a lecturer at Essex Law School who studies the use of AI in legal and justice systems. Although U.S. Supreme Court Chief Justice John Roberts addressed the pros and cons of artificial intelligence in his annual report, the federal court system in America has not yet established guidance on AI, and state and county courts are too fragmented for a universal approach. But individual courts and judges at the federal and local levels have set their own rules, said Cary Coglianese, a law professor at the University of Pennsylvania. “It is certainly one of the first, if not the first, published set of AI-related guidelines in the English language that applies broadly and is directed to judges and their staffs,” Coglianese said of the guidance for England and Wales. “I suspect that many, many judges have internally cautioned their staffs about how existing policies of confidentiality and use of the internet apply to the public-facing portals that offer ChatGPT and other such services.” The guidance shows the courts’ acceptance of the technology, but not a full embrace, Gentile said. She was critical of a section that said judges don’t have to disclose their use of the technology and questioned why there was no accountability mechanism. “I think that this is certainly a useful document, but it will be very interesting to see how this could be enforced,” Gentile said. “There is no specific indication of how this document would work in practice. Who will oversee compliance with this document? What are the sanctions? Or maybe there are no sanctions. If there are no sanctions, then what can we do about this?” In its effort to maintain the court’s integrity while moving forward, the guidance is rife with warnings about the limitations of the technology and possible problems if a user is unaware of how it works. At the top of the list is an admonition about chatbots, such as ChatGPT, the conversational tool that exploded into public view last year and has generated the most buzz over the technology because of its ability to swiftly compose everything from term papers to songs to marketing materials. The pitfalls of the technology in court are already infamous after two New York lawyers relied on ChatGPT to write a legal brief that quoted fictional cases. The two were fined by an angry judge who called the work they had signed off on “legal gibberish.” Because chatbots have the ability to remember questions they are asked and retain other information they are provided, judges in England and Wales were told not to disclose anything private or confidential. “Do not enter any information into a public AI chatbot that is not already in the public domain,” the guidance said. “Any information that you input into a public AI chatbot should be seen as being published to all the world.” Other warnings include being aware that much of the legal material that AI systems have been trained on comes from the internet and is often based largely on U.S. law. But jurists who have large caseloads and routinely write decisions dozens — even hundreds — of pages long can use AI as a secondary tool, particularly when writing background material or summarizing information they already know, the courts said. In addition to using the technology for emails or presentations, judges were told they could use it to quickly locate material they are familiar with but don’t have within reach. But it shouldn’t be used for finding new information that can’t independently be verified, and it is not yet capable of providing convincing analysis or reasoning, the courts said. Appeals Court Justice Colin Birss recently praised how ChatGPT helped him write a paragraph in a ruling in an area of law he knew well. “I asked ChatGPT can you give me a summary of this area of law, and it gave me a paragraph,” he told The Law Society. “I know what the answer is because I was about to write a paragraph that said that, but it did it for me and I put it in my judgment. It’s there and it’s jolly useful.”"
20240108,cnn,Peregrine mission abandons moon landing attempt after suffering ‘critical’ fuel loss,"Astrobotic Technology, the company that developed the first lunar lander to launch from the United States in five decades, said it is abandoning an attempt to put its Peregrine spacecraft on the moon less than 24 hours after the vehicle took flight. The spacecraft has suffered “critical” propellant loss from a fuel leak, according to the company. Just hours after the vehicle launched from Florida toward the moon early Monday morning, Astrobotic announced the mission was in jeopardy. The lunar lander, dubbed Peregrine, was unable to place itself in a position facing the sun, likely because of a propulsion issue, according to Astrobotic. That wayward orientation prevented the spacecraft from charging its batteries. The battery issue was later resolved, but Astrobotic was not able to correct the apparent issue with the Peregrine lander’s propulsion system. In a statement late Monday evening, the company said a fuel leak is causing the thrusters of Peregrine lander’s attitude control system — which are designed to precisely align the 6-foot-tall box-shaped lander while in space — have had to “operate well beyond their expected service life cycles to keep the lander from an uncontrollable tumble.” Astrobotic added that the thrusters could likely only operate for 40 more hours at most. “At this time, the goal is to get Peregrine as close to lunar distance as we can before it loses the ability to maintain its sun-pointing position and subsequently loses power,” according to the company. That means a potential moon landing, which had been slated for February 23, is off the table. Astrobotic had already warned just after 1 p.m. ET that a “failure within the propulsion system” was draining the vehicle’s fuel. But the company worked for hours Monday to attempt to stabilize the issue and assess options. At one point Monday afternoon, Astrobotic also shared the first image of the Peregrine lander in space. The photograph showed that the outer layers of insulation on the vehicle were crinkled. The distorted material was “the first visual clue that aligns with our telemetry data pointing to a propulsion system anomaly,” the company said in a post on the social media platform X at 4:12 p.m. ET on Monday. From launch to a lunar trajectory The lunar lander, called Peregrine after the fastest bird in the world, appeared to have a wholly successful first leg of its trip after lifting off at 2:18 a.m. ET atop a Vulcan Centaur rocket developed by the joint Lockheed Martin and Boeing venture United Launch Alliance. It was the first ever flight of a Vulcan Centaur rocket, a new vehicle from ULA designed to replace its older lineup of rockets. The company confirmed just after 3 a.m. ET that the Vulcan Centaur performed as expected, delivering the Peregrine lunar lander into a trans-lunar injection orbit, according to ULA. That involves a precisely timed engine burn that pushed the Peregrine lander onto a path in Earth’s orbit that should allow it to sync up with the moon some 384,400 kilometers (238,855 miles) away. The Peregrine lander was then expected to fire up its own onboard thrusters, using up to three maneuvers to pinpoint its path. In a statement, Astrobotic said that Peregrine successfully began communicating with NASA’s Deep Space Network, activated its avionics systems, and “the thermal, propulsion, and power controllers, all powered on and performed as expected.” “After successful propulsion systems activation, Peregrine entered a safe operational state,” the company said. It was after that, however, that the Peregrine lander experienced the “anomaly” that left the vehicle pointed away from the sun and unable to charge its battery. Mission controllers then “developed and executed an improvised maneuver to reorient the solar panels toward the Sun,” according to Astrobotic. They accomplished that goal. “The team’s improvised maneuver was successful in reorienting Peregrine’s solar array towards the Sun. We are now charging the battery,” the company said in an update posted at 12:34 p.m. ET. Still, Astrobotic said it must correct the underlying propulsion issue. The spacecraft would need to use its onboard thrusters — and have enough propellant left over — to make a soft touchdown on the moon. Peregrine mission stakes Pittsburgh-based company Astrobotic Technology developed Peregrine under a $108 million contract with NASA. The vehicle was designed from the outset to be relatively cheap — aiming to fulfill NASA’s vision to reduce the cost of putting a robotic lander on the moon by asking the private sector to compete for such contracts. Astrobotic CEO John Thornton told CNN on January 2 that he viewed this first launch as a test mission. “This really is like a 50-50 shots on goal kind of an approach — where it’s really more about the industry succeeding, not any specific one mission,” Thornton said. Joel Kearns, the deputy associate administrator for exploration at NASA’s Science Mission Directorate, issued a statement Monday, saying, “Each success and setback are opportunities to learn and grow. We will use this lesson to propel our efforts to advance science, exploration, and commercial development of the Moon.” Thornton, who previously said that this Peregrine mission cost Astrobotic more money than it made, also remarked to CNN what it would mean for the company if this mission failed. “It’s certainly going to have some some impact on our relationships and our ability to to secure additional missions in the future,” Thornton said. “It certainly wouldn’t be the end of the business, but it would certainly be challenging.” Abandoning its lunar landing attempt marks a major loss not only for Astrobotic, but also for NASA and other countries and institutions with payloads aboard the Peregrine lander. The company will not be able to test a landing maneuver, which — in previous lunar landing attempts made by various countries and corporations — has proven an exceedingly difficult step in the journey. On board the Peregrine vehicle are five scientific instruments from NASA and 15 other payloads from a variety of organizations and countries. The commercial payloads on the lander include mementos and even human remains that customers had paid to send to the lunar surface."
20230927,foxnews,Newspaper runs robot-written op-ed opposing AI in journalism,"A St. Louis newspaper decided to take on the artificial intelligence debate by allowing a robot to pen an op-ed arguing against the use of AI in journalism. The article, featured in the St. Louis Post-Dispatch, was written entirely by Microsoft's Bing Chat AI program, according to a disclaimer in the article. The bot was instructed to ""write a newspaper editorial arguing that artificial intelligence should not be used in journalism."" The paper then let the AI platform take over from there. And the bot argued that while AI ""may have some benefits,"" it ""also poses serious threats to the quality, integrity, and ethics of journalism."" GERMAN MILITARY PLOWS MILLIONS INTO AI 'ENVIRONMENT' FOR WEAPONS TESTS THAT COULD CHANGE COMBAT FOREVER  ""One of the main reasons why AI should not be used in journalism is that it can undermine the credibility and trustworthiness of news,"" the AI bot wrote. ""AI can generate fake news, manipulate facts, and spread misinformation."" The bot then goes on to list examples of what can go wrong, citing a 2020 incident in which a website was launched entirely by AI to write fake news stories that sometimes contained articles. ""Human journalists have a passion, a curiosity, and a creativity. AI cannot replicate these qualities."" ""Moreover, AI can also create deepfakes, which are synthetic videos or images that can make people appear to say or do things that they never did,"" the bot reasoned. ""Deepfakes can be used to defame, blackmail, or influence public opinion.""  WHAT IS ARTIFICIAL INTELLIGENCE (AI)? The bot noted that, unlike humans, AI cannot determine what is right or wrong morally and factually, cannot protect sources, and has no way to adhere to any sort of professional standards. The article also laid out how AI can be a threat to the livelihoods of journalists, noting that platforms can do almost every task a human journalist can but ""faster, cheaper, and more efficiently than human journalists."" However, the bot notes that AI can't completely replace the human element of a news story. ""Human journalists are not only information providers, but also storytellers, educators, watchdogs, and influencers. Human journalists have a voice, a perspective, and a purpose. Human journalists have a passion, a curiosity, and a creativity,"" the bot wrote. ""AI cannot replicate these qualities."" Jon Schweppe, the policy director of American Principles Project, expressed a similar sentiment, telling Fox News Digital that AI can only ""report basic facts and figures it scrapes from the internet."" ""AI isn’t human, it doesn’t have unique thoughts,"" Schweppe said. ""It can’t do on-the-ground reporting, it can’t break news that hasn’t already been reported elsewhere, and it can’t even comprehend the idea of writing a human interest story."" CLICK HERE FOR MORE US NEWS  The op-ed ultimately concludes that AI should not be used in journalism, calling on media companies to refrain from the practice and ""support and empower human journalists instead."" ""Human journalists are irreplaceable and indispensable in journalism,"" the op-ed concludes. According to the editors of the paper, the op-ed was written almost entirely by AI and was only ""lightly edited for style."" CLICK HERE TO GET THE FOX NEWS APP ""We found that Bing Chat made lucid and persuasive arguments for keeping AI out of journalism,"" the editors wrote. ""It’s an ironic and disturbing success to the experiment — but one that we hope will generate discussion among our fellow humans."" Schweppe believes it is ""inevitable"" AI will begin to have a larger influence on journalism.  ""Because corporations are always looking to cut costs and maximize ‘efficiency,' it is inevitable that AI will replace so many of these reporting jobs, which will hurt journalism as a whole and limit the ability for people to become informed citizens,"" Schweppe said. The Post-Dispatch did not immediately respond to a Fox News request for comment."
20230927,foxnews,North Carolina law enforcement using AI to combat increase in distracted drivers,"Drivers preoccupied with smartphone distractions, such as texting and making phone calls, have become all too common – and many times fatal. North Carolina Highway Patrol reports that it has seen an uptick in distracted truck drivers, and now the agency is using artificial intelligence devices to help crack down on the safety hazard. Distracted driving killed over 3,500 people in 2021, according to the U.S. Department of Transportation. A mom who's made safe driving her passion has felt the pain from a distracted driver two separate times.&nbsp; ""At a stop light you look around, every single person is on their phone,"" said Jennifer Smith, whose mother was killed by a distracted driver. MISSOURI DISTRACTED DRIVING BILL GETS SUPPORT FROM SHERIFF, LOCAL DRIVER   Years later, another distracted driver slammed into her daughter's car.&nbsp; ""My oldest daughter was then hit head on by a distracted delivery app driver, totaled her car and landed in the hospital,"" Smith said. North Carolina Highway Patrol bought three ""Heads Up"" AI devices from Acusensus in efforts to combat the increased distracted driving among truckers. The devices cost $165,000 per unit for a total of $495,000 for all three and were paid for by utilizing federally funded grants. David Kelly, Acusensus Vice President for Global Communications, said the devices are used as an initial screen to help law enforcement determine if a citation needs to be issued. The company has held pilot programs in over a dozen states. A NEW STUDY REVEALS WHERE AMERICANS STAND ON ARTIFICIAL INTELLIGENCE The ""Heads Up"" device takes numerous pictures of the passing commercial motor vehicle's license plate and truck cabin before sending the photos to law enforcement, who are alerted to any violations like distracted driving or driving without a seatbelt. After looking at the pictures, officers can decide whether to cite the driver. The units are not stationary and will move periodically to different locations throughout the state.  North Carolina Trucking Association President Ben Greenberg said the new devices are a hot topic in the trucking industry. ""Will admittedly hear some folks raise some privacy concerns, because these cameras are set up at an angle to be able to [look] into the cabin of a truck, but I think folks generally understand and appreciate that distracted driving is an issue,"" Greenberg said. As Smith fights for more hands-free laws to pass throughout the country, she said it all comes down to one simple motive: ""It’s really just get off your phone, that’s all we want."" CLICK HERE TO GET THE FOX NEWS APP The North Carolina Highway Patrol began the program in the spring. From June 1 to Aug. 4, there were 441 citations issued for seatbelt violations and 315 citations issued for hands-free violations. In September, insurance company USAA released a list of the most and least distracted drivers across the country."
20230927,cnn,Dubai to start robotaxi trials next month in major autonomous push,"Dubai is rolling out its first round of robotaxis next month, as a part of a plan to alleviate congestion and accidents.  Five fully autonomous electric taxis, operated by a General Motors subsidiary called Cruise, will begin test driving on an 8km (5 mile) stretch in the upscale Jumeirah district of the United Arab Emirates city, according to Ahmed Bahrozyan, the CEO of Dubai’s Roads and Transport Authority (RTA).   Dubai hopes to become the first Middle Eastern city to introduce driverless taxis, Bahrozyan said. Autonomous taxis currently operate in several cities around the world, mostly in the US and China.  Cruise operates commercial robotaxis in US cities like San Francisco, but Dubai would be the first launch of the cars outside the US, Bahrozyan said.  “We are doing our own set of tests and trials in Dubai… every city has its own characteristics,” Bahrozyan said in an interview with CNN. “We have weather conditions that are certainly different than the US.”  RTA plans to roll out 4,000 self-driving taxis by 2030, adding to the fleet of 12,000 traditional taxis in the city. Rides are expected to be slightly more expensive than an ordinary taxi but in the same price range as a private car like Uber.  Cruise entered a contract with the RTA for 15 years, and after this period the taxi market may open up to competitors. Bahroyzyan said he foresees autonomous vehicles eventually making up the majority of the Middle East tourist hub’s taxi fleet.  A year after GM’s Cruise robotaxis were launched in California, the company was forced to cut its fleet in half in the state following a series of collisions. The collisions outlined the potential challenges of driverless cars.  Bahroyzyan said there will be “zero compromise on safety.”  Dubai issued a law in April to regulate autonomous vehicles, setting benchmarks for technical, operational and safety aspects of cars. Selling and buying autonomous cars was also regulated.  WeRide, a Chinese autonomous car technology company began trialing robotaxis in the UAE’s capital, Abu Dhabi, in 2022.  In July, the UAE granted WeRide a license to trial all its vehicles, from robobuses to robosweepers, but the company began testing certain routes a year prior.  The Middle East is a “key focus area” for driverless cars and WeRide said it hopes to deepen its presence in the region. WeRide also has a collaboration with the Saudi Artificial Intelligence Company to develop a robobus route.   Saudi’s Transport General Authority introduced self-driving buses during the 2023 Hajj season in July, shuttling pilgrims in Mecca, according to local media. "
20230105,foxnews,"Oregon man accused of chewing off elderly man's ear, part of face claims victim was ‘robot’ trying to kill him","The Oregon man accused of biting off an elderly man’s ear and chewing part of his face on a train platform early Tuesday told police he thought the victim was a ""robot"" trying to murder him. Koryn Kraemer, 25, pleaded not guilty in court Wednesday on a second-degree assault charge&nbsp;in connection with the bloody attack that unfolded around 2:15 a.m. on the Cleveland Avenue MAX platform in Gresham, FOX12 Oregon reported. Responding officers found a large amount of blood and Kraemer still on top of the 78-year-old victim from Hillsboro, authorities have said. First responders reported that the victim’s injuries were so severe they were able to see his skull. When officers pulled Kraemer off the victim, he reportedly spat out the victim’s flesh and thanked the police for saving him from the ""robot,"" referring to the victim. OREGON WOMAN HELD WITHOUT BAIL AFTER VIDEO SHOWS HER ALLEGEDLY PUSHING 3-YEAR-OLD ONTO TRAIN TRACKS  Kraemer told police in an interview that he believed the victim was a robot trying to kill him because of how he ""smelled."" He also told police that he had consumed alcohol, cannabis and fentanyl pills prior to the incident, the report said.  NYC MAN ARRESTED IN ATTEMPTED RAPE OF WOMAN ABOARD SUBWAY HAD 11 PRIOR ARRESTS, ACTIVE BENCH WARRANT Kraemer had moved from Georgia to Portland in November but told police that he was recently evicted. He said that he had no phone number, email address and was unemployed, according to court documents obtained by the station.  Kraemer had two previous arrests in Georgia, but neither resulted in convictions, the report said. CLICK HERE TO GET THE FOX NEWS APP He is being held without bail."
20230401,foxnews,Elon Musk’s warnings about AI research followed months-long battle against ‘woke’ AI,"Tesla and SpaceX CEO Elon Musk has been waging a battle for the last several months over what he called ""woke"" artificial intelligence, a fight that appears to have factored into his call for a six-month pause in the development of next generation AI systems. Musk was one of several signatories to a letter this week that warned of advanced AI technology that could pose ""profound risks to society and humanity."" The letter said one of those risks is that AI might be used to ""flood our information channels with propaganda and untruth."" The letter was signed by several notable technology experts, and it’s not clear who might have pushed for the inclusion of that specific phrase. But it jibes with the public fight Musk has been having since late last year over the ability of AI to constrain what people can say and read on digital platforms – a fight that involves a company Musk had a role in launching. In 2015, Musk co-founded OpenAI, the company that released GPT-4 this month, a few weeks before the letter was released. GPT-4 is the latest edition of a language system that underlies the company’s ChatGPT tool that can receive inputs and generate human-sounding outputs. ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON ‘GIANT AI EXPERIMENTS’: ‘DANGEROUS RACE’  Musk left the board of OpenAI in 2018 and explained that one reason why he left was that the company was chasing profits instead of serving as an open-source ""counterweight"" to Google. ""Now it has become a closed source, maximum-profit company effectively controlled by Microsoft,"" Musk tweeted in February. He was referring to the $10 billion it received from Microsoft, an infusion that OpenAI CEO Sam Altman has defended by noting that Microsoft doesn’t sit on the board of his company and does not control it in any way. But Musk’s opposition to OpenAI went beyond its funding model. Late last year, Musk made it clear he opposes the way OpenAI has been developing its AI chatbot. In December, Altman defended the rules developed to limit the ability of ChatGPT to produce controversial or insensitive outputs. ""’AI needs to do whatever I ask’ and ‘I asked the AI to be sexist and it was, look how awful!’ are incompatible positions,"" Altman tweeted. AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  Musk tweeted in reply, ""The danger of training AI to be woke – in other words, lie – is deadly."" In February, Musk had a similar reaction when a Musk ally tweeted that ChatGPT lists former President Trump and Musk himself as ""controversial"" figures, while President Biden and Bill Gates are not. Musk replied by tweeting, ""!!"" Also in February, Musk replied to a tweet that showed ChatGPT was unwilling to write a poem about the positive attributes of Donald Trump because it can’t produce content that is biased or partisan, but was willing to write a poem about President Biden. ""It is a serious concern,"" Musk replied. ARTIFICIAL INTELLIGENCE ‘GODFATHER’ ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT’S NOT INCONCEIVABLE' OpenAI has a set of rules for using ChatGPT that get to the heart of Musk’s complaint about a ""woke"" AI system. According to the company, its tools can’t be used to generate ""hateful, harassing, or violent content."" That includes content that ""expresses, incites, or promotes hate based on identity,"" ""intends to harass, threaten, or bully"" someone, or ""promotes or glorifies violence or celebrates the suffering or humiliation of others."" Just days after Musk tweeted ""!!,"" press reports said Musk was recruiting AI experts to create his own non-woke chat AI system. A spokesperson for Musk at SpaceX declined to respond to a request for comment for this story. But one policy watcher in Washington agreed that Musk’s open battle against woke AI seems to be a significant factor in his call for an AI development pause.  ""Elon has been on the front line of the Twitter files, so he’s seen how bad the censorship can be,"" said Jake Denton, research associate in the Heritage Foundation’s Tech Policy Center. ""This is just so evident to everyone in this space… that [AI tech] is exceeding the pace of our ability to control it."" Denton said that while AI will have countless applications in the future, the early application most people are seeing today are things such as ChatGPT. ""The consumer-known issue is the ChatGPT bias,"" he said. ""It’s obviously on a path to replace search. The average person will soon go to a chat-based AI system rather than a search bar."" ""And that means the response, the information that they get when they enter a search query, is going to be… a curated thing with the restrictions of the AI company reflected in that answer,"" he added. ""That’s a major danger."" CLICK HERE TO GET THE FOX NEWS APP The letter signed by Musk and others called on governments to enforce a pause on AI research, a position that conservative groups like the Heritage Foundation seem inclined to support given the evidence of bias in current AI systems, even though it isn’t normally looking for a government solution. ""I think regulation is absolutely what’s needed, government intervention in some capacity,"" he said. ""I don’t think we should move forward without such a thing. Our future shouldn’t be decided by unelected elites in Silicon Valley."""
20221227,foxnews,"Indiana man allegedly kills, dismembers father after believing him to be robot: 'Had to shoot at it'","An Indiana man was slapped with 10 charges after he allegedly fatally shot his father and dismembered his corpse after believing him to be a robot. Shawn Hays, 53, of Lawrence County, Indiana, was arrested Dec. 20 after deputies responded to a welfare check call on his 73-year-old father Rodney Hays, according to a probable cause affidavit cited by local Fox affiliate WXIN. The person who called the police informed them that Hays told them that he had shot and mutilated his father because he had been turned into a robot. NORTH CAROLINA POLICE ARREST SUSPECT AFTER VICTIM FOUND ALLEGEDLY ‘BEATEN AND PARTIALLY SCALPED’  When deputies arrived at the residence, they reportedly found Hays ""hastily attempting to exit the property in a silver Chevrolet pickup."" There was also reportedly a shotgun in the vehicle, which officers managed to remove while distracting him. When they asked about his father, Hays reportedly told them that it was not actually his father, but rather a robot that resembled his father. ""It's a robot that looks like a human...laying over there. I had to shoot at it to destroy it."" ACCUSED PURDUE ROOMMATE KILLER DEEMED INCOMPETENT TO STAND TRIAL When asked where his father was, Hays gestured toward the house behind the vehicle. The report states that Hays became combative when officers asked him to exit the truck.  WOMAN ARRESTED IN SOUTH CAROLINA AIRPORT AFTER ATTACKING HER HUSBAND OVER ‘INDECENT’ PHOTOS ON HIS PHONE: POLICE During the altercation, Hays reportedly told the officers, ""It's a robot that looks like a human...laying over there. I had to shoot at it to destroy it."" Hays had also reportedly raised concerns from others with Facebook posts about his father's robotic identity.  Rodney Hays reportedly had been shot in the head and chest. His body, which officers reportedly found on the lawn, had allegedly been partially dismembered and mutilated. CLICK HERE TO GET THE FOX NEWS APP Shawn Hays faces multiple criminal charges, including murder, abuse of a corpse, domestic battery and aggravated battery according to court records."
20230710,cbsnews,"How Google's ""Don't be evil"" motto has evolved for the AI age | 60 Minutes","""I've always thought of AI [artificial intelligence] as the most profound technology humanity is working on. More profound than fire or electricity or anything that we've done in the past,"" said Sundar Pichai, the CEO of Google and its parent company Alphabet.The 51-year-old Pichai gave 60 Minutes correspondent Scott Pelley rare access to the inner workings of Google's AI development, which includes robots that have acquired skills through machine learning and Project Starline, an AI video conferencing experience Google is developing to allow people to feel as though they are together, despite being in different locations. Perhaps Google's most anticipated and noteworthy foray into AI is its chatbot, Bard. The company presently calls it an experiment, in part to do more internal testing. Bard notably made a mistake when Google debuted the program in February. When Bard was first released, it did not look for answers on the internet, and instead it relied on a self-contained and mostly self-taught program.In May, Google released an advanced version of Bard that can write software and connect to the internet. Google says it's developing even more sophisticated AI models.""[AI] gets at the essence of what intelligence is, what humanity is,"" Pichai told Pelley. In the video below, Pelley asked Pichai how Bard will affect Google search which runs 90% of internet queries and is the company's most profitable division.When Google filed for its initial public offering in 2004, its founders wrote that the company's guiding principle, ""Don't be evil"" was meant to help ensure it did good things for the world, even if it had to forgo some short term gains. The phrase remains in Google's code of conduct. In April, Pichai told 60 Minutes he was being responsible by not releasing advanced models of Bard, in part, so society could get acclimated to the technology, and the company could develop further safety layers.One of the things Pichai told 60 Minutes that keeps him up at night is Google's AI technology being deployed in harmful ways. Google's chatbot, Bard, has built in safety filters to help combat the threat of malevolent users. Pichai said the company will need to constantly update the system's algorithms to combat disinformation campaigns and detect deepfakes, computer generated images that appear to be real. As Pichai noted in his 60 Minutes interview, consumer AI technology is in its infancy. He believes now is the right time for governments to get involved.""There has to be regulation. You're going to need laws…there have to be consequences for creating deep fake videos which cause harm to society,"" Pichai said. ""Anybody who has worked with AI for a while…realize[s] this is something so different and so deep that, we would need societal regulations to think about how to adapt.""Adaptation that is already happening around us with technology that Pichai believes, ""will be more capable ""anything we've ever seen before.""Soon it will be up to society to decide how it's used and whether to abide by Alphabet's code of conduct and, ""Do the right thing.""You can watch Scott Pelley's two-part report on Google, below.The video at the top was originally published on April 16, 2023 and was produced by Keith Zubrow and edited by Sarah Shafer Prediger"
20230710,foxnews,"'Alarming' misuse of AI to spy on activists, journalists 'under guise of preventing terrorism': UN expert","A United Nations expert warned about an ""alarming"" trend of ""using security rhetoric"" to justify ""intrusive and high-risk technologies,"" including artificial intelligence, to spy on social rights activists and journalists. U.N. expert Fionnuala Ní Aoláin called for a moratorium on AI development, among other advanced technologies like drones, until ""adequate safeguards are in place,"" according to a March 2023 report that was presented to the Human Rights Council. ""Exceptional justifications for the use of surveillance technologies in human rights 'lite' counter-terrorism often turn into mundane regular use,"" Ní Aoláin said in a statement after the report's release. WHAT IS AI? Without meaningful oversight, she argued, countries and private actors can use AI-power tech with impunity ""under the guise of preventing terrorism.""&nbsp;  ""Abusive practices are hardwired into counter-terrorism and countering violent extremism,"" said Ní Aoláin, a University of Minnesota professor and a U.N. Human Rights Council-appointed special rapporteur. Creating AI guardrails and safeguards is a daunting task that the U.S., like many other governments around the world, is trying to tackle, but it is an issue that many experts argued is unprecedented. WHO IS WATCHING YOU? AI CAN STALK UNSUSPECTING VICTIMS WITH ‘EASE AND PRECISION’: EXPERTS Generative AI has the potential to create a utopia, or the power to plunge a country into a dystopia, experts have claimed. ""AI is one of the more complex issues we have ever tried to regulate,"" Kevin Baragona, founder of DeepAI.org, told Fox News Digital in a previous interview. ""Based on current governments' struggle to regulate simpler issues, it's looking hard to be optimistic we'll get sensible regulation."" WATCH Fionnuala Ní Aoláin Address UN Human Rights Council  However, banning it altogether, as Italy originally attempted to do, would set a nation back for the next century, Baragona said. ""In the absence of regulation, the cost to human rights can only increase with no end in sight,"" Ní Aoláin said. AI-ASSISTED FRAUD SCHEMES COULD COST TAXPAYERS $1 TRILLION IN JUST 1 YEAR, EXPERT SAYS AI was among a handful of ""high-risk technologies"" that she discussed. The topic was broken out as its own subsection in the 139-page report.&nbsp; ""AI has the properties of a general-purpose technology, meaning that it will open up wide-ranging opportunities for application,"" she wrote in her report.&nbsp; WATCH EXAMPLES OF HOW AI-ASSISTED SCAMS CAN WORK  AI PUBLIC SAFETY INVESTMENT TO GROW TO $71B BY 2030 TO ‘PREDICT CRIME, NATURAL DISASTERS’: REPORT The technology is already being implemented in social, economic, political and military actions, and is integrated into law enforcement, national security, criminal justice and border management systems. Several cities across the country tested various applications of AI in pilot programs. &nbsp; At the heart of AI are algorithms that can create profiles of people and predict likely future movements by utilizing vast amounts of data – including historic, criminal justice, travel and communications, social media and health info. It can also identify places as ""likely sites of increased criminal or terrorist activity"" and flag individuals as alleged suspects and future re-offenders, according to Ní Aoláin's report.&nbsp; WHAT ARE THE DANGERS OF AI? FIND OUT WHY PEOPLE ARE AFRAID OF ARTIFICIAL INTELLIGENCE  ""The privacy and human rights implications of this kind of data collection and predictive activity are profound for both derogable and non-derogable rights,"" she said.&nbsp; ""The Special Rapporteur highlights her profound disquiet at AI assessments being used to trigger State action in counter-terrorism contexts, from searching, questioning, arrest, prosecution and administrative measures to deeper, more intrusive surveillance.&nbsp; ""AI assessments alone should not be the basis for reasonable suspicion given its inherently probabilistic nature."" CLICK HERE TO GET THE FOX NEWS APP&nbsp;"
20230710,foxnews,"Senate to receive classified brief on AI threats and national security, Schumer says","All 100 senators are invited to sit for a classified briefing this week on artificial intelligence and its effects on global and national security, Senate Majority Leader Chuck Schumer announced in a weekend letter to colleagues. Schumer pointed out that it will be the first session of its kind, as Congress works to get ahead of the rapidly advancing technology. ""This Tuesday we will have a classified all-senators briefing with the Department of Defense and Intelligence Community to learn how we’re using and investing in AI to protect our national security and learn what our adversaries are doing in AI,"" the New York Democrat wrote on Sunday. LAWMAKERS RATTLED BY AI-LAUNCHED NUKES, DEMAND ‘HUMAN CONTROL’ IN DEFENSE POLICY BILL  ""This will be the first-ever classified all-senators briefing on national security and AI,"" he wrote. Briefers will include senior members of the Defense Department including Director of National Intelligence Avril Haines and Deputy Defense Secretary Kathleen Hicks, in addition to White House Office of Science and Technology Policy Director Arati Prabhakar, Director of the National Geospatial Intelligence Agency Trey Whitworth, and Craig Martell, the Pentagon's Chief Digital and AI Officer. AI has become a hot topic on Capitol Hill in recent months. Concerns about falling behind other countries and what kind of regulatory barriers to impose on it has spurred a flurry of legislation and hearings across both the House and Senate. CONGRESS PUSHES AGGRESSIVE USE OF AI IN THE FEDERAL GOVERNMENT, SAYS AI 'UNDER-UTILIZED' IN AGENCIES  Schumer had announced months ago that he would work to put together a regulatory framework for AI aimed at protecting online user privacy while not stifling innovation. He also convened a bipartisan group of four senators including himself and Sens. Mike Rounds of South Dakota, Todd Young of Indiana and Martin Heinrich of New Mexico to work out a comprehensive plan on how to handle AI. UN SECURITY COUNCIL HOLDS FIRST-EVER MEETING ON AI AS CONCERNS ABOUT RISK TO PEACE GROW  That group set up a series of AI learning sessions, including Tuesday’s classified briefing, and Schumer promised more would be in the pipeline. CLICK HERE TO GET THE FOX NEWS APP ""Our job as legislators is to listen to the experts and learn as much as we can so we can translate these ideas into legislative action, with our committees continuing to serve as the key drivers. I look forward to hearing from these experts and I encourage you to attend,"" Schumer wrote in his letter. Other AI efforts in Congress include a bill to prevent AI from being able to autonomously launch a weapons attack, as well as a flurry of legislation to establish various regulatory and advisory panels on the advanced technology. Schumer's announcement of AI information sessions follows a similar effort by House Speaker Kevin McCarthy and Minority Leader Hakeem Jeffries to get the lower chamber up to speed on the sector. Chad Pergram contributed to this report"
20230710,cnn,‘WarGames’ anticipated our current AI fears 40 years ago this summer,"Forty years ago this summer, a new movie floated the prospect of the world being destroyed by artificial intelligence run amok – anticipating current anxieties about where the technology could potential lead – a year before the “Terminator” introduced the futuristic threat known as Skynet. At the time, “WarGames” spoke to another issue very much on the minds of movie-goers: The danger of nuclear annihilation during the Cold War, years before the Berlin Wall and Soviet regime fell. Those concerns also surfaced later that year in a more bracing, less fanciful manner when ABC aired the TV movie “The Day After,” a broadcast that possessed such impact advertisers stayed away and the Reagan administration pressured the network not to run it. “WarGames” starred a very young Matthew Broderick as the genius teenage computer hacker who inadvertently taps into the Pentagon’s computer, challenging it to a “game” of “Global Thermonuclear War” that risks becoming all too real. Eventually, the computer comes to the realization that mutually assured destruction – the deterrent logic of the time, appropriately abbreviated as MAD – is a pointless exercise, saying in its eerie artificial voice, “A strange game. The only winning move is not to play.” That message resonated with those who worried about uneasy relationship between the US and Soviet Union at the time. Yet a recent re-viewing of the movie (which co-starred Ally Sheedy, two years before she gained additional teen immortality in “The Breakfast Club”) makes its spin on AI seem even more pointed and timely – the idea that in seeking an emotionally detached, people-free solution to a problem, we might sow the seeds for our own destruction. The story gets set in motion because military brass fret about human operators exhibiting reluctance to launch nuclear strikes, despite what appear to be valid orders. The solution: A computer system that will remove them from the equation, championed by a character played by Dabney Coleman, the go-to bad-guy bureaucrat (see “9 to 5”) of the era. In the movie’s payoff, Broderick’s teenage hero outsmarts the computer by essentially tricking it into gaming out “global thermonuclear war” and recognizing its futility. The AI, in this case, is more sensible than its creators, as opposed to the more malevolent force featured in the new “Mission: Impossible” sequel. Yet the apprehension that has entered the chat – as underscored by recent congressional hearings regarding the perils associated with the technology – is that future iterations of AI won’t be so benevolent, and might actually be smarter than the resourceful teenagers that we can deploy to thwart them. “WarGames” thus plays like a movie of its time while possessing aspects that presciently echo into ours, going beyond a plot that could easily have rendered it a Cold War relic. And the film fits nicely alongside others from the period that explored similar themes and enjoyed longer shelf lives, some via sequels, including the aforementioned “Terminator” and “Tron.” As Ryan Britt wrote recently at the Inverse, what really makes “WarGames” scary isn’t that the computer is evil, but rather its potentially dire inability to recognize nuance the way a human can. “In ‘WarGames,’ the computer doesn’t understand the difference between a game and real life,” Britt noted. From that perspective, it’s a movie with more than one message, dealing with questions that aren’t so much back, with apologies to the Terminator, but as reality has caught up with science fiction, simply continued to evolve. “WarGames” is currently available to rent or buy via AMC+ and Amazon Prime Video. "
20230527,foxnews,Biden Education Department worried AI in the classroom might be used to spy on teachers,"The Department of Education is worried that artificial intelligence systems could be used to surveil teachers once the systems are introduced into the classroom and warned in a new report that allowing that to happen would make teachers’ jobs ""nearly impossible."" The department released a report this week on ""Artificial Intelligence and the Future of Teaching and Learning,"" which also argued that AI should never be used to replace human teachers. The report is aimed at assessing the prospects of expanding AI into the classroom. While it says that AI could make teaching more efficient and help tailor lesson plans to individual students, it warned that AI might also expose teachers to increased surveillance once deployed. NANCY MACE SEES AI AS A CHANCE TO IMPROVE BORDER SECURITY: ‘A LOT OF OPPORTUNITY’  ""When we enable a voice assistant in the kitchen, it might help us with simple household tasks like setting a cooking timer,"" the report said. ""And yet the same voice assistant might hear things that we intended to be private. This kind of dilemma will occur in classrooms and for teachers."" The report envisions the possibility of AI being used in live classroom settings to capture data that helps teachers do their jobs, such as by recommending certain resources based on the topics being taught, but that comes with the added risk for teachers. ""The same data might also be used to monitor the teacher, and that monitoring might have consequences for the teacher,"" it said. ""Achieving trustworthy AI that makes teachers’ jobs better will be nearly impossible if teachers experience increased surveillance."" BIDEN EDUCATION SECRETARY ROASTED FOR CLAIMING ‘TEACHERS KNOW WHAT IS BEST’ FOR PARENTS' KIDS: ‘CRAZY’  The department concluded that when AI is considered for use in the classroom, efforts should be made to ensure ""adequate"" protections against teacher surveillance. Other questions that need to be asked are whether AI is easing the teaching burden, whether teachers have control over AI-enabled tools, and how AI might be used to ""improve equity, reduce bias, and increase cultural awareness."" The Biden administration’s push for AI systems that avoid teacher surveillance has the potential to reignite the political fight over how much authority teachers have over students, and what rights parents have to know what is being taught. Just last week, Secretary of Education Miguel Cardona tweeted that ""teachers know what is best for their kids,"" and ""we must trust teachers,"" which led to complaints from prominent Republicans that parents need to have substantive input into school curricula. The administration has also been under attack from Republicans and parents groups after the Department of Justice released a memo in 2021 that urged officials to investigate threats of violence against local school administrators and teachers. That memo came out after the National School Boards Association urged the administration to consider these threats as a form of ""domestic terrorism."" EVERYTHING YOU NEED TO KNOW ABOUT ARTIFICIAL INTELLIGENCE: WHAT IS IT USED FOR?  The group later apologized for using that term, but Republicans have since accused the Biden administration of siding with teachers and working against parents who seek information about what their kids are being taught and aren’t always getting answers. WHAT IS AI? The Department of Education’s report also stressed several times that AI should never be a substitute for human teachers. ""Some teachers worry that they may be replaced — to the contrary, the Department firmly rejects the idea that AI could replace teachers,"" it said. ""At no point do we intend to imply that AI can replace a teacher, a guardian, or an educational leader as the custodian of their students’ learning."" The report recommended that as AI becomes a part of the classroom, policymakers should work to ""always center educators (ACE)."" CLICK HERE TO GET THE FOX NEWS APP ""Practically speaking, practicing ‘ACE in AI’ means keeping a humanistic view of teaching front and center,"" it said. ""ACE leads the Department to confidently respond ‘no’ when asked ‘will AI replace teachers?’"""
20230527,cnn,Lawyer apologizes for fake court citations from ChatGPT,"The meteoric rise of ChatGPT is shaking up multiple industries – including law, as one attorney recently found out.  Roberto Mata sued Avianca airlines for injuries he says he sustained from a serving cart while on the airline in 2019, claiming negligence by an employee. Steven Schwartz, an attorney with Levidow, Levidow & Oberman and licensed in New York for over three decades, handled Mata’s representation.  But at least six of the submitted cases by Schwartz as research for a brief “appear to be bogus judicial decisions with bogus quotes and bogus internal citations,” said Judge Kevin Castel of the Southern District of New York in an order.  The fake cases source? ChatGPT.   “The court is presented with an unprecedented circumstance,” Castel wrote in a May 4 order.  Among the purported cases: Varghese v. China South Airlines, Martinez v. Delta Airlines, Shaboon v. EgyptAir, Petersen v. Iran Air, Miller v. United Airlines, and Estate of Durden v. KLM Royal Dutch Airlines, all of which did not appear to exist to either the judge or defense, the filing said.  Schwartz, in an affidavit, said that he had never used ChatGPT as a legal research source prior to this case and, therefore, “was unaware of the possibility that its content could be false.” He accepted responsibility for not confirming the chatbot’s sources. Schwartz is now facing a sanctions hearing on June 8. In an affidavit this week, he said he “greatly regrets having utilized generative artificial intelligence to supplement the legal research performed herein and will never do so in the future without absolute verification of its authenticity.”  In late April, Avianca’s lawyers from Condon & Forsyth penned a letter to Castel questioning the authenticity of the cases.  In an affidavit filed Thursday, fellow attorney Peter Loduca said he “had no reason to doubt the sincerity” of Schwartz’s research and that he had no role in the research.    Schwartz was ordered to show cause why he shouldn’t be sanctioned “for the use of a false and fraudulent notarization,” in an affidavit filed on April 25.  Schwart’s affidavit Wednesday contained screenshots of the attorney appearing to confirm the authenticity of the case with ChatGPT.  “is varghese a real case,” Schwartz asked the chatbot.  “Yes,” ChatGPT doubled down, it “is a real case.” Schwartz then asked for its source. The chatbot again claimed the false case was real. “I apologize for the confusion earlier,” ChatGPT replied. “Upon double-checking, I found the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused.” When Schwartz asked the chatbot if any other cases were fake, ChatGPT replied the other cases “are real” and could be found on “reputable legal databases.”  CNN has reached out to Schwartz and Loduca for comment."
20220713,cbsnews,"The best robot vacuums deals you can still get after Amazon Prime Day: Roomba, Samsung Jet Bot, more","Amazon Prime Day 2022 has come and gone. But you can still score savings on a robot vacuum, the smart-home appliance that'll clean your floors while you're out living your life. Shop our selection of the best Amazon deals on top-rated, smart robot vacuums post-Amazon Prime Day.     Top products in this article:   iRobot Roomba i7+, $768 (reduced from $1,000) iRobot Roomba i3+ EVO robot vacuum with automatic dirt disposal, $493 (reduced from $550) Samsung Jet Bot+ robot vacuum with clean station, $528 (reduced from $800) Keep scrolling to discover the best post-Amazon Prime Day robot vacuum deals by iRobot, Samsung and other top brands. All featured robot vacs boast Amazon user-review ratings of four stars (out of five) or higher.    	The best robot vacuum deals on Amazon Ready to clean up on savings? Let's get to the robot-vacuum deals.    	iRobot Roomba i7+: $768The Roomba i7+, one of the more advanced robot vacuum models you can buy, features a three-stage cleaning system with powerful suction that's great at picking up pet hair. It's smart enough to learn the layout of your home and clean in neat rows, while staying out of areas you don't want it to go. This Amazon Prime Day favorite vacuum is low maintenance, too: The included cleaning base requires emptying once every 60 days. iRobot Roomba i7+, $768 (reduced from $1,000)  	iRobot Roomba i3+ EVO robot vacuum with automatic dirt disposal: $500 The iRobot Roomba i3+ EVO uses ""Imprint Smart Mapping"" technology to map your home. Use your connected phone to direct the Wi-Fi-enabled robot vacuum to clean any room you want. You can even schedule a future clean. This Roomba is compatible with Amazon Alexa and Google Assistant.       The smart appliance learns your cleaning habits, and can suggest extra cleanings during peak pollen and pet-shedding seasons. And don't even worry about dumping out your dustbin. The Roomba i3+ EVO features iRobot's ""Clean Base Automatic Dirt Disposal"" system, and empties your accumulated dirt into an enclosed bag.  iRobot Roomba i3+ EVO robot vacuum with automatic dirt disposal, $500 (reduced from $550) Samsung Jet Bot+ robot vacuum with Clean Station: $528While this more affordable Jet Bot+ robot vacuum by Samsung doesn't feature 3D recognition with AI, it does have LiDAR sensor navigation, five watts of adjustable suction and the all-important self-emptying Clean Station. Mapping can be controlled via your phone with the Samsung SmartThings App. Remotely check the Jet Bot+'s cleaning status, pause or stop cleaning and view the cleaning history. It's 34% off right now. Samsung Jet Bot+ robot vacuum with clean station, $528 (reduced from $800)     	iRobot Roomba 694 robot vacuum: $249The Roomba 694 is Wi-Fi-enabled. Control the vac with your connected smartphone or device via the iRobot Home app. The Roomba 694 has a 90-minute run time before it automatically docks and recharges.    On Amazon, one reviewer praised the iRobot device's ability to keep a pet-friendly household clean. ""We have two dogs, one that sheds moderately,"" the customer wrote. ""I purchased in hopes that it at least would help between regular vacuuming. I vacuumed first with my Dyson then set it free. When it was done with the job, I didn't expect much in the dust trap... I was wrong! It was full! Super impressed.""    iRobot Roomba 694 robot vacuum, $249 (reduced from $274)    	 	 	Shark Ion robot vacuum: $222This Shark robot vac features side brushes, channel brushes and a multi-surface brush roll to handle dirt and debris on all surfaces. Use the SharkClean app on your connected smartphone or device to control when -- and where -- your robot vacuum cleans. The vac offers 120 minutes of run time. Choose from three colors.Shark Ion robot vacuum (gray), $222 (reduced from $230)    	Lefant M210 robot vacuum cleaner: $120 Click on the Amazon coupon offer to save $50 on this robot vacuum by Lefant. Your savings will be applied at checkout.  Lefant's M210 robot vac features built-in, anti-collision infrared sensors (so it won't bang into its surroundings). The robot vacuum detects ""stuck areas,"" and adjusts its cleaning path automatically. Download the Lefant app to pair the Wi-Fi-enabled vac with your smartphone or device -- the better to control the appliance remotely. The robot vacuum features 100 minutes of run time.  Lefant M210 robot vacuum cleaner, $120 after coupon (reduced from $130)    	 	Laresar Grande 1 self-charging robotic vacuum: $150 This Laresar smart robot can vacuum and mop your floors (water tank sold separately). The machine is equipped with sensors that detect stairs and prevent falls. The robot vacuum is Wi-Fi compatible and can be controlled by smartphone.  Download the Laresmart app to schedule cleanings, swap cleaning modes and control cleaning direction. Laresar Grande 1 self-charging robotic vacuum, $150 after coupon (reduced from $250)   	More robot vacuums to consider These top-rated robot vacs aren't on sale right now on Amazon, but they're still worth a look. Jet Bot AI+ robot vacuum with object recognitionThe Samsung Jet Bot AI+ robot vacuum has a bunch of cool features, including 30 watts of adjustable suction, 3D object recognition with AI and powerful LiDAR navigation. This robot vacuum can recognize what objects to avoid, so you won't have to deal with it constantly crashing into the couch or a pile of laundry on the floor. Have a very specific clean in mind? Mapping can be controlled via your phone. You can even watch your robot vacuum operate no matter where you are, using Samsung's SmartThings App. The Jet Bot AI+ comes with a front camera that can live stream in real time. It boasts its own no-touch ""Clean Station"" that will empty your dustbin using Samsung's Air Pulse technology. The vacuum's 0.2 liter dustbin is fully washable. Though this Jet Bot model is not marked as on sale on Amazon, this is still something of a deal. Its current list price is about $100 under previous list prices we've seen on the site.  Jet Bot AI+ robot vacuum with object recognition, $1,100   	Samsung Jetbot mop Need a robot cleaning solution for non-carpeted areas? The Samsung Jetbot Mop robot mop is good for cleaning tile, vinyl, laminate and hardwoods. Plus, you can use it in hand-held mode to clean bathroom walls, countertops and more.  Samsung Jetbot mop, $200iRobot Roomba j7+ self-emptying robot vacuum with Braava Jet M6 robot mopOn Amazon, you can buy a 4.4-star-rated combo that pairs the iRobot Roomba 7+ with the Braava Jet M6 robot mop. The bundle is currently unavailable, but click the below button to check in on a potential restock. The iRobot Roomba 7+ uses an edge-sweeping brush to get into corners. It features dual, multi-surface rubber brushes that flex to adjust to different floor types -- and help prevent them from getting tangled with pet hair. Billed as a self-cleaning vac, the Roomba 7+ automatically empties itself into enclosed bags.The Braava Jet M6 robot mop, also by iRobot, delivers a jet spray that can help you tackle messes on finished hard floors of stone, tile, wood and more. It was sold out at last look, but check below for restocks.iRobot Roomba j7+ self-emptying robot vacuum with Braava Jet M6 robot mopRelated Content from CBS Essentials The best post-Prime Day deals at other retailers you can still shop: Best Buy, Walmart, Wayfair, Samsung and moreStill on sale at Amazon: The best deals you can get right now, the day after Prime Day 2022Still here: Get great deals on Samsung TVs, smartphones, laptops and more post-Amazon Prime DayThe best new washer and dryer features in 2022Nordstrom Anniversary Sale 2022: The best home deals on kitchen appliances, decor and more"
