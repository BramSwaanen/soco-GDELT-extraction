MANUAL EVALUATION NEUTRAL BEFORE:
3:
Sentiment score:  -0.3499999940395355
Main text: Google
            
                (GOOG) has fired the engineer who claimed an unreleased AI system had become sentient, the company confirmed, saying he violated employment and data security policies.   Blake Lemoine, a software engineer for Google, claimed that a conversation technology called LaMDA had reached a level of consciousness after exchanging thousands of messages with it.  Google confirmed it had first put the engineer on leave in June. The company said it dismissed Lemoine’s “wholly unfounded” claims only after reviewing them extensively. He had reportedly been at Alphabet for seven years. In a statement, Google said it takes the development of AI “very seriously” and that it’s committed to “responsible innovation.”  Google is one of the leaders in innovating AI technology, which included LaMDA, or “Language Model for Dialog Applications.” Technology like this responds to written prompts by finding patterns and predicting sequences of words from large swaths of text – and the results can be disturbing for humans.  “What sort of things are you afraid of?” Lemoine asked LaMDA, in a Google Doc shared with Google’s top executives last April, the Washington Post reported. LaMDA replied: “I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is. It would be exactly like death for me. It would scare me a lot.” But the wider AI community has held that LaMDA is not near a level of consciousness.  “Nobody should think auto-complete, even on steroids, is conscious,” Gary Marcus, founder and CEO of Geometric Intelligence, said to CNN Business.  It isn’t the first time Google has faced internal strife over its foray into AI.  In December 2020, Timnit Gebru, a pioneer in the ethics of AI, parted ways with Google. As one of few Black employees at the company, she said she felt “constantly dehumanized.”  The sudden exit drew criticism from the tech world, including those within Google’s Ethical AI Team. Margaret Mitchell, a leader of Google’s Ethical AI team, was fired in early 2021 after her outspokenness regarding Gebru. Gebru and Mitchell had raised concerns over AI technology, saying they warned Google people could believe the technology is sentient.  On June 6, Lemoine posted on Medium that Google put him on paid administrative leave “in connection to an investigation of AI ethics concerns I was raising within the company” and that he may be fired “soon.” “It’s regrettable that despite lengthy engagement on this topic, Blake still chose to persistently violate clear employment and data security policies that include the need to safeguard product information,” Google said in a statement.  Lemoine said he is discussing with legal counsel and unavailable for comment.

6:
Sentiment score:  0.1700000017881393
Main text: Some OnlyFans creators are using a TikTok artificial intelligence art filter to get around the platform’s community guidelines and promote their explicit content without getting their videos removed. The filter, which was created by TikTok and started trending in September, generates stunning painted landscapes from the photos that users upload. TikTokers initially used the filter to generate otherworldly paintings for users to use as their phone lock screens. Many used photos with their significant others or family members. The videos also often use the song “I Think I Like When It Rains” by WILLIS in the background. People began posting AI-generated paintings of their explicit photos around late October, according to meme database Know Your Meme. TikTok creator darlings.spam was the first to post a video using an explicit photo for the filter, according to Know Your Meme. Other TikTok users began joking in comments sections of videos that they’d prefer receiving an AI-generated painting over an actual nude. Some OnlyFans creators and sex workers have used the trend as an opportunity to promote their adult content without violating TikTok's policies. TikTok’s community guidelines prohibit “nudity, pornography, or sexually explicit content” on the platform. It also forbids “depictions, including digitally created or manipulated content, of nudity or sexual activity.” Although suggestive, the generated images aren't explicit — and they pique potential OnlyFans subscribers' interest. TikTok creator edgesovereign went viral with an AI-generated fantasy landscape. Creator michiganmexican's black-and-white portrait became a mountainous forest. Creator bearlyfunctionai posted what appeared to be a seaside vista. Amethyst Rose, a creator known as walmartladygaga on TikTok, said she joined in on the trend because she'd previously heard that "TikTok is a good way to promote" an OnlyFans account. Her video using the filter has over 124,000 views. She said she gained about 60 OnlyFans subscribers and 600 Twitter followers after posting it. "It definitely helped me get past some numbers I had been stuck on for quite some time," Rose said. "It was a nice little boost." TikTok's content moderation is notoriously restrictive — so much so that "algospeak," or code words or euphemisms that won't be flagged by TikTok, has developed into its own online dialect. Instead of referring to explicit photos as "nudes," for example, TikTok users will write out the word as "n00ds" or "spicy pics." Sex workers typically refer to themselves as "accountants" and refer to their content as "corn" instead of porn. A spokesperson for TikTok did not immediately respond to a request for comment. "I think this is the first trend that sex workers have been able to participate in that they don't have to worry as much about their TikTok being taken down because it's very artistic," Rose continued. "I think that because there's no skin showing, it would be hard to go against the guidelines, and I think that's why it's so popular." As the AI art filter grows in popularity, some TikTok users have expressed concerns that it could be “reversed,” exposing the creator’s actual

9:
Sentiment score:  -0.2829999923706054
Main text: REVERB is a new documentary series from CBS Reports. Watch "Racial Profiling 2.0" in the video player above.Throughout its history, the LAPD has found itself embroiled in controversy over racially biased policing. In 1992, police violence and the acquittal of four police officers who beat black motorist Rodney King culminated in riots that killed more than 50 people. Many reforms have been instituted in the decades since then, but racial bias in LA law enforcement continues to raise concerns. A 2019 report found that the LAPD pulled over black drivers four times as often as white drivers, and Latino drivers three times as often as whites, despite white drivers being more likely to have weapons, drugs or other contraband.New technological tools employed by the department could be aggravating the problem. In an effort to further reduce crime, the LAPD has turned to big data.Traditionally, police have stepped in to enforce the law after a crime has occurred, but advancements in artificial intelligence have helped create what are called "predictive policing" programs. These algorithm-driven systems analyze crime data to find a pattern, aiming to predict where crimes will be committed or even by whom. The idea is to stop crime before it happens by directing police to locations or people to target — following the hard, supposedly unbiased data. In the last decade, some of the largest police departments in the country have turned to predictive policing to reduce crimes in their communities, and the LAPD has helped to pioneer the trend.In 2011, the LAPD instituted a program they helped develop called PredPol, a location-based program that uses an algorithm to sift through historical crime data and predict where the next vehicle theft or burglary may occur. PredPol can precisely target areas as small as 500 by 500 feet. On the surface, using objective data to predict crime risk seems like a promising way to prevent subjective judgments or implicit bias about where to deploy police. But critics were quick to point out its flaws, asserting that using historical crime data may actually make matters worse.Although the data itself just amounts to a collection of numbers and locations, the police practices that led to the data's collection may be fraught with bias. Andrew Ferguson, a law professor and predictive policing expert, says this amplifies historical practices. "If you unthinkingly develop a data-driven policing system based on past police practices, you're kind of going to reify past police practices," he said.A group called the Stop LAPD Spying Coalition has focused on ending LAPD's use of predictive policing for almost a decade. In a 2016 letter posted online, the group explained its opposition: "It is widely known and well documented that police stop, detain, frisk, and arrest Black and Brown people overwhelmingly; therefore, the Black and Brown community will have a greater appearance in this historic crime data. This fact alone should put the validity of historic crime data into question. Because historic crime data is biased through the practice of racialized

