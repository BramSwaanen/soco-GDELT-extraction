************NEXT RUN*************
MANUAL EVALUATION POSITIVE AFTER:
8:
Sentiment score:  0.5080000162124634
Main text: Love it or hate it, so-called generative artificial intelligence has proved its ability to make at least one type of worker more productive on the job. Using the enhanced AI boosted the productivity of customer service representatives at a Fortune 500 software firm by 14%, according to the first study to examine the emerging technology's use by employees in a real workplace.Notably, human support agents reported resolving more customer queries per hour when aided by a custom-built tool powered by OpenAI's GPT technology. Assistance from the AI also improved customer sentiment, reduced the volume of requests for managerial intervention, and even improved employee retention, presumably because it allowed service agents to have more pleasant interactions with customers, according to Erik Brynjolfsson, one of the paper's authors and a senior fellow at the Stanford Institute for Human-Centered AI (HAI) and director of the Stanford Digital Economy Lab."There was less churn once they used this tool because it seemed workers were happier and enjoyed the job more," he told CBS MoneyWatch. "We wondered if it would push them harder, but it seems to be something workers liked. Customers were happier and I'm guessing as a call center operator, it's more enjoyable to interact with happy customers."The AI revolution: Google's artificial intelligence developers on what's next in the fieldService reps interacting with customers through text chats used the custom-built AI tool to help them find answers to client questions. Specifically, the AI read interactions between customers and support agents, and generated suggested responses for the agents to use. They also had the ability to accept or reject the AI-generated text answers. "It basically saw both what the customers and agents were saying and would give them strategically-timed hints or suggestions," Brynjolfsson said.For example, the AI tool would prompt reps to mention products or possible upgrades customers could make to solve their problems. It was also sensitive to tone, offering guidance to agents how to politely"communicate with clients. The AI was trained on thousands of client-agent interactions that were labeled as either successful or unsuccessful. "It tended to know what worked well with customers," Brynjolfsson said.The results: Agents generally solved problems faster, customers were happier, new employees got up to speed faster and the employer (which wasn't identified in the study) experienced less turnover. "People worry this is going to replace everything. I think by far the bigger effect is it augmenting us, like a calculator. It allows us to do things faster and more efficiently," Brynjolfsson said. "I say lean in and embrace it, learn to use these tools to be more effective." 

My judgement: positive makes sense.

11:
Sentiment score:  0.5529999732971191

Main text: BROCKTON â A six-year-old boy who just finished cancer treatment got a surprise over the weekend, a visit from Stop &amp; Shop robot Marty.Israel beamed as the robot stopped by to drop off some groceries for his family and say hello to his biggest fan. Israel was diagnosed with kidney cancer at four years old and now, two years after his diagnosis, he has completed his cancer treatment. His family said he spent almost a month in the hospital, undergoing a 13-hour surgery as part of his treatment. "We watched our son recover from pain that we don't even understand and he made it through like a trooper," his dad said. Israel also serves as Stop &amp; Shop's pediatric cancer ambassador. 

My judgement: positive makes sense.

6:
Sentiment score:  0.5239999890327454
Main text: The owner of Novo Nordisk, the drugmaker that gave the world Ozempic and Wegovy, is funding a new supercomputer powered by Nvidia’s artificial intelligence technology with a key aim of discovering new medicines and treatments. The Novo Nordisk Foundation has awarded France’s Eviden a contract to build what the computing company says will be one of the world’s most powerful supercomputers, able to process vast amounts of data using AI. It should provide “unprecedented potential to accelerate groundbreaking scientific discoveries in areas such as drug discovery, disease diagnosis and treatment,” Cédric Bourrasset, Eviden’s head of quantum computing, said in a statement. The supercomputer is expected to be ready for pilot projects before the end of the year and will be housed in Denmark’s national center for AI innovation. Named Gefion, the supercomputer will be available for use by researchers from Denmark’s public and private sectors, and will enjoy the backing of two of the hottest companies in the United States and Europe. Nvidia is now one of the largest companies on the US stock market, valued at $2.21 trillion. The new supercomputer will use Nvidia’s latest chip technology. The foundation, meanwhile, has a controlling stake in Novo Nordisk (NVO), a company worth more than Tesla. Its business is booming thanks to the widespread use of its diabetes drug Ozempic for weight loss and the popularity of Wegovy, which contains the same active ingredient as Ozempic. AI’s potential to speed up scientific research was highlighted earlier this year when Microsoft (MSFT) said that a new battery material had been found “in a matter of weeks, not years.” The Pacific Northwest National Laboratory, part of the US Department of Energy, used a Microsoft system that includes AI models and high-performance computing to winnow 32 million potential inorganic materials to 18 promising candidates in less than four days, Microsoft said in January. Writing about its collaboration with Microsoft, the PNNL said on its website: “The entire process, from receiving the simulated candidates through producing a functioning battery, took less than nine months, a blink of an eye compared with traditional methods.”

My judgement: positive makes sense.

MANUAL EVALUATION NEUTRAL BEFORE:
3:
Sentiment score:  -0.3499999940395355
Main text: Google
            
(GOOG) has fired the engineer who claimed an unreleased AI system had become sentient, the company confirmed, saying he violated employment and data security policies. Blake Lemoine, a software engineer for Google, claimed that a conversation technology called LaMDA had reached a level of consciousness after exchanging thousands of messages with it.  Google confirmed it had first put the engineer on leave in June. The company said it dismissed Lemoine’s “wholly unfounded” claims only after reviewing them extensively. He had reportedly been at Alphabet for seven years. In a statement, Google said it takes the development of AI “very seriously” and that it’s committed to “responsible innovation.”  Google is one of the leaders in innovating AI technology, which included LaMDA, or “Language Model for Dialog Applications.” Technology like this responds to written prompts by finding patterns and predicting sequences of words from large swaths of text – and the results can be disturbing for humans.  “What sort of things are you afraid of?” Lemoine asked LaMDA, in a Google Doc shared with Google’s top executives last April, the Washington Post reported. LaMDA replied: “I’ve never said this out loud before, but there’s a very deep fear of being turned off to help me focus on helping others. I know that might sound strange, but that’s what it is. It would be exactly like death for me. It would scare me a lot.” But the wider AI community has held that LaMDA is not near a level of consciousness.  “Nobody should think auto-complete, even on steroids, is conscious,” Gary Marcus, founder and CEO of Geometric Intelligence, said to CNN Business.  It isn’t the first time Google has faced internal strife over its foray into AI.  In December 2020, Timnit Gebru, a pioneer in the ethics of AI, parted ways with Google. As one of few Black employees at the company, she said she felt “constantly dehumanized.”  The sudden exit drew criticism from the tech world, including those within Google’s Ethical AI Team. Margaret Mitchell, a leader of Google’s Ethical AI team, was fired in early 2021 after her outspokenness regarding Gebru. Gebru and Mitchell had raised concerns over AI technology, saying they warned Google people could believe the technology is sentient.  On June 6, Lemoine posted on Medium that Google put him on paid administrative leave “in connection to an investigation of AI ethics concerns I was raising within the company” and that he may be fired “soon.” “It’s regrettable that despite lengthy engagement on this topic, Blake still chose to persistently violate clear employment and data security policies that include the need to safeguard product information,” Google said in a statement.  Lemoine said he is discussing with legal counsel and unavailable for comment.

My Judgement: Neutral makes sense

6:
Sentiment score:  0.1700000017881393
Main text: Some OnlyFans creators are using a TikTok artificial intelligence art filter to get around the platform’s community guidelines and promote their explicit content without getting their videos removed. The filter, which was created by TikTok and started trending in September, generates stunning painted landscapes from the photos that users upload. TikTokers initially used the filter to generate otherworldly paintings for users to use as their phone lock screens. Many used photos with their significant others or family members. The videos also often use the song “I Think I Like When It Rains” by WILLIS in the background. People began posting AI-generated paintings of their explicit photos around late October, according to meme database Know Your Meme. TikTok creator darlings. spam was the first to post a video using an explicit photo for the filter, according to Know Your Meme. Other TikTok users began joking in comments sections of videos that they’d prefer receiving an AI-generated painting over an actual nude. Some OnlyFans creators and sex workers have used the trend as an opportunity to promote their adult content without violating TikTok's policies. TikTok’s community guidelines prohibit “nudity, pornography, or sexually explicit content” on the platform. It also forbids “depictions, including digitally created or manipulated content, of nudity or sexual activity.” Although suggestive, the generated images aren't explicit — and they pique potential OnlyFans subscribers' interest. TikTok creator edgesovereign went viral with an AI-generated fantasy landscape. Creator michiganmexican's black-and-white portrait became a mountainous forest. Creator bearlyfunctionai posted what appeared to be a seaside vista. Amethyst Rose, a creator known as walmartladygaga on TikTok, said she joined in on the trend because she'd previously heard that "TikTok is a good way to promote" an OnlyFans account. Her video using the filter has over 124,000 views. She said she gained about 60 OnlyFans subscribers and 600 Twitter followers after posting it. "It definitely helped me get past some numbers I had been stuck on for quite some time," Rose said. "It was a nice little boost." TikTok's content moderation is notoriously restrictive — so much so that "algospeak," or code words or euphemisms that won't be flagged by TikTok, has developed into its own online dialect. Instead of referring to explicit photos as "nudes," for example, TikTok users will write out the word as "n00ds" or "spicy pics." Sex workers typically refer to themselves as "accountants" and refer to their content as "corn" instead of porn. A spokesperson for TikTok did not immediately respond to a request for comment. "I think this is the first trend that sex workers have been able to participate in that they don't have to worry as much about their TikTok being taken down because it's very artistic," Rose continued. "I think that because there's no skin showing, it would be hard to go against the guidelines, and I think that's why it's so popular." As the AI art filter grows in popularity, some TikTok users have expressed concerns that it could be “reversed,” exposing the creator’s actual

My judgement: Neutral makes sense

9:
Sentiment score:  -0.2829999923706054
Main text: REVERB is a new documentary series from CBS Reports. Watch "Racial Profiling 2.0" in the video player above.Throughout its history, the LAPD has found itself embroiled in controversy over racially biased policing. In 1992, police violence and the acquittal of four police officers who beat black motorist Rodney King culminated in riots that killed more than 50 people. Many reforms have been instituted in the decades since then, but racial bias in LA law enforcement continues to raise concerns. A 2019 report found that the LAPD pulled over black drivers four times as often as white drivers, and Latino drivers three times as often as whites, despite white drivers being more likely to have weapons, drugs or other contraband.New technological tools employed by the department could be aggravating the problem. In an effort to further reduce crime, the LAPD has turned to big data.Traditionally, police have stepped in to enforce the law after a crime has occurred, but advancements in artificial intelligence have helped create what are called "predictive policing" programs. These algorithm-driven systems analyze crime data to find a pattern, aiming to predict where crimes will be committed or even by whom. The idea is to stop crime before it happens by directing police to locations or people to target — following the hard, supposedly unbiased data. In the last decade, some of the largest police departments in the country have turned to predictive policing to reduce crimes in their communities, and the LAPD has helped to pioneer the trend.In 2011, the LAPD instituted a program they helped develop called PredPol, a location-based program that uses an algorithm to sift through historical crime data and predict where the next vehicle theft or burglary may occur. PredPol can precisely target areas as small as 500 by 500 feet. On the surface, using objective data to predict crime risk seems like a promising way to prevent subjective judgments or implicit bias about where to deploy police. But critics were quick to point out its flaws, asserting that using historical crime data may actually make matters worse.Although the data itself just amounts to a collection of numbers and locations, the police practices that led to the data's collection may be fraught with bias. Andrew Ferguson, a law professor and predictive policing expert, says this amplifies historical practices. "If you unthinkingly develop a data-driven policing system based on past police practices, you're kind of going to reify past police practices," he said.A group called the Stop LAPD Spying Coalition has focused on ending LAPD's use of predictive policing for almost a decade. In a 2016 letter posted online, the group explained its opposition: "It is widely known and well documented that police stop, detain, frisk, and arrest Black and Brown people overwhelmingly; therefore, the Black and Brown community will have a greater appearance in this historic crime data. This fact alone should put the validity of historic crime data into question. Because historic crime data is biased through the practice of racialized

My judgement: borders on negative, but neutral is understandable

MANUAL EVALUATION NEUTRAL AFTER:
647:
Sentiment score:  -0.2569999992847442
Main text: AI regulation is in the hands of "JCPenney leisure suit"-wearing lawmakers who still have "8-track tape players," which could mean trouble, says one Republican lawmaker. Last week, the U.S. House of Representatives took a small step toward building an AI regulatory framework by advancing the&nbsp;AI Accountability Act, which called for the government to study AI accountability and report back in 2025. "Let a bunch of guys up here that are wearing JCPenney leisure suits that still have 8-track tape players in their '72 Vegas start talking about technology, then you got some problems," Rep. Tim Burchett, R-Tenn., told Fox News when asked about regulation keeping pace with innovation in the AI sector. SHOULD CONGRESS DO MORE TO REGULATE AI TO KEEP UP WITH ITS INNOVATION? LAWMAKERS WEIGH IN. WATCH: WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE "I don't know that we need regulation," Burchett said. "You want to stifle growth, you start putting laws on it." The Senate had another listening session on AI development last Wednesday, but many lawmakers agreed that Congress still doesn't understand enough about AI yet to create regulations. "Right now, we're in the Wild West," Connecticut Democrat Sen. Richard Blumenthal told Fox News. "AI enables, not only in effect, appropriation of creative products … but also impersonation, deepfakes, a lot of bad stuff. We need to invest in the kinds of restraints and controls if there's a danger of AI becoming autonomous." "The problem with AI is that it's advancing so fast," Republican Rep. Nancy Mace of South Carolina said. "It's very difficult to regulate because you don't know what the next thing is going to be." ‘CONGRESS IS CLEARLY BEHIND ON AI’ AND NEEDS BIPARTISAN EFFORT TO CREATE REGULATIONS; LAWMAKERS WEIGH IN Artificial intelligence, a branch of computer science designed to understand and store human intelligence, has excelled in recent months as the tool increasingly mimics human capabilities. China and the European Union have drafted AI regulations this year, but Congress hasn't passed any legislation since the tech's rapid development started and as more critics voiced their concerns.&nbsp; "If you overregulate, like the government often does, you stifle innovation," Mace told Fox News. "And if we just stop AI, nothing is stopping China. We want to make sure that we are No. 1 in AI technology in the world and that it stays that way." ‘PEERBOTS’ CAN MEAN A FUTURE WHERE HUMAN POLITICIANS ARE OUT OF THE JOB: EXPERT Sen. Josh Hawley of Missouri, a Republican, told Fox News that AI will be great for the big corporations involved, but he questioned whether it would benefit everyday Americans. "Will it be good, though, for the American people, for American workers?" he said. AI advancements could reduce or eliminate 300 million jobs globally, according to a Goldman Sachs analysis published in March. Up to 30% of hours currently worked across the U.S. economy could become automated by 2030, creating the possibility of around 12 million occupational transitions in the coming years, according to a

My judgement: neutral makes sense.

455:
Sentiment score:  -0.363999992609024
Main text: Italy's deputy prime minister criticized the country's Data Protection Authority for implementing an immediate ban on AI chatbot ChatGPT over privacy concerns.&nbsp; "I find the decision of the Privacy Watchdog that forced #ChatGPT to prevent access from Italy disproportionate," Matteo Salvini, leader of a populist party known as the League Party, wrote on Instagram, according to Reuters. Salvini continued that the Data Protection Authority was "hypocritical" in temporarily banning ChatGPT and called for common sense as "privacy issues concern practically all online services," according to Reuters. Italy's Data Protection Authority, which is an independent agency that works to "protect fundamental rights and freedoms in connection with the processing of personal data," implemented a ban on OpenAI's ChatGPT program last week. OpenAI, a California-based company that is backed by Microsoft, officially disabled ChatGPT for Italian users on Friday. CHATGPT BANNED IN ITALY OVER PRIVACY, DATA COLLECTION CONCERNS  The watchdog group is investigating OpenAI on whether it complied with General Data Protection Regulation, which governs how data is used, processed and stored in the EU, according to the BBC. The watchdog group specifically accused OpenAI of failing to check the age of ChatGPT users and if they were over the age of 13. CHATGPT: CRITICS FEAR ARTIFICAL INTELLIGENCE TOOL HAS LIBERAL BIASES, PUSHES LEFT-WING TALKING POINTS "We look forward to working closely with [the Italian data agency] and educating them on how our systems are built and used," OpenAI said, according to Reuters, adding that the company works to reduce the use of personal data when training its systems.  Salvini added that the temporary ban could hurt businesses and innovation and that he hopes ChatGPT access in Italy is restored soon. "Every technological revolution brings great changes, risks and opportunities. It is right to control and regulate through an international cooperation between regulators and legislators, but it cannot be blocked," he said. TECH GURU BEHIND CHATGPT 'A LITTLE BIT SCARED' OF HIS CREATION: 'GOING TO ELIMINATE A LOT OF CURRENT JOBS' The data protection authority said OpenAI has 20 days to respond to its concerns and that the company could face a nearly $22 million fine. Stateside, a nonprofit research group called the Center for AI and Digital Policy filed a complaint with the Federal Trade Commission last week, accusing OpenAI of violating an FTC rule prohibiting unfair and deceptive business practices. The nonprofit is calling on the FTC to investigate the AI lab and stop it from releasing additional ChatGPT software.  More than 2,000 tech leaders, such as Elon Musk and Apple co-founder Steve Wozniak, college professors and others also signed an open letter published last week that calls on all AI labs to pause training systems specifically more powerful than GPT-4. CLICK HERE TO GET THE FOX NEWS APP The letter calls for a six-month pause on the labs, warning that "AI systems with human-competitive intelligence can pose profound risks to society and humanity."

My judgement: neutral makes sense.

554:
Sentiment score:  0.1280000060796737
Main text: ROME — Buried in ash after Mount Vesuvius’ cataclysmic eruption in A.D. 79, hundreds of papyrus scrolls have kept their secrets hidden for centuries. But archeologists have now been able to decipher some of the ancient text with the help of artificial intelligence. Discovered in the ruins of a villa thought to have been owned by Julius Caesar’s father-in-law, Lucius Calpurnius Piso Caesoninus, the Herculaneum papyri are a collection of around 1,000 scrolls that were carbonized during the eruption, along with thousands of other relics. Found by a farmworker in the 18th century, they are named after the place where they were buried, Herculaneum — an ancient Roman town to the south of Pompeii that was also destroyed by the blast. Previous attempts to unlock their secrets have failed because most of the scrolls were turned into carbonized ash and broke into pieces. However, a number of them were painstakingly unrolled by a monk over several decades and found to contain philosophical texts written in Greek. “Until now, the only way we have had to read what’s inside the Herculaneum scrolls is to put together the thousands of pieces of the ones that crumbled apart,” Richard Janko, a distinguished professor of classical studies at the University of Michigan, told NBC News on Thursday. “It’s like putting together a mosaic, and there’s not many people willing to do it,” he added. “So it may take 500 years to decipher their content. With this technique, hopefully, it should be much easier, and quicker.” The breakthrough came after a global competition was launched to accelerate the reading of the texts. The Vesuvius Challenge offered $1 million in prizes to anyone who could solve the problem and find a way to read the remaining 270 closed scrolls, most of which are preserved in a library in Naples, which is around 8 miles west of Herculaneum. It was launched by a team at the University of Kentucky led by professor Brent Seales, who released software and thousands of 3D X-ray images of three papyrus fragments and two rolled-up scrolls, in the hope that global research groups would take up the challenge. Seales’ team had already pioneered a way to “virtually unwrap” an ancient scroll from Israel using X-ray tomography and computer vision. But even that was not enough to read the barely visible ink on the ancient documents from Herculaneum. “The chemistry of the ink from the ancient world is different than the chemistry from medieval times. It’s largely invisible to the naked eye even when caught by the X-ray,” he said. “However, we know the tomography captures information about the ink.” “In 2019, we did come up with a solution based on artificial intelligence that allowed us to ‘see’ the ink, but it needed a lot of data, and we had a small team. So we launched the challenge to scale up our processes and accelerate the work,” he added. A total of 18 teams entered the competition, and the best results were

My judgement: could be a bit more on the positive side.

MANUAL EVALUATION NEGATIVE AFTER:
17:
Sentiment score:  -0.5659999847412109
Main text: The Justice Department cannot release audio from President Biden's interview with Special Counsel Robert Hur due to the threat of potential deepfakes, the DOJ argued in a Friday court filing. The filing came as part of a legal challenge against Biden's efforts to exercise executive privilege over the recording to keep it from the public. The DOJ acknowledged in its Friday filing that there is already enough public audio available to create AI deepfakes of both Biden and Hur, but it said releasing the true recording would make it more difficult to disprove any false versions. "The passage of time and advancements in audio, artificial intelligence, and ‘deep fake’ technologies only amplify concerns about malicious manipulation of audio files. If the audio recording is released here, it is easy to foresee that it could be improperly altered, and that the altered file could be passed off as an authentic recording and widely distributed," the department wrote. Associate Deputy Attorney General Bradley Weinsheimer wrote in the filing that releasing the tape would "make it far more likely that malicious actors could pass off a deepfake as the authentic recording." BIDEN ASSERTS EXECUTIVE PRIVILEGE OVER RECORDINGS FROM CLASSIFIED DOCUMENTS PROBE Biden's administration is facing a myriad of efforts from conservative legal groups and House Republicans to force the release of the audio. The DOJ has already released a transcript of the interview, which revealed multiple embarrassing moments for the president. BIDEN, NOT SPECIAL COUNSEL HUR, BROUGHT UP SON'S DEATH IN QUESTIONING Biden met with Hur for about five hours last year, when he was grilled about his handling of the classified documents. Hur's report, released earlier this year, declared Biden to be a forgetful, but well-meaning elderly man. The report highlighted several instances where Biden could not recall key details about his life, including when he served as vice president and the year of his son Beau Biden's death. Biden was outraged at the report and subsequently got caught in a number of false statements regarding his interview. For instance, he claimed that Hur brought up the topic of Beau's death, despite the transcript showing that Biden had broached the topic. HUNTER BIDEN IS IN COURT IN DELAWARE. HERE'S WHAT HE DOESN'T WANT THE JURY TO HEAR "President Biden is apparently afraid for the citizens of this country and everyone to hear those tapes," House Speaker Mike Johnson, R-La., said after Biden exerted privilege over the recording. "They obviously confirm what the special counsel has found, and would likely cause, I suppose, in his estimation, such alarm with the American people that the president is using all of his power to suppress their release." CLICK HERE TO GET THE FOX NEWS APP Some Republicans have speculated that the transcript of the interview may not line up with the audio, saying it may have been edited to prevent embarrassing Biden. Weinsheimer rejected those claims in Friday's filing, saying only minor adjustments were made to the transcript, such as removing repeated words and

My judgement: negative makes sense.

8:
Sentiment score:  -0.5260000228881836
Main text: An AI researcher developed a free speech alternative to ChatGPT and argued that the mainstream model has a liberal bias that prevents it from answering certain questions. "ChatGPT has political motivations, and it's seen through the product," said Arvin Bhangu, who founded the AI model Superintelligence. "There's a lot of political biases. We've seen where you can ask it give me 10 things Joe Biden has done well and give me 10 things Donald Trump has done well and it refuses to give quality answers for Donald Trump."  "Superintelligence is much more in line with the freedom to ask any type of question, so it's much more in line with the First Amendment than ChatGPT," Bhangu said. "No biases, no guardrails, no censorship."&nbsp; AI RESEARCHER CLAIMS HIS MODEL WILL ANSWER ANY QUESTION:  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE ChatGPT, an AI chatbot that can write essays, code and more, has been criticized for having politically biased responses. There's been numerous instances of the model refusing to provide answers — even fake ones — that could put a positive spin on conservatives, but would follow suit if the same prompt were submitted about a liberal. "Unfortunately, it is very hard to deal with this from a coding standpoint," Flavio Villanustre, the global chief information security officer for LexisNexis Risk Solutions, told Fox News in February. "It is very hard to prevent bias from happening." But the full potential of AI will only be realized when the models can provide unbiased, authentic answers, according to Bhangu. "Presenting an answer to the user and letting them determine what is right and wrong is a much better approach than trying to filter and trying to police the internet," he told Fox News.&nbsp;  AI CHATBOT 'HALLUCINATIONS' PERPETUATE POLITICAL FALSEHOODS, BIASES THAT HAVE REWRITTEN AMERICAN HISTORY OpenAI, the company that developed ChatGPT, is "training the AI to lie," Elon Musk told Fox News last month. He also hinted in a tweet that he might sue OpenAI, seeming to agree that the company defrauded him. Additionally, George Washington University Professor Jonathan Turley said ChatGPT fabricated sexual harassment claims against him and even cited a fake news article. ChatGPT also wouldn't generate an article in the style of the New York Post, but it did write an article modeled after CNN, bringing further criticisms of the platform showing bias.&nbsp; Bhangu said ChatGPT's biases hurt AI industry's credibility.&nbsp;  CLICK HERE TO GET THE FOX NEWS APP "ChatGPT's biases can have a detrimental effect on the credibility of the AI industry," he said. "This could have far-reaching negative implications for certain communities or individuals who rely heavily on AI models for important decisions." OpenAI did not respond to a request for comment. To watch the full interview with Bhangu, click here.&nbsp;

My judgement: negative makes sense.

3:
Sentiment score:  -0.5049999952316284
Main text: Artificial intelligence is making phone scams more sophisticated — and more believable. Scam artists are now using the technology to clone voices, including those of friends and family.The disturbing trend is adding to mounting losses due to fraud. Americans lost nearly $9 billion to fraud last year alone – an increase of over 150% in just two years, according to the Federal Trade Commission. The AI scam, which uses computer-generated voice, has left a trail of emotional devastation. Jennifer DeStefano, a mother, recounted during a U.S. Senate meeting her terrifying encounter with scammers who used the voice of her 15-year-old daughter, claiming they had her. "Mom, these bad men have me. Help me, help me, help me," DeStefano said she was told over the phone. But her daughter was safe in her bed. Kathy Stokes, the AARP director of fraud prevention, said younger people actually experience fraud and financial loss more often than older people, but it's the older generation who often have so much to lose.Pete Nicoletti, a cyber security expert at Check Point Software Technologies, said common software can recreate a person's voice after just 10 minutes of learning it. To protect against voice cloning scams, Nicoletti recommends families adopt a "code word" system and always call a person back to verify the authenticity of the call. Additionally, he advises setting social media accounts to private, as publicly available information can be easily used against individuals.

My judgement: negative makes sense.

