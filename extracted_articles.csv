Date,NewsPaper,Headline,MainText
20230329,cnn,"Using artificial intelligence and archival news articles, this teen found that Black homicide victims were less humanized in news coverage","Using artificial intelligence and archival news articles, a teenager in Northern Virginia created a program to measure media biases – and in researching older news articles, she found that Black homicide victims were less likely to be humanized in news coverage. Emily Ocasio, an 18-year-old from Falls Church, Virginia, created an AI program that analyzed FBI homicide records between 1976 and 1984 and their corresponding coverage published in The Boston Globe to determine whether victims were presented in a humanizing or impersonal way.   After analyzing 5,042 entries, the results showed that Black men under the age of 18 were 30% less likely to receive humanizing coverage than their White counterparts, Ocasio told CNN.  Black women were 23% less likely to be humanized in news stories, Ocasio added.   A news article was considered humanizing when it mentioned additional information about the victim and presented them “as a person, not just a statistic,” Ocasio said in her project presentation.     Her findings have not been reviewed by the larger scientific community, but she told CNN she hopes to expand her research and get it published in a scientific journal.     Ocasio’s project earned her second place in the prestigious Regeneron Science Talent Search on March 14 as well as a $175,000 scholarship.  Every year about 1,900 high school students from across the country participate in the competition, which started in 1942 and seeks to serve as a platform for young scientists to share original research.   Ocasio was among 40 finalists from more than 2,000 applications, according to Maya Ajmera, president and CEO of the Society for Science and executive publisher of Science News, who runs the competition sponsored by Regeneron.  “By using AI to document these biases, Emily shows that it can be safely used to help society answer complex social science questions,” her biography on the Society for Science website says. Ocasio said she has always been interested in social justice and science and saw this project as an opportunity to combine them. “Without the research, and without the statistics, you have no ability of understanding that entire communities are being left behind,” she said. Ocasio analyzed The Boston Globe’s news coverage because the newspaper had digital copies of its articles for the ’70s to ‘80s time period she focused on for her project, she said. CNN has reached out to the Boston Globe for comment.   Despite her findings, Ocasio believes science can’t explain everything: “You can never run an experiment in a lab that tells you about how racism works in society.” Ocasio, who has Puerto Rican heritage, said her own experiences helped shape her perspective of different races and cultures, and drew her to researching racism and inequalities. She wants to replicate her research to analyze other news outlets as well, she said. The talent search’s first-place winner, Neel Moudgal, told CNN the research done by the teenagers across the US is essential to helping solve some of society’s greatest challenges.   “I firmly believe that science is going to be the solution to a lot of our problems,” Moudgal said. His prize-winning project was a computer model that predicts the structure of RNA molecules to help develop tests and drugs for diseases such as cancer, autoimmune diseases, and viral infections.   Ajmera said seeing such projects from high school students gives her “an enormous hope for the future.” “We’re looking for the future scientific leaders of this country,” she said."
20230329,foxnews,"Elon Musk, Apple co-founder, other tech experts call for pause on 'giant AI experiments': 'Dangerous race'","Elon Musk, Steve Wozniak, and a host of other tech leaders and artificial intelligence experts are urging AI labs to pause development of powerful new AI systems in an open letter citing potential risks to society. The letter asks AI developers to ""immediately pause for at least 6 months the training of AI systems more powerful than GPT-4."" It was issued by the Future of Life Institute and signed by more than 1,000 people, including Musk, who argued that safety protocols need to be developed by independent overseers to guide the future of AI systems. GPT-4 is the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab.&nbsp; ""Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,"" the letter said. The letter warns that at this stage, no one ""can understand, predict, or reliably control"" the powerful new tools developed in AI labs. The undersigned tech experts cite the risks of propaganda and lies spread through AI-generated articles that look real, and even the possibility that Ai programs can outperform workers and make jobs obsolete.&nbsp; AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  ""AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts,"" the letter states. ""In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems."" ARTIFICIAL INTELLIGENCE ‘GODFATHER’ ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT’S NOT INCONCEIVABLE'  The signatories, which include Stability AI CEO Emad Mostaque, researchers at Alphabet-owned DeepMind, as well as AI heavyweights Yoshua Bengio and Stuart Russell, emphasize that AI development in general should be not paused, writing that their letter is calling for ""merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities."" According to the European Union's transparency register, the Future of Life Institute is primarily funded by the Musk Foundation, as well as London-based effective altruism group Founders Pledge, and Silicon Valley Community Foundation. ARTIFICIAL INTELLIGENCE EXPERTS ADDRESS BIAS IN CHATGPT: ‘VERY HARD TO PREVENT BIAS FROM HAPPENING’  Musk, whose electric car company Tesla uses AI for its autopilot system, has previously raised concerns about the rapid development of AI.&nbsp; Since its release last year, Microsoft-backed OpenAI's ChatGPT has prompted rivals to accelerate developing similar large language models, and companies to integrate generative AI models into their products. CLICK HERE TO GET THE FOX NEWS APP Notably absent from the letter's signatories was Sam Altman, CEO of OpenAI.&nbsp; Reuters contributed to this report."
20230329,foxnews,"Elon Musk's AI warning is 'unprecedented' and shows 'extraordinary' level of concern, says Douglas Murray","In an open letter, tech experts and leaders in the industry called for a six-month pause on AI experiments, a move that Fox News contributor Douglas Murray believes shows a ""deep concern"" that is growing about the risks of artificial intelligence. The letter, which was signed by Elon Musk and Apple co-founder Steve Wozniak, reads, in part: ""AI systems with human-competitive intelligence can pose profound risks to society … and should be planned for and managed with commensurate care. … Unfortunately, this level of planning and management is not happening."" Murray said on ""Fox &amp; Friends"" Wednesday that the request for a moratorium is extraordinary and is a sign that experts are worried. I INTERVIEWED CHATGPT AS IF IT WAS A HUMAN; HERE'S WHAT IT HAD TO SAY THAT GAVE ME CHILLS ""The fact that there has now been this stressing that we could be in trouble. This is unprecedented,"" Murray told host Brian Kilmeade. He explained that concerns are stemming from the idea that the artificial intelligence technology is able to operate at a higher level than human intelligence. For example, Murray said ChatGPT is producing work that other technologies cannot detect as computer-generated. ""So we are already in a state where the technology is running faster than teachers in America can run,"" he said.  Tristan Harris, co-founder of the Center for Humane Technology, said on ""The Brian Kilmeade Show"" that the world is witnessing the birth of a new age. ""I know that might sound like an extreme statement to make, but I really do think of it like the birth of the nuclear age,"" Harris said. AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  The GPT technology, Harris explained, has the ability to identify vulnerabilities in cybersecurity on command or seamlessly replicate a person’s voice using only three seconds of real audio.&nbsp; ""Our democracy, our society runs on language,"" he said. ""Code is language, law is language, contracts are language, media is language. When I can synthesize anyone saying anything else and then flood a democracy with untruths, … this is going to exponentiate a lot of the things that we saw with social media."" CLICK HERE TO GET THE FOX NEWS APP ""If you let a machine that runs on viral information, your society can sort of spin out into untruths really, really fast,"" Harris said. Murray compared the new artificial intelligence technology to the printing press, which revolutionized life in the Middle Ages. ""We don't know what the consequences of this are going to be,"" Murray said of AI development. ""And we are currently living through an era where it's printing press after printing press is being discovered underneath us."""
20230329,cbsnews,"Elon Musk, Bill Gates and other tech leaders call for pause on 'out of control' AI race","MIAMI -- Some of the biggest names in tech are calling for artificial intelligence labs to stop the training of the most powerful AI systems for at least six months, citing ""profound risks to society and humanity.""Elon Musk, Bill Gates and Steve Wozniak are among the dozens of tech leaders, professors and researchers who signed the letter, which was published by the Future of Life Institute, a nonprofit backed by Musk.The letter comes just two weeks after OpenAI announced GPT-4, an even more powerful version of the technology that underpins the viral AI chatbot tool, ChatGPT. In early tests and a company demo, the technology was shown drafting lawsuits, passing standardized exams and building a working website from a hand-drawn sketch.The letter, which was also signed by the CEO of OpenAI, said the pause should apply to AI systems ""more powerful than GPT-4."" It also said independent experts should use the proposed pause to jointly develop and implement a set of shared protocols for AI tools that are safe ""beyond a reasonable doubt.""""Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources,"" the letter said. ""Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one -- not even their creators -- can understand, predict, or reliably control.""If a pause is not put in place soon, the letter said governments should step in and create a moratorium.The wave of attention around ChatGPT late last year helped renew an arms race among tech companies to develop and deploy similar AI tools in their products. OpenAI, Microsoft and Google are at the forefront of this trend, but IBM, Amazon, Baidu and Tencent are working on similar technologies. A long list of startups are also developing AI writing assistants and image generators.Artificial intelligence experts have become increasingly concerned about AI tools' potential for biased responses, the ability to spread misinformation and the impact on consumer privacy. These tools have also sparked questions around how AI can upend professions, enable students to cheat, and shift our relationship with technology.Lian Jye Su, an analyst at ABI Research, said the letter shows legitimate concerns among tech leaders over the unregulated usage of AI technologies. But he called parts of the petition ""ridiculous,"" including the premise of asking for a hiatus in AI development beyond GPT-4. He said this could help some of the people who signed the letter preserve their dominance in the field.Musk was a founding member of OpenAI in 2015 but left three years later and has since criticized the company. Gates cofounded Microsoft, which has invested billions of dollars in OpenAI.""Corporate ambitions and desire for dominance often triumph over ethical concerns,"" Su said. ""I won't be surprised if these organizations are already testing something more advanced than ChatGPT or [Google's] Bard as we speak.""Still, the letter hints at the broader discomfort inside and outside the industry with the rapid pace of advancement in AI. Some governing agencies in China, the EU and Singapore have previously introduced early versions of AI governance frameworks."
20230329,foxnews,"Musk’s push to halt AI development makes no sense unless China is on board, GOP senator says","The top Republican on the Senate Artificial Intelligence Caucus warned Wednesday that pausing the development of AI technology could raise ""national security"" concerns on the same day that top tech industry giants called for a pause. In an open letter earlier in the day, tech industry giants like Tesla founder Elon Musk and Apple co-founder Steve Wozniak called on AI labs ""to immediately pause for at least 6 months the training of AI systems"" more advanced than the latest chatbot known as GPT-4. But Sen. Mike Rounds, R-S.D., who leads the Senate AI caucus, disagreed. ""Unless China, the Communist Party in China, is prepared to show evidence that they're going to do the same thing, I'm afraid then that we would be restricting our ability to move forward with AI for a period of six months while China does not,"" Rounds told Fox News Digital. AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  He explained that while he believes the push for a moratorium is endorsed by ""really bright people,"" it could leave the U.S. at a ""six months to a year disadvantage"" against U.S. adversaries, which he said would pose a challenge to U.S. national security. ""That does concern me. At the same time, I know that, in their letter, they didn't say that they couldn't improve the existing structures within existing AI, and I get that. I'm just not sure that it's enforceable with our adversary or peer competitors in the rest of the world,"" Rounds said.&nbsp; ARTIFICIAL INTELLIGENCE ‘GODFATHER’ ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT’S NOT INCONCEIVABLE' ""These are really bright people that have signed on to this. Maybe they think that they have the advantage,"" Rounds said. ""I’d like to hear their logic … the reasoning for why they're suggesting it right now, and what they hope to accomplish in six months.""  Rep. Jay Obernolte, R-Calif., who has led efforts to open pathways for the U.S. to improve its military capabilities through AI, concurred that such a delay could put the country at a disadvantage. ""The benefits to society will almost certainly far outweigh the costs, but it is critically important that we protect Americans from the misuse of AI systems while still enabling the industry to grow and innovate,"" Obernolte told Fox News Digital. ""Unfortunately, arbitrarily halting development of artificial intelligence is&nbsp;unlikely to solve these problems because unscrupulous actors seeking economic gain and adversaries seeking&nbsp;competitive&nbsp;advantage&nbsp;will certainly continue its development, exacerbating the potential disruption to our economy and our national security."" SENS. ROUNDS, JOHNSON TAKE ON ATF, INTRODUCE BILL TO EXPAND FULL-TIME TRAVELERS’ GUN OWNERSHIP RIGHTS Some of Rounds’ colleagues were more willing to get behind the tech industry’s bid to slow AI development. Sen. Michael Bennet, D-Colo., told Fox News Digital the American AI sector should be ""cautious."" ""When you have … some of the leading voices in tech ringing the alarm bells, saying that we need to figure out what the implications of this are gonna be for humanity before we impose another science experiment on the children in this country … we should be cautious,"" the Democrat said.  Sen. JD Vance, R-Ohio, deferred to the experts warning about AI’s potentially harmful capabilities. ""The one thing I'd say is that if Elon Musk and Wozniak and some of these people who know the computing industry better than anybody else are saying that we should be cautious, I'm inclined to agree with them. Because these guys know what they're talking about,"" Vance told Fox News Digital. The letter, published by Future of Life, warned that ""Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."" Failure to do that could ""risk loss of control of our civilization,"" it claims."
20230329,nbcnews,GPT-4 and OpenAI have shifted the direction of these 5 companies,"SAN FRANCISCO — Businesses and nonprofit groups agree on one thing after testing some of the latest in artificial intelligence: It is already changing the course of their operations.  Five organizations that were among the first to get access to GPT-4, the latest product from San Francisco startup OpenAI, said in interviews that they were reassigning employees, reorienting internal teams and re-evaluating their strategies in anticipation of the technology upending much of their work.  Their experiences back up the idea that, for better or worse, AI technology may very soon radically alter some people’s daily lives.  But the organizations also said that the technology required enormous amounts of work to customize to their specific needs, with employees giving daily feedback to the software to train it on terminology and methods specific to their fields, such as education or finance. OpenAI, best known for creating the AI chatbot ChatGPT, can then integrate the data from that work into its own model to potentially make its technology better.  In effect, each of the early testers is a microcosm of what others might go through as access to GPT-4 expands.  “There’s a perception in the marketplace now that you plug into these machines and they give you all the answers,” said Jeff McMillan, head of analytics, data and innovation for Morgan Stanley’s wealth management division.  That’s not true, he said. He said the bank has 300 employees putting some of their time into testing their tech using GPT-4.  “We have a team of people who literally review every response from the prior day,” he said.  For Morgan Stanley, the result has been a specialized chatbot built with GPT-4 that serves as an internal research tool for its staff of financial advisers. McMillan said the tool is trained not only on 60,000 research reports on parts of the global economy, but also 40,000 other internal documents from the firm — making it an expert on any financial subject that a financial adviser might want to look up.  To be sure, the early adopters of GPT-4 are not a random sample of the economy. OpenAI, which became for-profit in 2019, hand-picked the organizations over the past weeks and months.  Critics of OpenAI and its competitors allege that the AI sector has benefited from unskeptical hype over the past several months. OpenAI was looking for positive examples to show when it reached out six months ago to Khan Academy, a nonprofit educational organization, founder Sal Khan said.  “The context was: We’re going to be working on a next generation model; we want to be able to launch it with positive use cases,” he said.  Khan Academy is best known for its videos on YouTube, but since OpenAI reached out, Khan said it has poured resources into creating Khanmigo, a chatbot tutor that is specially trained in established concepts of teaching.  “We collectively spent about 100 hours fine-tuning the model so that it potentially can behave like a really good tutor,” he said.  “If you look at the cost of tutoring, this could be a very, very big deal,” Khan added. “It’s like having an amazing grad student or tutor or professor that you can start talking with in the moment.”  Stripe, a tech company that makes payments software and related products for business, said that when it got early access to GPT-4 in January, it pulled 100 employees from their regular jobs and assigned them to an internal “hackathon” in which each person spent a week on average testing out ideas.  Duolingo, an app for learning languages, got access to GPT-4 in the fall, and employees said that CEO Luis von Ahn was so taken with it that he called a meeting for 8 a.m. the following morning and immediately changed people’s jobs.  “He, after that, said, ‘Pivot your team,’” Edwin Bodge, a product manager, said. “Since then, we’ve been working extremely closely with GPT-4 and with the OpenAI team.”  So far, Duolingo has added a new, paid subscription tier costing $29.99 per month or $167.88 annually, which allows access to a a conversation chatbot in French or Spanish. They’ve also added an AI bot which will explain grammatical concepts to you as you progress through typical Duolingo lessons. According to Bodge, the company has crafted 1,000-2,000 word prompts for GPT-4 that power the bots. The company would not share the prompts upon request. All of the organizations who spoke with NBC News said they were proceeding with some degree of caution, given that AI technology is so new and the potential peril is unknown. Mike Buckley, CEO of Be My Eyes, a company that makes an app for people who are blind or have low vision, said that he’d like to get a test version of the app with GPT-4 into more hands, “but we want to be thoughtful and safe.”  “Could we launch this more broadly to the community in six to eight weeks? It’s possible, but we’re going to go where the data and the use cases take us,” he said.  The company works by connecting low-vision people with volunteers who, on a video call, can describe to app users what is around them — such as a product label in a grocery store, the directions through an airport or the wording in a greeting card. The version with GPT-4 works without a volunteer on the other end because the AI describes what it “sees” with the camera.  One of the app’s blind spokespeople used it to get directions on the London Underground subway system, according to a video she posted on TikTok.  “We’ve tried to break it,” Buckley said, adding that his staff ran thousands of tests. “We’ve slammed the technology as hard as we could for several weeks, and we’ve been pleasantly surprised.”  He said his company hadn’t run into any safety concerns with GPT-4, but it has made errors; for example, mixing up a toaster for a slow-cooker on a website. "
20230330,foxnews,Schools deploy AI technology to protect against active shooters,"WASHINGTON – While most people look to artificial intelligence, or AI, for quick answers to complex problems, a growing number of school districts are turning to the technology to keep their students and staff safe. A school district in Charles County, Maryland, roughly an hour from Washington D.C., is in the process of installing software and hardware which would allow their current security cameras to detect a potential active shooter.ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ ""This artificial intelligence has the ability to be able to identify a weapon, to assess what’s going on and how that person is acting,"" said Jason Stoddard, Director of School safety and Security for Charles County Public Schools. The district, through a state grant, is in the process of installing AI gun detection technology at all of its campuses. The cameras, which were installed years prior, will now communicate with a third party monitoring center if a gun is detected.&nbsp;  ""It plays the role of the human being that might or might not be monitoring,"" said Dave Fraser, CEO of Omnilert, which is one of a handful of companies offering the gun detection technology. ""The system is designed to allow for monitors to determine if a threat is real and if so, alert local police and school authorities within seconds.""TENNESSEE SCHOOL SHOOTING: WHAT TO KNOW ABOUT COVENANT SCHOOL IN NASHVILLE ZeroEyes, a Pennsylvania-based AI gun detection company, told Fox News its seen a surge of interest in recent years following multiple mass shootings on school campuses nationwide. The company told FOX it proudly employs law enforcement experts, people who’ve severed on the front lines, to faster assist schools when reviewing threats.  ""We have 135 employees and 80% of them come from the veteran community,"" said Mike Lahiff, CEO of ZeroEyes in an interview with FOX on Wednesday.  Tech experts admit the AI products do have limits and would not detect weapons hidden under coats or in backpacks. In Maryland, school officials said they have a multi-layer plan to deal with security and employ multiple methods for keeping students safe.CLICK HERE TO GET THE FOX NEWS APP ""It's not replacing the pillars that we have, which are building relationships and positive cultures inside our schools by having a well-trained staff and student body,"" added Stoddard."
20230330,foxnews,Democrats and Republicans coalesce around calls to regulate AI development: 'Congress has to engage',"Lawmakers in the highly-polarized 118th Congress appear to be finding some common ground with regard to artificial intelligence (AI). Several have indicated they would like to see some kind of regulation to rein in the fast-moving sector on the heels of a stunning warning from tech industry leaders. ""I think what you have to do is, to identify what is not allowed in terms of ethics and illegal activities, whether it is AI or not – you impose on AI activities the same level of ethics and privacy that you do for other competencies today,"" Sen. Mike Rounds, a leader of the Senate AI Caucus, told Fox News Digital. Homeland Security and Government Affairs Committee Chair Gary Peters, D-Mich., pointed out to Fox News Digital that his committee had recently held a hearing on the ""pros and cons"" of AI technology. ""I intend to have a series of hearings in Homeland Security and Government Affairs taking up AI and what we should be thinking about,"" Peters added. ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’  It comes on the heels of a dramatic letter signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants calling for a six-month pause to advanced AI developments, citing ""profound risks to society and humanity."" Sen. Michael Bennet, D-Colo., who sent a letter to tech company leaders last week calling for them to consider the safety of children when rolling out AI systems such as chatbots, suggested that an agency could be created to regulate the relatively restriction-free AI industry ""in the long term."" For now, however, the senator said these companies have to police themselves. ""I think we do have a role to play,"" he said when asked if Congress should step in to regulate AI. ""In the long run, I think what we could do is set up, you know, an agency here. They can negotiate on behalf of the American people, so we can actually have a negotiation about privacy… In the near term, I think it’s going to be important for tech to police itself."" AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  Sen. Brian Schatz, D-Hawaii, shared a similar suggestion, pointing out that he co-led legislation in the previous Congress aimed at enacting more barriers on AI’s growth. ""Congress has to sink its teeth into what to do about it. We've worked with [Retired Sen. Rob Portman, R-Ohio] to establish a law for AI, a commission for AI in government,"" Schatz told Fox News Digital. ""I think we should do something broader for AI throughout the private sector. But I think the first step is to recognize that this is a legitimate area for federal policy."" However, in his earlier comments, Rounds questioned whether existing laws were enough to cover the fast-moving sector.&nbsp; ""So if you're in a business, you know that there are certain rules you can't break,"" Rounds said. ""Those same things need to be applied to AI. The question is, do we have the appropriate language in the law today to address the things that AI might create, that we haven't thought about in our existing law?""  Over on the House side, Rep. Ken Buck, R-Colo., a leader in the efforts to crack down on Big Tech, also urged Congress to take the reins. ""With the emergence of AI comes both opportunity and challenges. We have seen the impact and consequences of a decade of inaction on Big Tech. Congress cannot afford to be caught sleeping at the wheel again. AI has great promise but left unscrutinized could be used to spread propaganda, dangerously restructure our economy, and increase the size of current Big Tech monopolies,"" Buck told Fox News Digital. CLICK HERE TO GET THE FOX NEWS APP Sen. JD Vance, R-Ohio, however, broke from his Senate colleagues to caution them to not rush into action before understanding the complicated technology. ""It's way too early to say what role Congress should take. I think right now, we need to understand this a little bit better. And, you know, look –we’re in the very early days of this process,"" Vance said. ""So I wouldn't want to commit to a congressional strategy before we even understand the problem."""
20230330,foxnews,"Unbridled AI tech risks spread of disinformation, requiring policy makers step in with rules: experts","Scores of technology experts and college professors across different academic backgrounds signed onto an open letter calling for a six-month pause on developing rapidly-evolving AI technology, which they say threatens humanity and society.&nbsp; At the heart of the argument for the pause is to give policymakers space to develop safeguards that would allow for researchers to keep developing the technology, but not at the reported threat of upending the lives of people across the world with disinformation.&nbsp; ""The federal government needs to play a central role using legislation and regulations to require the companies to impose much stricter safety measures and guardrails. However, legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed,"" Geoffrey Odlum, a retired 28-year diplomat who currently serves as president of Odlum Global Strategies, which advises the government and corporations on national security and tech policy issues, told Fox News Digital.&nbsp; Odlum is one of the more than 1,000 signatories of an open letter calling for all AI labs to pause their research for at least six months, arguing ""p​​owerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."" ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE'  The Elon Musk-backed letter specifically calls for AI labs to pause training systems that are more powerful than GPT-4, the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab.&nbsp; After the letter was released Wednesday, some critics dismissed it as ""just dripping with AI hype,"" including the authors behind a study cited in the letter.&nbsp; ""They basically say the opposite of what we say and cite our paper,"" said computer scientist Timnit Gebru on Twitter. Gebru is an author behind a study cited in the letter as alleged proof that ""AI systems with human-competitive intelligence can pose profound risks to society and humanity.""&nbsp; Gebru was joined by her co-author Emily Bender in lambasting the letter, saying their research was not about AI being ""too powerful,"" but instead focused on the risks of AI and its ""concentration of power in the hands of people, about reproducing systems of oppression, about damage to the information ecosystem,"" the Economist reported.&nbsp; ""Legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed. That's why I support the call for a 6-month pause in further developments[.]"" However, to those who signed on, they described that AI technology has essentially morphed into a dangerous Wild West that needs a governor.&nbsp; Such technology, supporters of the letter say, could be used to create disinformation, including by U.S. adversaries who want to cause chaos stateside. Odlum pointed to AI technology such as Dall-e 2, which can create realistic images depicting a phony arrest of former President Trump or President Biden kneeling to Chinese President Xi Jinping.&nbsp; ""It's clearly fake, but it looks photorealistic. So the average American would see that and freak out,"" Odlum told Fox News Digital.&nbsp; I INTERVIEWED CHATGPT AS IF IT WAS A HUMAN; HERE'S WHAT IT HAD TO SAY THAT GAVE ME CHILLS University of Pennsylvania professor of Medical Ethics and Health Policy, Jonathan D. Moreno, described to Fox News Digital he has similar concerns.&nbsp; ""This specific danger at the moment is our inability to know with confidence whether an AI platform has created a document or even an image - a moving image or a stationary image. We don't know what the system is doing,"" he said.&nbsp;  Currently, the U.S. has a handful of bills in Congress on AI, while some states have also tried to tackle the issue. However, the lack of hard-set rules has reportedly left some consumers and corporations in a confusing limbo, which is why Odlum is calling for the highest echelons of government to roll out uniform regulations.&nbsp; ""The White House does have an AI research office, and they have released what they called an AI Bill of Rights. Which called for the tech industry to develop AI responsibly and to protect data and to make sure algorithms aren't discriminatory,"" Odlum said, adding the document is ""a useful starting point."" CHATGPT NEW ANTI-CHEATING TECHNOLOGY INSTEAD CAN HELP STUDENTS FOOL TEACHERS AI labs that create technology that could be used by bad actors for disinformation or chaos do not currently face consequences for violating guides put forth by the White House or government agencies. To create these rules, the government needs to act swiftly, the retired diplomat said.&nbsp; ""Legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed. That's why I support the call for a 6-month pause in further developments, to allow the government time to examine the risks and engage the technology industry and civil society in a collaborative way to produce laws and regulations, safety measures and guardrails, to make sure that generative AI is not used by adversaries to create disinformation that divides us any further,"" Odlum said.&nbsp; ""It's not enough for one company to decide what the rules are, and not have a public conversation about it, try to get a sense of how to prevent bad actors. Although this horse may be out of the barn already."" Moreno told Fox News Digital that ""there's really no review at all"" regarding researchers’ work to make computers smarter, saying it is ""something that I think we've kind of let go of without asking industry to do a little more public consideration."" ELON MUSK'S AI WARNING IS 'UNPRECEDENTED' AND SHOWS 'EXTRAORDINARY' LEVEL OF CONCERN, SAYS DOUGLAS MURRAY Moreno has written about AI extensively in recent years, highlighting the question of regulating the industry back in 2019.&nbsp;  ""There is a great deal of regulation concerning biological experiments that could inadvertently create a ‘smart’ laboratory animal—like putting human-sourced neurons into a non-human primate embryo—but none concerning engineering developments that could lead to the singularity,"" Moreno wrote at the time in The Regulatory Review.&nbsp; ""Singularity"" in this context is defined as when a computer reaches superhuman intelligence, and was coined by mathematician Vernor Vinge 30 years ago.&nbsp; ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ ""Should some agency like the U.S. Consumer Product Safety Commission be empowered to verify that the standards are being administered? By the time the singularity has been achieved, a recall may be beside the point,"" Moreno wrote.&nbsp; He warned, ""At that point, in the words of the Borg in ""Star Trek,""'resistance is futile.'"" Fast-forward to 2023 when AI has become ""human-competitive at general tasks,"" according to the letter.&nbsp;Moreno said he wishes he were ""optimistic"" about creating rules on AI that would be industry-wide. CLICK HERE TO GET THE FOX NEWS APP ""Am I optimistic that we can actually create some rules that would be industry-wide? I wish I were. But I think at least, It's not enough for one company to decide what the rules are, and not have a public conversation about it, try to get a sense of how to prevent bad actors. Although this horse may be out of the barn already."""
20230330,foxnews,CONGRESS WEIGHS IN: Should tech companies pause 'giant AI experiments' as Elon Musk and others suggest?,"Congressional lawmakers weighed in Thursday on whether companies should pause advanced artificial intelligence training in the wake of an open letter signed by Elon Musk and other tech leaders. ""I think Elon Musk is rightfully being cautious,"" Rep. Brian Mast, a Florida Republican, told Fox News. ""I appreciate that he's looking to put the brakes on, and I agree with it.""  ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE' Musk, 2020 presidential candidate Andrew Yang, Apple co-founder Steve Wozniak and several other tech leaders urged AI labs to pause development of advanced systems in a recent open letter titled ""Pause Giant AI Experiments.""&nbsp; ""AI systems with human-competitive intelligence can pose profound risks to society and humanity,"" warns the letter, which has been signed by more than 1,400 people. The letter asks developers to halt training AI systems more powerful than GPT-4 for at least six months. San Francisco startup OpenAI's GPT-4 is the successor to the popular AI chatbot ChatGPT. SHOULD TECH COMPANIES PAUSE ‘GIANT AI EXPERIMENTS’?  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Rep. Victoria Spartz said she's less concerned with Musk's opinion and more concerned with protecting Americans' data and online privacy. ""We as the government have a duty to protect people rights and rights to life, liberty and property and we do not have good definitions on who owns your data,"" the Indiana Republican said. ""Big Tech companies are really abusing that and using unlimited immunity to suppress people's rights. And I think that's very dangerous."" Rep. Marcus Molinaro said innovation is important, but so is protecting privacy.  CLICK HERE TO GET THE FOX NEWS APP ""I would hope we could find some area of common ground to establish the appropriate guardrails,"" the New York Republican said. To hear more from lawmakers, click here."
20230330,foxnews,White House tight-lipped as push for congressional intervention into rapid AI developments heats up,"The White House remains largely on the sidelines of what has become a growing debate among Americans and lawmakers about the rapid developments being made in the artificial intelligence (AI) industry and whether there should be some type of congressional intervention. Fielding questions from the briefing room on Thursday, White House press secretary Karine Jean-Pierre did not say whether the Biden administration would urge lawmakers to federally regulate AI after she was asked by Fox News White House correspondent Peter Doocy about an open letter, which was signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants, that cited AI's ""profound risks to society and humanity."" ""It highlights a number of challenges addressed directly in the administration's blueprint for an AI bill of rights, which was released last October,"" Jean-Pierre said of the letter. ""It includes principles and practices AI creators can use to ensure protections related to safety, civil rights, civil liberties are integrated into AI systems from start to finish."" ""Right now, there's a comprehensive process that is underway to ensure a cohesive federal government approach to AI-related risks and opportunities, including how to ensure that AI innovation and deployment proceeds with appropriate prudence and safety foremost in mind,"" she added. ""I don't have anything else to announce at this point, at this time, but there is a comprehensive process in place."" BIDEN ADMIN SILENT AMID GROWING CONCERN FROM LAWMAKERS OVER RAPID DEVELOPMENT OF AI TECHNOLOGY  Doocy pressed Jean-Pierre on the seriousness of the matter and cited comments made by Eliezer Yudkowsky, a decision theorist at the Machine Intelligence Research Institute, who wrote in a recent op-ed that the six-month ""pause"" on developing ""AI systems more powerful than GPT-4"" — as called for by Musk and hundreds of other innovators and experts — understates the ""seriousness of the situation."" He would go further by implementing a moratorium on new large AI learning models that is ""indefinite and worldwide."" ""Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die,"" Yudkowsky said. ""Not as in 'maybe possibly some remote chance' but as in 'that is the obvious thing that would happen.'"" ""Would you agree that does not sound good?"" Doocy asked Jean-Pierre of Yudkowsky's claim. ""Your delivery, Peter, it's quite something,"" Jean-Pierre responded with a laugh. ""It sounds crazy, but is it?"" Doocy asked. ""All I can say is that there's a comprehensive process in place. We put out a blueprint back in October, as you know,"" she said in response. ""I don't have anything to share. We have seen the letter. We understand what their concerns are. Again, comprehensive process — we're gonna let that flow."" Doocy then asked Jean-Pierre whether President Biden is ""worried that artificial intelligence could become self-aware."" AI EXPERT WARNS ELON MUSK-SIGNED LETTER DOESN'T GO FAR ENOUGH, SAYS 'LITERALLY EVERYONE ON EARTH WILL DIE' ""Look, we are — again, there is a comprehensive process,"" she said. ""We are taking this very seriously. … I just don't want to get ahead of our findings and what that's going to look like, but it is a cohesive federal government approach to AI-related risks as you just laid out in a very dramatic way.""  ""We're going to move on. But thank you, Peter, for the drama,"" Jean-Pierre added. The Blueprint for an AI Bill of Rights — as referenced by Jean-Pierre during the briefing — was published by the White House Office of Science and Technology Policy in October and is a ""set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence."" The five principles featured in the blueprint include: safe and effective systems; algorithmic discrimination protections; data privacy; notice and explanation; and human alternatives, consideration and fallback. When reached for comment about the issue and whether the White House has concern over the rapid development of AI or believes it should be federally regulated, Jean-Pierre referred Fox News Digital to the National Security Council (NSC), which serves as Biden's ""principal forum for considering national security and foreign policy matters with his or her senior advisers and cabinet officials."" Despite signaling that it would respond rapidly to Fox News' request, after more than 24 hours, the NSC did not provide comment on the Biden administration's reaction to the call for an AI development moratorium.  CLICK HERE TO GET THE FOX NEWS APP The relative silence from the White House over potentially disruptive developments in AI comes as lawmakers from both sides of the aisle in the 118th Congress appear to be finding common ground in calling for oversight of the burgeoning technology. ""I think what you have to do is to identify what is not allowed in terms of ethics and illegal activities, whether it is AI or not. You impose on AI activities the same level of ethics and privacy that you do for other competencies today,"" South Dakota GOP Sen. Mike Rounds, a leader of the Senate AI Caucus, told Fox News Digital on Wednesday. Sen. Gary Peters, D-Mich., said the Senate Homeland Security and Governmental Affairs Committee, which he chairs, recently held a hearing on the ""pros and cons"" of AI technology. ""I intend to have a series of hearings in Homeland Security and [Governmental] Affairs taking up AI and what we should be thinking about,"" Peters said. Fox News' Chris Pandolfo contributed to this report."
20220125,nbcnews,Timnit Gebru is part of a wave of Black women working to change AI,"A computer scientist who said she was pushed out of her job at Google in December 2020 has marked the one-year anniversary of her ouster with a new research institute aiming to support the creation of ethical artificial intelligence.  Timnit Gebru, a known advocate for diversity in AI, announced the launch of the Distributed Artificial Intelligence Research Institute, or DAIR. Its website describes it as “a space for independent, community-rooted AI research free from Big Tech’s pervasive influence.” Part of how Gebru imagines creating such research is by moving away from the Silicon Valley ethos of “move fast and break things” — which was Facebook’s internal motto, coined by Mark Zuckerberg, until 2014 — to instead take a more deliberate approach to creating new technologies that serve marginalized communities. That includes recognizing and mitigating technologies’ potentials for harm from the beginning of their creation process, rather than after they’ve already caused damage to those communities, Gebru told NBC News.  “If those are our values, we can’t achieve them without slowing down and without putting in more resources per project that we’re working on,” she said.  Gebru said she learned from a December 2020 email from her manager’s manager that she had apparently resigned from her high-profile position as a co-lead of Google’s ethical AI team. Gebru said she never resigned, but was instead fired after requesting that executives explain why they demanded that Gebru retract a paper she co-authored. It was about how large language models — or AI trained on large amounts of text data, a version of which underpins Google’s own search engine — could reinforce racism, sexism and other systems of oppression.  Google’s head of research, Jeff Dean, said in a company email the paper “didn’t meet our bar for publication,” though others within the company cast doubt on that claim. Prior to her departure from Google, Gebru also emailed her colleagues informing them of the retraction request and detailing her frustrations with what she characterized as the company’s subpar efforts to create a more diverse and inclusive workplace.  The news of the alleged firing made headlines in the tech world and beyond, and it mobilized thousands of Google employees to join a solidarity campaign in support of Gebru, who is also the co-founder of Black in AI. At least two engineers resigned in protest of Gebru’s ousting. Google declined to comment for this story. A year later, DAIR has found financial support from major backers. The MacArthur Foundation, the Ford Foundation, Open Society Foundations, The Rockefeller Foundation and the Kapor Center have provided a cumulative $3.7 million in grants, Gebru said. She plans to publish DAIR’s research findings in both academic journals and alternative platforms and at a slower pace than the traditional timelines of both the tech industry and academia, she said. Researchers will be encouraged to disseminate their findings in forms that are accessible to the public, including websites and different forms of data visualization, Gebru said, adding that use of some DAIR data sets may require approval to maintain the institute’s mission of encouraging ethical applications of AI.  Of how she thinks about the relationship between future DAIR research and the actions of large tech companies like Microsoft or Google, Gebru said, “DAIR isn’t doing research for these companies but in the public interest.”  DAIR researchers will be recruited from, and embedded in, communities around the world, rather than being expected to originate from, or converge in, U.S. tech hubs, she added. DAIR’s first fellow, Raesetje Sefala, is based in Johannesburg, where she has been conducting research on the legacy of spatial apartheid, by creating the first publicly available data set of townships, or underdeveloped urban areas where Black South Africans were segregated through the end of apartheid in the 1990s.  Gebru and Sefala plan to continue to expand the research and hope to partner with policymakers to “help us advocate for policies that desegregate neighborhoods,” the paper noted. Coming from the community she studied, Sefala said, was crucial to the project’s success. “Just having that knowledge and experience of coming from a township, firstly I was able to better coordinate how to label those neighborhoods, and when the models were getting it wrong, it was very easy for me to go in and see why,” she said. “If you don’t know anything about townships and you just have this data set, I think it would’ve been very difficult for you to understand.” The project, Gebru said, is one example of how AI research can be enriched by researchers’ diversity of perspectives and lived experiences.  “There is no way I would have done this research on South Africa if it wasn’t for all my collaborators who are South African,” she said. “Their knowledge is just not something I can acquire myself.” In founding DAIR, Gebru joins a wave of Black female researchers who have founded their own independent institutes dedicated to pioneering more ethical and accountability-driven applications of AI systems, including Yeshimabeit Milner, founder of Data for Black Lives; Ruha Benjamin, founder of the Ida B. Wells Just Data Lab at Princeton University; and Joy Buolamwini, founder of the Algorithmic Justice League.  Buolamwini and Gebru co-authored an influential 2018 paper that showed that facial recognition technologies — used by Microsoft, IBM and the Chinese company Megvii — misclassified darker-skinned women at much higher rates than they did light-skinned men. Following the publication of that paper, IBM and Microsoft released statements acknowledging the research and announcing their commitments to improving the accuracy of their facial recognition technologies. The information from that paper also led IBM, Microsoft and Amazon to stop offering its facial recognition technologies to police.  In recent years, a growing body of research has found race-, gender- and ability-based biases embedded in algorithms used in policing, health care, hiring tools and remote testing technologies, among others. Gebru, Buolamwini and others have attributed these biases to the underrepresentation of women of all races, and people of color of all genders, in the AI workforce. A report published last year by the Stanford Institute for Human-Centered Artificial Intelligence found that women have accounted for just 18.3 percent of graduates of AI and computer science doctoral programs within the past 10 years and that Black and Latino 2019 graduates of AI doctoral programs accounted for just 2.4 percent and 3.2 percent of graduates overall, respectively. That report also cited a 2020 membership survey of 100 people by Queer in AI, which found that nonbinary people accounted for less than 10 percent of group members and that transgender women and men accounted for 5 percent and 2.5 percent of members, respectively. (The report did not measure the intersections of gender identity and race, nor did it measure the number of disabled people in AI.)  A 2019 report published by the AI Now Institute at New York University found that women constitute 18 percent of authors at leading AI conferences and between 10 and 15 percent of AI research staff at Facebook and Google. Black and Hispanic workers constituted between 2.5 and 6 percent of workers at Google, Facebook and Microsoft, the report noted.  Buolamwini said DAIR’s mission is critical in light of these inequities that are embedded into both the AI workforce and the technologies themselves.  “DAIR’s focus on research that centers the lived experiences of the excoded — those impacted by algorithmic harms — is a necessary intervention in a tech ecosystem that so often excludles, exploits, and expunges the very people who can transform the industry from within and without,” Buolamwini said in an emailed statement. “Dr. Timnit Gebru’s intellect is only outmatched by the depth of her compassion and the strength of her convictions in fighting for those whom society has relegated to the margins,” Buolamwini added, noting that she also hopes to see the organization receive more funding in the future. Gebru does, too: She’s hopeful that DAIR’s alternative approach to conducting AI research will give rise to new incentive structures that reward and fund research based not on the speed at which it’s produced, but on the communities that it serves. “A proactive approach means funding other visions of AI,” she said.   Follow NBCBLK on Facebook, Twitter and Instagram."
20230114,nbcnews,ChatGPT used by mental health tech app in AI experiment with users,"When people log in to Koko, an online emotional support chat service based in San Francisco, they expect to swap messages with an anonymous volunteer. They can ask for relationship advice, discuss their depression or find support for nearly anything else — a kind of free, digital shoulder to lean on. But for a few thousand people, the mental health support they received wasn’t entirely human. Instead, it was augmented by robots. In October, Koko ran an experiment in which GPT-3, a newly popular artificial intelligence chatbot, wrote responses either in whole or in part. Humans could edit the responses and were still pushing the buttons to send them, but they weren’t always the authors.  About 4,000 people got responses from Koko at least partly written by AI, Koko co-founder Robert Morris said.  The experiment on the small and little-known platform has blown up into an intense controversy since he disclosed it a week ago, in what may be a preview of more ethical disputes to come as AI technology works its way into more consumer products and health services.  Morris thought it was a worthwhile idea to try because GPT-3 is often both fast and eloquent, he said in an interview with NBC News.  “People who saw the co-written GTP-3 responses rated them significantly higher than the ones that were written purely by a human. That was a fascinating observation,” he said.  Morris said that he did not have official data to share on the test. Once people learned the messages were co-created by a machine, though, the benefits of the improved writing vanished. “Simulated empathy feels weird, empty,” Morris wrote on Twitter.  When he shared the results of the experiment on Twitter on Jan. 6, he was inundated with criticism. Academics, journalists and fellow technologists accused him of acting unethically and tricking people into becoming test subjects without their knowledge or consent when they were in the vulnerable spot of needing mental health support. His Twitter thread got more than 8 million views.  Senders of the AI-crafted messages knew, of course, whether they had written or edited them. But recipients saw only a notification that said: “Someone replied to your post! (written in collaboration with Koko Bot)” without further details of the role of the bot.  In a demonstration that Morris posted online, GPT-3 responded to someone who spoke of having a hard time becoming a better person. The chatbot said, “I hear you. You’re trying to become a better person and it’s not easy. It’s hard to make changes in our lives, especially when we’re trying to do it alone. But you’re not alone.”  No option was provided to opt out of the experiment aside from not reading the response at all, Morris said. “If you got a message, you could choose to skip it and not read it,” he said.  Leslie Wolf, a Georgia State University law professor who writes about and teaches research ethics, said she was worried about how little Koko told people who were getting answers that were augmented by AI.  “This is an organization that is trying to provide much-needed support in a mental health crisis where we don’t have sufficient resources to meet the needs, and yet when we manipulate people who are vulnerable, it’s not going to go over so well,” she said. People in mental pain could be made to feel worse, especially if the AI produces biased or careless text that goes unreviewed, she said.  Now, Koko is on the defensive about its decision, and the whole tech industry is once again facing questions over the casual way it sometimes turns unassuming people into lab rats, especially as more tech companies wade into health-related services.  Congress mandated the oversight of some tests involving human subjects in 1974 after revelations of harmful experiments including the Tuskegee Syphilis Study, in which government researchers denied proper treatment to Black men with syphilis and some of the men died. As a result, universities and others who receive federal support must follow strict rules when they conduct experiments with human subjects, a process enforced by what are known as institutional review boards, or IRBs.  But, in general, there are no such legal obligations for private corporations or nonprofit groups that don’t receive federal support and aren’t looking for approval from the Food and Drug Administration.  Morris said Koko has not received federal funding.  “People are often shocked to learn that there aren’t actual laws specifically governing research with humans in the U.S.,” Alex John London, director of the Center for Ethics and Policy at Carnegie Mellon University and the author of a book on research ethics, said in an email.  He said that even if an entity isn’t required to undergo IRB review, it ought to in order to reduce risks. He said he’d like to know which steps Koko took to ensure that participants in the research “were not the most vulnerable users in acute psychological crisis.”  Morris said that “users at higher risk are always directed to crisis lines and other resources” and that “Koko closely monitored the responses when the feature was live.”  After the publication of this article, Morris said in an email Saturday that Koko was now looking at ways to set up a third-party IRB process to review product changes. He said he wanted to go beyond current industry standard and show what’s possible to other nonprofits and services.  There are infamous examples of tech companies exploiting the oversight vacuum. In 2014, Facebook revealed that it had run a psychological experiment on 689,000 people showing it could spread negative or positive emotions like a contagion by altering the content of people’s news feeds. Facebook, now known as Meta, apologized and overhauled its internal review process, but it also said people should have known about the possibility of such experiments by reading Facebook’s terms of service — a position that baffled people outside the company due to the fact that few people actually have an understanding of the agreements they make with platforms like Facebook.  But even after a firestorm over the Facebook study, there was no change in federal law or policy to make oversight of human subject experiments universal.  Koko is not Facebook, with its enormous profits and user base. Koko is a nonprofit platform and a passion project for Morris, a former Airbnb data scientist with a doctorate from the Massachusetts Institute of Technology. It’s a service for peer-to-peer support — not a would-be disrupter of professional therapists — and it’s available only through other platforms such as Discord and Tumblr, not as a standalone app.  Koko had about 10,000 volunteers in the past month, and about 1,000 people a day get help from it, Morris said.  “The broader point of my work is to figure out how to help people in emotional distress online,” he said. “There are millions of people online who are struggling for help.”  There’s a nationwide shortage of professionals trained to provide mental health support, even as symptoms of anxiety and depression have surged during the coronavirus pandemic.  “We’re getting people in a safe environment to write short messages of hope to each other,” Morris said.  Critics, however, have zeroed in on the question of whether participants gave informed consent to the experiment.  Camille Nebeker, a University of California, San Diego professor who specializes in human research ethics applied to emerging technologies, said Koko created unnecessary risks for people seeking help. Informed consent by a research participant includes at a minimum a description of the potential risks and benefits written in clear, simple language, she said.  “Informed consent is incredibly important for traditional research,” she said. “It’s a cornerstone of ethical practices, but when you don’t have the requirement to do that, the public could be at risk.”  She noted that AI has also alarmed people with its potential for bias. And although chatbots have proliferated in fields like customer service, it’s still a relatively new technology. This month, New York City schools banned ChatGPT, a bot built on the GPT-3 tech, from school devices and networks.  “We are in the Wild West,” Nebeker said. “It’s just too dangerous not to have some standards and agreement about the rules of the road.”  The FDA regulates some mobile medical apps that it says meet the definition of a “medical device,” such as one that helps people try to break opioid addiction. But not all apps meet that definition, and the agency issued guidance in September to help companies know the difference. In a statement provided to NBC News, an FDA representative said that some apps that provide digital therapy may be considered medical devices, but that per FDA policy, the organization does not comment on specific companies.   In the absence of official oversight, other organizations are wrestling with how to apply AI in health-related fields. Google, which has struggled with its handling of AI ethics questions, held a “health bioethics summit” in October with The Hastings Center, a bioethics nonprofit research center and think tank. In June, the World Health Organization included informed consent in one of its six “guiding principles” for AI design and use.  Koko has an advisory board of mental-health experts to weigh in on the company’s practices, but Morris said there is no formal process for them to approve proposed experiments.  Stephen Schueller, a member of the advisory board and a psychology professor at the University of California, Irvine, said it wouldn’t be practical for the board to conduct a review every time Koko’s product team wanted to roll out a new feature or test an idea. He declined to say whether Koko made a mistake, but said it has shown the need for a public conversation about private sector research.  “We really need to think about, as new technologies come online, how do we use those responsibly?” he said.  Morris said he has never thought an AI chatbot would solve the mental health crisis, and he said he didn’t like how it turned being a Koko peer supporter into an “assembly line” of approving prewritten answers.  But he said prewritten answers that are copied and pasted have long been a feature of online help services, and that organizations need to keep trying new ways to care for more people. A university-level review of experiments would halt that search, he said.  “AI is not the perfect or only solution. It lacks empathy and authenticity,” he said. But, he added, “we can’t just have a position where any use of AI requires the ultimate IRB scrutiny.”  If you or someone you know is in crisis, call 988 to reach the Suicide and Crisis Lifeline. You can also call the network, previously known as the National Suicide Prevention Lifeline, at 800-273-8255, text HOME to 741741 or visit SpeakingOfSuicide.com/resources for additional resources."
20230328,cbsnews,What can Google's AI-powered Bard do? We tested it for you,"MOUNTAIN VIEW -- To use, or not to use, Bard? That is the Shakespearean question an Associated Press reporter sought to answer while testing out Google's artificially intelligent chatbot. The recently rolled-out bot dubbed Bard is the internet search giant's answer to the ChatGPT tool that Microsoft has been melding into its Bing search engine and other software.During several hours of interaction, the AP learned Bard is quite forthcoming about its unreliability and other shortcomings, including its potential for mischief in next year's U.S. presidential election. Even as it occasionally warned of the problems it could unleash, Bard repeatedly emphasized its belief that it will blossom into a force for good. At one point in its recurring soliloquies about its potential upsides, Bard dreamed about living up to the legacy of the English playwright that inspired its name. Bard explained that its creators at Google ""thought Shakespeare would be a good role model for me, as he was a master of language and communication.""But the chatbot also found some admirable traits in ""HAL,"" the fictional computer that killed some of a spacecraft's crew in the 1968 movie ""2001: A Space Odyssey."" Bard hailed HAL's intelligence calling it ""an interesting character"" before acknowledging its dark side.""I think HAL is a cautionary tale about the dangers of artificial intelligence,"" Bard assessed.WHAT'S BETTER — BARD OR BING?Bard praised ChatGPT, describing it as ""a valuable tool that can be used for a variety of purposes, and I am excited to see how it continues to develop in the future."" But Bard then asserted that it is just as intelligent as its rival, which was released late last year by its creator, the Microsoft-backed OpenAI. ""I would say that I am on par with ChatGPT,"" Bard said. ""We both have our own strengths and weaknesses, and we both have the ability to learn and grow.""During our wide-ranging conversation, Bard didn't display any of the disturbing tendencies that have cropped up in ChatGPT, which has likened another AP reporter to Hitler and tried to persuade a New York Times reporter to divorce his wife.IT'S FUNNY, BUT TAMER THAN BINGBard did get a little gooey at one point when asked to write a Shakespearean sonnet and responded seductively in one of the three drafts that it quickly created. ""I love you more than words can ever say, And I will always be there for you,"" Bard effused. ""You are my everything, And I will never let you go. So please accept this sonnet as a token Of my love for you, And know that I will always be yours.""But Bard seems to be deliberately tame most of the time, and probably for good reason, given what's at stake for Google, which has carefully cultivated a reputation for trustworthiness that has established its dominant search engine as the de facto gateway to the internet. An artificial intelligence tool that behaved as erratically as ChatGPT periodically might trigger a backlash that could damage Google's image and perhaps undercut its search engine, the hub of a digital advertising empire that generated more than $220 billion in revenue last year. Microsoft, in contrast, can afford to take more risks with the edgier ChatGPT because it makes more of its money from licensing software for personal computers.BARD ADMITS IT'S NOT PERFECTGoogle has programmed Bard to ensure it warns its users that it's prone to mistakes.Some inaccuracies are fairly easy to spot. For instance, when asked for some information about the AP reporter questioning it, Bard got most of the basics right, most likely by plucking tidbits from profiles posted on LinkedIn and Twitter.But Bard mysteriously also spit out inaccuracies about this reporter's academic background (describing him as a graduate of University of California, Berkeley, instead of San Jose State University) and professional background (incorrectly stating that he began his career at The Wall Street Journal before also working at The New York Times and The Washington Post).When asked to produce a short story about disgraced Theranos CEO Elizabeth Holmes, Bard summed up most of the highlights and lowlights of her saga. But one of Bard's three drafts incorrectly reported that Holmes was convicted of all the felony charges of fraud and conspiracy leveled against her during a four-month trial. Another version accurately reported Holmes was convicted on four counts of fraud and conspiracy without mentioning she was acquitted on four other charges (the jury hung on three other charges that were subsequently dismissed by prosecutors). ""I am still under development, and I am not perfect,"" Bard cautioned at one point. ""I can sometimes make mistakes, and I can sometimes be misunderstood. I am also aware that my technology can be used for both good and evil.""WHAT'S NEXT FOR BARD?Although Bard insisted it doesn't have a dark side, it acknowledged it can be used to damage reputations, disseminate propaganda that could incite violence and manipulate elections.""I could be used to create fake news articles or social media posts that could spread misinformation about candidates or their policies,"" Bard warned. ""I could also be used to suppress voter turnout by spreading discouraging messages or making it difficult for people to find information about how to vote.""On the lighter side, Bard proved helpful in finding interesting coloring books for adults and hit some notes that resonated during a discussion of rock and roll. When asked who is the greatest guitarist of all time, Bard responded with a broad list of candidates ranging from well-known rock artists such as Jimmy Page of Led Zeppelin to jazz virtuosos like Django Reinhardt before making the case for Jimi Hendrix ""as a strong contender for the title, his music continues to influence and inspire guitarists around the world.""Bard also seemed familiar with contemporary artists, such as Wet Leg, which recently won a Grammy award for best alternative music album. ""I've been listening to their debut album a lot lately and I really enjoy it. I think they have a bright future ahead of them,"" Bard said, and cited ""Ur Mom"" and ""Chaise Longue"" as its favorite songs by the group so far. Even with Bard's occasional miscues, it seemed savvy enough to ask about its potential role in reaching the singularity, a term popularized by computer scientist and author Ray Kurzweil to describe a turning point in the future when computers will be smarter than humans.""Some people believe that I am a big step toward the singularity,"" Bard said. ""I believe that I am a valuable tool that can help people to learn and understand the world around them. However, I do not believe that I am the singularity, and I do not believe that I will ever be able to replace human intelligence."""
20230328,cnn,Look of the Week: What Pope Francis’ AI puffer coat says about the future of fashion,"Over the weekend, a peculiar image of Pope Francis set the internet alight. Widely circulated on social media, the picture shows the 86-year-old pontiff dressed in a chunky longline white puffer coat, cinched at the waist and seemingly layered with other winter weather streetwear. It appeared to be a drastic departure from the typical regalia — robes, stoles and tall, pointed miter hats — often worn in the papal household. The outfit prompted a torrent of tongue-in-cheek questions online: Did the Pope have a new stylist? Has he always had a stylist? Was the look inspired by the backing dancers at Rihanna’s Superbowl show? More than anything, however, social media users exclaimed how they couldn’t believe the image was real. And it wasn’t. Twitter has since attached a contextual footnote to several of the best-performing tweets clarifying that it is AI-generated and was created using the software tool Midjourney. A 31-year-old construction worker from Chicago has since claimed ownership of the viral image.  AI (or artificial intelligence) imaging tools are becoming ever more sophisticated. The technology, which generates pictures based on users’ text prompts, has been used to design inclusive fashion shows, create entire graphic novels,  and even help envision new forms of architecture. But as AI develops and computer-generated “deep fake” imagery grows more convincing, many are concerned about the ethical implications, including the removal of subjects’ agency (placing people in fabricated scenarios that may be defamatory or malicious, for example)  and whether machine learning technology will one day make fake news indiscernible.  Just last week, AI-generated photos of Donald Trump being arrested spread like wildfire after the former president wrote on social media that he was expecting to be indicted in connection with a campaign finance investigation in New York. (Trump, who maintains his innocence, has yet to be charged on any counts.)    AI and the future of fashion If dressing is an important form of self-expression, then an AI-generated outfit might not only diminish the power and messaging inherent in clothes — but a person’s autonomy. In the papacy,  each garment holds religious significance. The color of the Pope’s vestments is specially selected to align with specific celebrations: red can only be worn during specific occasions, such as Palm Sunday, Good Friday and Pentecost, because it represents the blood of Jesus Christ, while pink is worn just twice a year. As such,  fake images of the Pope wearing certain clothes outside  these — or in countless other — contexts could cause offense, alarm or even mistrust within the Catholic community. Digitally altering someone’s outfit could have a lasting reputational damage, too. A doctored 2005 photo that appeared to show Paris Hilton at a nightclub wearing an inflammatory tank top that read “Stop Being Poor” became one of the most recognizable pop culture images of the early aughts. It added to the public perception of Hilton as an out-of-touch heiress. She publicly addressed the fake image in 2021, insisting people shouldn’t “believe everything you read.” (The vest, a which came from a fashion collection designed by Hilton’s sister, Nicky, actually read “Stop Being Desperate.”) During a conference at the Vatican on Monday, Pope Francis addressed the emergence of AI technology and urged scientists to consider its human impact  (though he did not specifically reference the furor over his own appearance being faked). “I am convinced that the development of artificial intelligence and machine learning has the potential to contribute in a positive way to the future of humanity,” Francis said, before adding:  “I am certain that this potential will be realized only if there is a constant and consistent commitment on the part of those developing these technologies to act ethically and responsibly.”    “I would therefore encourage you, in your deliberations, to make the intrinsic dignity of every man and woman the key criterion in evaluating emerging technologies,” said Francis. “These will prove ethically sound to the extent that they help respect that dignity and increase its expression at every level of human life.”"
20240529,foxnews,Fox News AI Newsletter: Musk's AI prediction,"Welcome to Fox News’ Artificial Intelligence newsletter with the latest AI technology advancements. IN TODAY’S NEWSLETTER: - Elon Musk expects AI will replace all human jobs, lead to 'universal high income'- FCC’s proposal to regulate AI in political ads is misguided, commissioner says- Indian military ramps up AI capabilities in effort to keep up with regional powers  SHOW ME THE MONEY: Billionaire entrepreneur Elon Musk reiterated his stance this week that artificial intelligence will eventually eliminate the need for humans to work, giving his vision for how the future will look as the technology continues to rapidly advance. AI IN POLITICAL ADS: The Federal Communications Commission last week proposed a new regulation that would require the use of artificial intelligence in political advertisements to be disclosed, which has one commissioner slamming the move as regulatory overreach ahead of the election.  HI-TECH WAR PLANNING: India, a country blessed with a strong high-tech industry, is applying its brains not just to commercial artificial intelligence but also to its military, as its neighbor and regional rival China continues to pour billions into AI research. CASH INFLUX: Billionaire Elon Musk's artificial intelligence startup xAI announced Sunday that the company raised $6 billion in Series B funding that lifts the company's valuation to $24 billion after the investment.  DON’T BE DUPED: Advanced artificial intelligence scams are lurking behind innocuous search engine queries, leveraging what's known as ""search engine optimization"" to deceive users, according to expert advice from GuidePoint Security, highlighting how cybercriminals manipulate these systems. Subscribe now to get the Fox News Artificial Intelligence Newsletter in your inbox. FOLLOW FOX NEWS ON SOCIAL MEDIA FacebookInstagramYouTubeTwitterLinkedIn SIGN UP FOR OUR OTHER NEWSLETTERS Fox News FirstFox News OpinionFox News LifestyleFox News Health DOWNLOAD OUR APPS Fox NewsFox BusinessFox WeatherFox SportsTubi WATCH FOX NEWS ONLINE Fox News Go STREAM FOX NATION Fox Nation Stay up to date on the latest AI technology advancements and learn about the challenges and opportunities AI presents now and for the future with Fox News&nbsp;here."
