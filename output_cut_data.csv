Date,NewsPaper,Headline,MainText
20230329,cnn,"Using artificial intelligence and archival news articles, this teen found that Black homicide victims were less humanized in news coverage","Using artificial intelligence and archival news articles, a teenager in Northern Virginia created a program to measure media biases – and in researching older news articles, she found that Black homicide victims were less likely to be humanized in news coverage. Emily Ocasio, an 18-year-old from Falls Church, Virginia, created an AI program that analyzed FBI homicide records between 1976 and 1984 and their corresponding coverage published in The Boston Globe to determine whether victims were presented in a humanizing or impersonal way. After analyzing 5,042 entries, the results showed that Black men under the age of 18 were 30% less likely to receive humanizing coverage than their White counterparts, Ocasio told CNN. Black women were 23% less likely to be humanized in news stories, Ocasio added. A news article was considered humanizing when it mentioned additional information about the victim and presented them “as a person, not just a statistic,” Ocasio said in her project presentation. Her findings have not been reviewed by the larger scientific community, but she told CNN she hopes to expand her research and get it published in a scientific journal. Ocasio’s project earned her second place in the prestigious Regeneron Science Talent Search on March 14 as well as a $175,000 scholarship. Every year about 1,900 high school students from across the country participate in the competition, which started in 1942 and seeks to serve as a platform for young scientists to share original research. Ocasio was among 40 finalists from more than 2,000 applications, according to Maya Ajmera, president and CEO of the Society for Science and executive publisher of Science News, who runs the competition sponsored by Regeneron. “By using AI to document these biases, Emily shows that it can be safely used to help society answer complex social science questions,” her biography on the Society for Science website says. Ocasio said she has always been interested in social justice and science and saw this project as an opportunity to combine them. “Without the research, and without the statistics, you have no ability of understanding that entire communities are being left behind,” she said. Ocasio analyzed The Boston Globe’s news coverage because the newspaper had digital copies of its articles for the ’70s to ‘80s time period she focused on for her project, she said. CNN has reached out to the Boston Globe for comment. Despite her findings, Ocasio believes science can’t explain everything: “You can never run an experiment in a lab that tells you about how racism works in society.” Ocasio, who has Puerto Rican heritage, said her own experiences helped shape her perspective of different races and cultures, and drew her to researching racism and inequalities. She wants to replicate her research to analyze other news outlets as well, she said. The talent search’s first-place winner, Neel Moudgal, told CNN the research done by the teenagers across the US is essential to helping solve some of society’s greatest challenges. “I firmly believe that science is going to be"
20230329,foxnews,"Elon Musk, Apple co-founder, other tech experts call for pause on 'giant AI experiments': 'Dangerous race'","Elon Musk, Steve Wozniak, and a host of other tech leaders and artificial intelligence experts are urging AI labs to pause development of powerful new AI systems in an open letter citing potential risks to society. The letter asks AI developers to ""immediately pause for at least 6 months the training of AI systems more powerful than GPT-4."" It was issued by the Future of Life Institute and signed by more than 1,000 people, including Musk, who argued that safety protocols need to be developed by independent overseers to guide the future of AI systems. GPT-4 is the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab.&nbsp; ""Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,"" the letter said. The letter warns that at this stage, no one ""can understand, predict, or reliably control"" the powerful new tools developed in AI labs. The undersigned tech experts cite the risks of propaganda and lies spread through AI-generated articles that look real, and even the possibility that Ai programs can outperform workers and make jobs obsolete.&nbsp; AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  ""AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts,"" the letter states. ""In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems."" ARTIFICIAL INTELLIGENCE ‘GODFATHER’ ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT’S NOT INCONCEIVABLE'  The signatories, which include Stability AI CEO Emad Mostaque, researchers at Alphabet-owned DeepMind, as well as AI heavyweights Yoshua Bengio and Stuart Russell, emphasize that AI development in general should be not paused, writing that their letter is calling for ""merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities."" According to the European Union's transparency register, the Future of Life Institute is primarily funded by the Musk Foundation, as well as London-based effective altruism group Founders Pledge, and Silicon Valley Community Foundation. ARTIFICIAL INTELLIGENCE EXPERTS ADDRESS BIAS IN CHATGPT: ‘VERY HARD TO PREVENT BIAS FROM HAPPENING’  Musk, whose electric car company Tesla uses AI for its autopilot system, has previously raised concerns about the rapid development of AI.&nbsp; Since its release last year, Microsoft-backed OpenAI's ChatGPT has prompted rivals to accelerate developing similar large language models, and companies to integrate generative AI models into their products. CLICK HERE TO GET THE FOX NEWS APP Notably absent from the letter's signatories was Sam Altman, CEO of OpenAI.&nbsp; Reuters contributed to this report."
20230329,foxnews,"Elon Musk's AI warning is 'unprecedented' and shows 'extraordinary' level of concern, says Douglas Murray","In an open letter, tech experts and leaders in the industry called for a six-month pause on AI experiments, a move that Fox News contributor Douglas Murray believes shows a ""deep concern"" that is growing about the risks of artificial intelligence. The letter, which was signed by Elon Musk and Apple co-founder Steve Wozniak, reads, in part: ""AI systems with human-competitive intelligence can pose profound risks to society … and should be planned for and managed with commensurate care. … Unfortunately, this level of planning and management is not happening."" Murray said on ""Fox &amp; Friends"" Wednesday that the request for a moratorium is extraordinary and is a sign that experts are worried. I INTERVIEWED CHATGPT AS IF IT WAS A HUMAN; HERE'S WHAT IT HAD TO SAY THAT GAVE ME CHILLS ""The fact that there has now been this stressing that we could be in trouble. This is unprecedented,"" Murray told host Brian Kilmeade. He explained that concerns are stemming from the idea that the artificial intelligence technology is able to operate at a higher level than human intelligence. For example, Murray said ChatGPT is producing work that other technologies cannot detect as computer-generated. ""So we are already in a state where the technology is running faster than teachers in America can run,"" he said.  Tristan Harris, co-founder of the Center for Humane Technology, said on ""The Brian Kilmeade Show"" that the world is witnessing the birth of a new age. ""I know that might sound like an extreme statement to make, but I really do think of it like the birth of the nuclear age,"" Harris said. AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’  The GPT technology, Harris explained, has the ability to identify vulnerabilities in cybersecurity on command or seamlessly replicate a person’s voice using only three seconds of real audio.&nbsp; ""Our democracy, our society runs on language,"" he said. ""Code is language, law is language, contracts are language, media is language. When I can synthesize anyone saying anything else and then flood a democracy with untruths, … this is going to exponentiate a lot of the things that we saw with social media."" CLICK HERE TO GET THE FOX NEWS APP ""If you let a machine that runs on viral information, your society can sort of spin out into untruths really, really fast,"" Harris said. Murray compared the new artificial intelligence technology to the printing press, which revolutionized life in the Middle Ages. ""We don't know what the consequences of this are going to be,"" Murray said of AI development. ""And we are currently living through an era where it's printing press after printing press is being discovered underneath us."""
20230329,cbsnews,"Elon Musk, Bill Gates and other tech leaders call for pause on 'out of control' AI race","MIAMI -- Some of the biggest names in tech are calling for artificial intelligence labs to stop the training of the most powerful AI systems for at least six months, citing ""profound risks to society and humanity.""Elon Musk, Bill Gates and Steve Wozniak are among the dozens of tech leaders, professors and researchers who signed the letter, which was published by the Future of Life Institute, a nonprofit backed by Musk.The letter comes just two weeks after OpenAI announced GPT-4, an even more powerful version of the technology that underpins the viral AI chatbot tool, ChatGPT. In early tests and a company demo, the technology was shown drafting lawsuits, passing standardized exams and building a working website from a hand-drawn sketch.The letter, which was also signed by the CEO of OpenAI, said the pause should apply to AI systems ""more powerful than GPT-4."" It also said independent experts should use the proposed pause to jointly develop and implement a set of shared protocols for AI tools that are safe ""beyond a reasonable doubt.""""Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources,"" the letter said. ""Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one -- not even their creators -- can understand, predict, or reliably control.""If a pause is not put in place soon, the letter said governments should step in and create a moratorium.The wave of attention around ChatGPT late last year helped renew an arms race among tech companies to develop and deploy similar AI tools in their products. OpenAI, Microsoft and Google are at the forefront of this trend, but IBM, Amazon, Baidu and Tencent are working on similar technologies. A long list of startups are also developing AI writing assistants and image generators.Artificial intelligence experts have become increasingly concerned about AI tools' potential for biased responses, the ability to spread misinformation and the impact on consumer privacy. These tools have also sparked questions around how AI can upend professions, enable students to cheat, and shift our relationship with technology.Lian Jye Su, an analyst at ABI Research, said the letter shows legitimate concerns among tech leaders over the unregulated usage of AI technologies. But he called parts of the petition ""ridiculous,"" including the premise of asking for a hiatus in AI development beyond GPT-4. He said this could help some of the people who signed the letter preserve their dominance in the field.Musk was a founding member of OpenAI in 2015 but left three years later and has since criticized the company. Gates cofounded Microsoft, which has invested billions of dollars in OpenAI.""Corporate ambitions and desire for dominance often triumph over ethical concerns,"" Su said. ""I won't be surprised if these organizations are already testing something more advanced than ChatGPT or [Google's] Bard as"
20230329,foxnews,"Musk’s push to halt AI development makes no sense unless China is on board, GOP senator says","The top Republican on the Senate Artificial Intelligence Caucus warned Wednesday that pausing the development of AI technology could raise ""national security"" concerns on the same day that top tech industry giants called for a pause. In an open letter earlier in the day, tech industry giants like Tesla founder Elon Musk and Apple co-founder Steve Wozniak called on AI labs ""to immediately pause for at least 6 months the training of AI systems"" more advanced than the latest chatbot known as GPT-4. But Sen. Mike Rounds, R-S.D., who leads the Senate AI caucus, disagreed. ""Unless China, the Communist Party in China, is prepared to show evidence that they're going to do the same thing, I'm afraid then that we would be restricting our ability to move forward with AI for a period of six months while China does not,"" Rounds told Fox News Digital. AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’ He explained that while he believes the push for a moratorium is endorsed by ""really bright people,"" it could leave the U.S. at a ""six months to a year disadvantage"" against U.S. adversaries, which he said would pose a challenge to U.S. national security. ""That does concern me. At the same time, I know that, in their letter, they didn't say that they couldn't improve the existing structures within existing AI, and I get that. I'm just not sure that it's enforceable with our adversary or peer competitors in the rest of the world,"" Rounds said.&nbsp; ARTIFICIAL INTELLIGENCE ‘GODFATHER’ ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT’S NOT INCONCEIVABLE' ""These are really bright people that have signed on to this. Maybe they think that they have the advantage,"" Rounds said. ""I’d like to hear their logic … the reasoning for why they're suggesting it right now, and what they hope to accomplish in six months."" Rep. Jay Obernolte, R-Calif., who has led efforts to open pathways for the U.S. to improve its military capabilities through AI, concurred that such a delay could put the country at a disadvantage. ""The benefits to society will almost certainly far outweigh the costs, but it is critically important that we protect Americans from the misuse of AI systems while still enabling the industry to grow and innovate,"" Obernolte told Fox News Digital. ""Unfortunately, arbitrarily halting development of artificial intelligence is&nbsp;unlikely to solve these problems because unscrupulous actors seeking economic gain and adversaries seeking&nbsp;competitive&nbsp;advantage&nbsp;will certainly continue its development, exacerbating the potential disruption to our economy and our national security."" SENS. ROUNDS, JOHNSON TAKE ON ATF, INTRODUCE BILL TO EXPAND FULL-TIME TRAVELERS’ GUN OWNERSHIP RIGHTS Some of Rounds’ colleagues were more willing to get behind the tech industry’s bid to slow AI development. Sen. Michael Bennet, D-Colo., told Fox News Digital the American AI sector should be ""cautious."" ""When you have … some of the leading voices in tech ringing the alarm bells, saying that we need to figure out what the implications of this are gonna"
20230329,nbcnews,GPT-4 and OpenAI have shifted the direction of these 5 companies,"SAN FRANCISCO — Businesses and nonprofit groups agree on one thing after testing some of the latest in artificial intelligence: It is already changing the course of their operations. Five organizations that were among the first to get access to GPT-4, the latest product from San Francisco startup OpenAI, said in interviews that they were reassigning employees, reorienting internal teams and re-evaluating their strategies in anticipation of the technology upending much of their work. Their experiences back up the idea that, for better or worse, AI technology may very soon radically alter some people’s daily lives. But the organizations also said that the technology required enormous amounts of work to customize to their specific needs, with employees giving daily feedback to the software to train it on terminology and methods specific to their fields, such as education or finance. OpenAI, best known for creating the AI chatbot ChatGPT, can then integrate the data from that work into its own model to potentially make its technology better. In effect, each of the early testers is a microcosm of what others might go through as access to GPT-4 expands. “There’s a perception in the marketplace now that you plug into these machines and they give you all the answers,” said Jeff McMillan, head of analytics, data and innovation for Morgan Stanley’s wealth management division. That’s not true, he said. He said the bank has 300 employees putting some of their time into testing their tech using GPT-4. “We have a team of people who literally review every response from the prior day,” he said. For Morgan Stanley, the result has been a specialized chatbot built with GPT-4 that serves as an internal research tool for its staff of financial advisers. McMillan said the tool is trained not only on 60,000 research reports on parts of the global economy, but also 40,000 other internal documents from the firm — making it an expert on any financial subject that a financial adviser might want to look up. To be sure, the early adopters of GPT-4 are not a random sample of the economy. OpenAI, which became for-profit in 2019, hand-picked the organizations over the past weeks and months. Critics of OpenAI and its competitors allege that the AI sector has benefited from unskeptical hype over the past several months. OpenAI was looking for positive examples to show when it reached out six months ago to Khan Academy, a nonprofit educational organization, founder Sal Khan said. “The context was: We’re going to be working on a next generation model; we want to be able to launch it with positive use cases,” he said. Khan Academy is best known for its videos on YouTube, but since OpenAI reached out, Khan said it has poured resources into creating Khanmigo, a chatbot tutor that is specially trained in established concepts of teaching. “We collectively spent about 100 hours fine-tuning the model so that it potentially can behave like a really good tutor,” he said. “If you look"
20230330,foxnews,Schools deploy AI technology to protect against active shooters,"WASHINGTON – While most people look to artificial intelligence, or AI, for quick answers to complex problems, a growing number of school districts are turning to the technology to keep their students and staff safe. A school district in Charles County, Maryland, roughly an hour from Washington D.C., is in the process of installing software and hardware which would allow their current security cameras to detect a potential active shooter.ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ ""This artificial intelligence has the ability to be able to identify a weapon, to assess what’s going on and how that person is acting,"" said Jason Stoddard, Director of School safety and Security for Charles County Public Schools. The district, through a state grant, is in the process of installing AI gun detection technology at all of its campuses. The cameras, which were installed years prior, will now communicate with a third party monitoring center if a gun is detected.&nbsp;  ""It plays the role of the human being that might or might not be monitoring,"" said Dave Fraser, CEO of Omnilert, which is one of a handful of companies offering the gun detection technology. ""The system is designed to allow for monitors to determine if a threat is real and if so, alert local police and school authorities within seconds.""TENNESSEE SCHOOL SHOOTING: WHAT TO KNOW ABOUT COVENANT SCHOOL IN NASHVILLE ZeroEyes, a Pennsylvania-based AI gun detection company, told Fox News its seen a surge of interest in recent years following multiple mass shootings on school campuses nationwide. The company told FOX it proudly employs law enforcement experts, people who’ve severed on the front lines, to faster assist schools when reviewing threats.  ""We have 135 employees and 80% of them come from the veteran community,"" said Mike Lahiff, CEO of ZeroEyes in an interview with FOX on Wednesday.  Tech experts admit the AI products do have limits and would not detect weapons hidden under coats or in backpacks. In Maryland, school officials said they have a multi-layer plan to deal with security and employ multiple methods for keeping students safe.CLICK HERE TO GET THE FOX NEWS APP ""It's not replacing the pillars that we have, which are building relationships and positive cultures inside our schools by having a well-trained staff and student body,"" added Stoddard."
20230330,foxnews,Democrats and Republicans coalesce around calls to regulate AI development: 'Congress has to engage',"Lawmakers in the highly-polarized 118th Congress appear to be finding some common ground with regard to artificial intelligence (AI). Several have indicated they would like to see some kind of regulation to rein in the fast-moving sector on the heels of a stunning warning from tech industry leaders. ""I think what you have to do is, to identify what is not allowed in terms of ethics and illegal activities, whether it is AI or not – you impose on AI activities the same level of ethics and privacy that you do for other competencies today,"" Sen. Mike Rounds, a leader of the Senate AI Caucus, told Fox News Digital. Homeland Security and Government Affairs Committee Chair Gary Peters, D-Mich., pointed out to Fox News Digital that his committee had recently held a hearing on the ""pros and cons"" of AI technology. ""I intend to have a series of hearings in Homeland Security and Government Affairs taking up AI and what we should be thinking about,"" Peters added. ARTIFICIAL INTELLIGENCE 'GODFATHER' ON AI POSSIBLY WIPING OUT HUMANITY: ‘IT'S NOT INCONCEIVABLE’ It comes on the heels of a dramatic letter signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants calling for a six-month pause to advanced AI developments, citing ""profound risks to society and humanity."" Sen. Michael Bennet, D-Colo., who sent a letter to tech company leaders last week calling for them to consider the safety of children when rolling out AI systems such as chatbots, suggested that an agency could be created to regulate the relatively restriction-free AI industry ""in the long term."" For now, however, the senator said these companies have to police themselves. ""I think we do have a role to play,"" he said when asked if Congress should step in to regulate AI. ""In the long run, I think what we could do is set up, you know, an agency here. They can negotiate on behalf of the American people, so we can actually have a negotiation about privacy… In the near term, I think it’s going to be important for tech to police itself."" AI EXPERTS WEIGH DANGERS, BENEFITS OF CHATGPT ON HUMANS, JOBS AND INFORMATION: ‘DYSTOPIAN WORLD’ Sen. Brian Schatz, D-Hawaii, shared a similar suggestion, pointing out that he co-led legislation in the previous Congress aimed at enacting more barriers on AI’s growth. ""Congress has to sink its teeth into what to do about it. We've worked with [Retired Sen. Rob Portman, R-Ohio] to establish a law for AI, a commission for AI in government,"" Schatz told Fox News Digital. ""I think we should do something broader for AI throughout the private sector. But I think the first step is to recognize that this is a legitimate area for federal policy."" However, in his earlier comments, Rounds questioned whether existing laws were enough to cover the fast-moving sector.&nbsp; ""So if you're in a business, you know that there are certain rules you can't break,"" Rounds said. ""Those same things need"
20230330,foxnews,"Unbridled AI tech risks spread of disinformation, requiring policy makers step in with rules: experts","Scores of technology experts and college professors across different academic backgrounds signed onto an open letter calling for a six-month pause on developing rapidly-evolving AI technology, which they say threatens humanity and society.&nbsp; At the heart of the argument for the pause is to give policymakers space to develop safeguards that would allow for researchers to keep developing the technology, but not at the reported threat of upending the lives of people across the world with disinformation.&nbsp; ""The federal government needs to play a central role using legislation and regulations to require the companies to impose much stricter safety measures and guardrails. However, legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed,"" Geoffrey Odlum, a retired 28-year diplomat who currently serves as president of Odlum Global Strategies, which advises the government and corporations on national security and tech policy issues, told Fox News Digital.&nbsp; Odlum is one of the more than 1,000 signatories of an open letter calling for all AI labs to pause their research for at least six months, arguing ""p​​owerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."" ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE' The Elon Musk-backed letter specifically calls for AI labs to pause training systems that are more powerful than GPT-4, the latest deep learning model from OpenAI, which ""exhibits human-level performance on various professional and academic benchmarks,"" according to the lab.&nbsp; After the letter was released Wednesday, some critics dismissed it as ""just dripping with AI hype,"" including the authors behind a study cited in the letter.&nbsp; ""They basically say the opposite of what we say and cite our paper,"" said computer scientist Timnit Gebru on Twitter. Gebru is an author behind a study cited in the letter as alleged proof that ""AI systems with human-competitive intelligence can pose profound risks to society and humanity.""&nbsp; Gebru was joined by her co-author Emily Bender in lambasting the letter, saying their research was not about AI being ""too powerful,"" but instead focused on the risks of AI and its ""concentration of power in the hands of people, about reproducing systems of oppression, about damage to the information ecosystem,"" the Economist reported.&nbsp; ""Legislation and regulations take time, moving at bureaucratic speed, while generative AI is evolving at exponential speed. That's why I support the call for a 6-month pause in further developments[.]"" However, to those who signed on, they described that AI technology has essentially morphed into a dangerous Wild West that needs a governor.&nbsp; Such technology, supporters of the letter say, could be used to create disinformation, including by U.S. adversaries who want to cause chaos stateside. Odlum pointed to AI technology such as Dall-e 2, which can create realistic images depicting a phony arrest of former President Trump or President Biden kneeling to Chinese President Xi Jinping.&nbsp; ""It's clearly fake, but it"
20230330,foxnews,CONGRESS WEIGHS IN: Should tech companies pause 'giant AI experiments' as Elon Musk and others suggest?,"Congressional lawmakers weighed in Thursday on whether companies should pause advanced artificial intelligence training in the wake of an open letter signed by Elon Musk and other tech leaders. ""I think Elon Musk is rightfully being cautious,"" Rep. Brian Mast, a Florida Republican, told Fox News. ""I appreciate that he's looking to put the brakes on, and I agree with it.""  ELON MUSK, APPLE CO-FOUNDER, OTHER TECH EXPERTS CALL FOR PAUSE ON 'GIANT AI EXPERIMENTS': 'DANGEROUS RACE' Musk, 2020 presidential candidate Andrew Yang, Apple co-founder Steve Wozniak and several other tech leaders urged AI labs to pause development of advanced systems in a recent open letter titled ""Pause Giant AI Experiments.""&nbsp; ""AI systems with human-competitive intelligence can pose profound risks to society and humanity,"" warns the letter, which has been signed by more than 1,400 people. The letter asks developers to halt training AI systems more powerful than GPT-4 for at least six months. San Francisco startup OpenAI's GPT-4 is the successor to the popular AI chatbot ChatGPT. SHOULD TECH COMPANIES PAUSE ‘GIANT AI EXPERIMENTS’?  WATCH MORE FOX NEWS DIGITAL ORIGINALS HERE Rep. Victoria Spartz said she's less concerned with Musk's opinion and more concerned with protecting Americans' data and online privacy. ""We as the government have a duty to protect people rights and rights to life, liberty and property and we do not have good definitions on who owns your data,"" the Indiana Republican said. ""Big Tech companies are really abusing that and using unlimited immunity to suppress people's rights. And I think that's very dangerous."" Rep. Marcus Molinaro said innovation is important, but so is protecting privacy.  CLICK HERE TO GET THE FOX NEWS APP ""I would hope we could find some area of common ground to establish the appropriate guardrails,"" the New York Republican said. To hear more from lawmakers, click here."
20230330,foxnews,White House tight-lipped as push for congressional intervention into rapid AI developments heats up,"The White House remains largely on the sidelines of what has become a growing debate among Americans and lawmakers about the rapid developments being made in the artificial intelligence (AI) industry and whether there should be some type of congressional intervention. Fielding questions from the briefing room on Thursday, White House press secretary Karine Jean-Pierre did not say whether the Biden administration would urge lawmakers to federally regulate AI after she was asked by Fox News White House correspondent Peter Doocy about an open letter, which was signed by Tesla CEO Elon Musk, Apple co-founder Steve Wozniak and other tech giants, that cited AI's ""profound risks to society and humanity."" ""It highlights a number of challenges addressed directly in the administration's blueprint for an AI bill of rights, which was released last October,"" Jean-Pierre said of the letter. ""It includes principles and practices AI creators can use to ensure protections related to safety, civil rights, civil liberties are integrated into AI systems from start to finish."" ""Right now, there's a comprehensive process that is underway to ensure a cohesive federal government approach to AI-related risks and opportunities, including how to ensure that AI innovation and deployment proceeds with appropriate prudence and safety foremost in mind,"" she added. ""I don't have anything else to announce at this point, at this time, but there is a comprehensive process in place."" BIDEN ADMIN SILENT AMID GROWING CONCERN FROM LAWMAKERS OVER RAPID DEVELOPMENT OF AI TECHNOLOGY Doocy pressed Jean-Pierre on the seriousness of the matter and cited comments made by Eliezer Yudkowsky, a decision theorist at the Machine Intelligence Research Institute, who wrote in a recent op-ed that the six-month ""pause"" on developing ""AI systems more powerful than GPT-4"" — as called for by Musk and hundreds of other innovators and experts — understates the ""seriousness of the situation."" He would go further by implementing a moratorium on new large AI learning models that is ""indefinite and worldwide."" ""Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die,"" Yudkowsky said. ""Not as in 'maybe possibly some remote chance' but as in 'that is the obvious thing that would happen.'"" ""Would you agree that does not sound good?"" Doocy asked Jean-Pierre of Yudkowsky's claim. ""Your delivery, Peter, it's quite something,"" Jean-Pierre responded with a laugh. ""It sounds crazy, but is it?"" Doocy asked. ""All I can say is that there's a comprehensive process in place. We put out a blueprint back in October, as you know,"" she said in response. ""I don't have anything to share. We have seen the letter. We understand what their concerns are. Again, comprehensive process — we're gonna let that flow."" Doocy then asked Jean-Pierre whether President Biden is ""worried that artificial intelligence could become self-aware."" AI EXPERT WARNS ELON MUSK-SIGNED LETTER DOESN'T GO FAR ENOUGH, SAYS 'LITERALLY EVERYONE ON EARTH WILL DIE' ""Look,"
20220125,nbcnews,Timnit Gebru is part of a wave of Black women working to change AI,"A computer scientist who said she was pushed out of her job at Google in December 2020 has marked the one-year anniversary of her ouster with a new research institute aiming to support the creation of ethical artificial intelligence. Timnit Gebru, a known advocate for diversity in AI, announced the launch of the Distributed Artificial Intelligence Research Institute, or DAIR. Its website describes it as “a space for independent, community-rooted AI research free from Big Tech’s pervasive influence.” Part of how Gebru imagines creating such research is by moving away from the Silicon Valley ethos of “move fast and break things” — which was Facebook’s internal motto, coined by Mark Zuckerberg, until 2014 — to instead take a more deliberate approach to creating new technologies that serve marginalized communities. That includes recognizing and mitigating technologies’ potentials for harm from the beginning of their creation process, rather than after they’ve already caused damage to those communities, Gebru told NBC News. “If those are our values, we can’t achieve them without slowing down and without putting in more resources per project that we’re working on,” she said. Gebru said she learned from a December 2020 email from her manager’s manager that she had apparently resigned from her high-profile position as a co-lead of Google’s ethical AI team. Gebru said she never resigned, but was instead fired after requesting that executives explain why they demanded that Gebru retract a paper she co-authored. It was about how large language models — or AI trained on large amounts of text data, a version of which underpins Google’s own search engine — could reinforce racism, sexism and other systems of oppression. Google’s head of research, Jeff Dean, said in a company email the paper “didn’t meet our bar for publication,” though others within the company cast doubt on that claim. Prior to her departure from Google, Gebru also emailed her colleagues informing them of the retraction request and detailing her frustrations with what she characterized as the company’s subpar efforts to create a more diverse and inclusive workplace. The news of the alleged firing made headlines in the tech world and beyond, and it mobilized thousands of Google employees to join a solidarity campaign in support of Gebru, who is also the co-founder of Black in AI. At least two engineers resigned in protest of Gebru’s ousting. Google declined to comment for this story. A year later, DAIR has found financial support from major backers. The MacArthur Foundation, the Ford Foundation, Open Society Foundations, The Rockefeller Foundation and the Kapor Center have provided a cumulative $3.7 million in grants, Gebru said. She plans to publish DAIR’s research findings in both academic journals and alternative platforms and at a slower pace than the traditional timelines of both the tech industry and academia, she said. Researchers will be encouraged to disseminate their findings in forms that are accessible to the public, including websites and different forms of data visualization, Gebru said, adding that use of some DAIR"
20230114,nbcnews,ChatGPT used by mental health tech app in AI experiment with users,"When people log in to Koko, an online emotional support chat service based in San Francisco, they expect to swap messages with an anonymous volunteer. They can ask for relationship advice, discuss their depression or find support for nearly anything else — a kind of free, digital shoulder to lean on. But for a few thousand people, the mental health support they received wasn’t entirely human. Instead, it was augmented by robots. In October, Koko ran an experiment in which GPT-3, a newly popular artificial intelligence chatbot, wrote responses either in whole or in part. Humans could edit the responses and were still pushing the buttons to send them, but they weren’t always the authors. About 4,000 people got responses from Koko at least partly written by AI, Koko co-founder Robert Morris said. The experiment on the small and little-known platform has blown up into an intense controversy since he disclosed it a week ago, in what may be a preview of more ethical disputes to come as AI technology works its way into more consumer products and health services. Morris thought it was a worthwhile idea to try because GPT-3 is often both fast and eloquent, he said in an interview with NBC News. “People who saw the co-written GTP-3 responses rated them significantly higher than the ones that were written purely by a human. That was a fascinating observation,” he said. Morris said that he did not have official data to share on the test. Once people learned the messages were co-created by a machine, though, the benefits of the improved writing vanished. “Simulated empathy feels weird, empty,” Morris wrote on Twitter. When he shared the results of the experiment on Twitter on Jan. 6, he was inundated with criticism. Academics, journalists and fellow technologists accused him of acting unethically and tricking people into becoming test subjects without their knowledge or consent when they were in the vulnerable spot of needing mental health support. His Twitter thread got more than 8 million views. Senders of the AI-crafted messages knew, of course, whether they had written or edited them. But recipients saw only a notification that said: “Someone replied to your post! (written in collaboration with Koko Bot)” without further details of the role of the bot. In a demonstration that Morris posted online, GPT-3 responded to someone who spoke of having a hard time becoming a better person. The chatbot said, “I hear you. You’re trying to become a better person and it’s not easy. It’s hard to make changes in our lives, especially when we’re trying to do it alone. But you’re not alone.” No option was provided to opt out of the experiment aside from not reading the response at all, Morris said. “If you got a message, you could choose to skip it and not read it,” he said. Leslie Wolf, a Georgia State University law professor who writes about and teaches research ethics, said she was worried about how little Koko told people"
20230328,cbsnews,What can Google's AI-powered Bard do? We tested it for you,"MOUNTAIN VIEW -- To use, or not to use, Bard? That is the Shakespearean question an Associated Press reporter sought to answer while testing out Google's artificially intelligent chatbot. The recently rolled-out bot dubbed Bard is the internet search giant's answer to the ChatGPT tool that Microsoft has been melding into its Bing search engine and other software.During several hours of interaction, the AP learned Bard is quite forthcoming about its unreliability and other shortcomings, including its potential for mischief in next year's U.S. presidential election. Even as it occasionally warned of the problems it could unleash, Bard repeatedly emphasized its belief that it will blossom into a force for good. At one point in its recurring soliloquies about its potential upsides, Bard dreamed about living up to the legacy of the English playwright that inspired its name. Bard explained that its creators at Google ""thought Shakespeare would be a good role model for me, as he was a master of language and communication.""But the chatbot also found some admirable traits in ""HAL,"" the fictional computer that killed some of a spacecraft's crew in the 1968 movie ""2001: A Space Odyssey."" Bard hailed HAL's intelligence calling it ""an interesting character"" before acknowledging its dark side.""I think HAL is a cautionary tale about the dangers of artificial intelligence,"" Bard assessed.WHAT'S BETTER — BARD OR BING?Bard praised ChatGPT, describing it as ""a valuable tool that can be used for a variety of purposes, and I am excited to see how it continues to develop in the future."" But Bard then asserted that it is just as intelligent as its rival, which was released late last year by its creator, the Microsoft-backed OpenAI. ""I would say that I am on par with ChatGPT,"" Bard said. ""We both have our own strengths and weaknesses, and we both have the ability to learn and grow.""During our wide-ranging conversation, Bard didn't display any of the disturbing tendencies that have cropped up in ChatGPT, which has likened another AP reporter to Hitler and tried to persuade a New York Times reporter to divorce his wife.IT'S FUNNY, BUT TAMER THAN BINGBard did get a little gooey at one point when asked to write a Shakespearean sonnet and responded seductively in one of the three drafts that it quickly created. ""I love you more than words can ever say, And I will always be there for you,"" Bard effused. ""You are my everything, And I will never let you go. So please accept this sonnet as a token Of my love for you, And know that I will always be yours.""But Bard seems to be deliberately tame most of the time, and probably for good reason, given what's at stake for Google, which has carefully cultivated a reputation for trustworthiness that has established its dominant search engine as the de facto gateway to the internet. An artificial intelligence tool that behaved as erratically as ChatGPT periodically might trigger a backlash that could damage Google's image and perhaps undercut"
20230328,cnn,Look of the Week: What Pope Francis’ AI puffer coat says about the future of fashion,"Over the weekend, a peculiar image of Pope Francis set the internet alight. Widely circulated on social media, the picture shows the 86-year-old pontiff dressed in a chunky longline white puffer coat, cinched at the waist and seemingly layered with other winter weather streetwear. It appeared to be a drastic departure from the typical regalia — robes, stoles and tall, pointed miter hats — often worn in the papal household. The outfit prompted a torrent of tongue-in-cheek questions online: Did the Pope have a new stylist? Has he always had a stylist? Was the look inspired by the backing dancers at Rihanna’s Superbowl show? More than anything, however, social media users exclaimed how they couldn’t believe the image was real. And it wasn’t. Twitter has since attached a contextual footnote to several of the best-performing tweets clarifying that it is AI-generated and was created using the software tool Midjourney. A 31-year-old construction worker from Chicago has since claimed ownership of the viral image. AI (or artificial intelligence) imaging tools are becoming ever more sophisticated. The technology, which generates pictures based on users’ text prompts, has been used to design inclusive fashion shows, create entire graphic novels, and even help envision new forms of architecture. But as AI develops and computer-generated “deep fake” imagery grows more convincing, many are concerned about the ethical implications, including the removal of subjects’ agency (placing people in fabricated scenarios that may be defamatory or malicious, for example) and whether machine learning technology will one day make fake news indiscernible. Just last week, AI-generated photos of Donald Trump being arrested spread like wildfire after the former president wrote on social media that he was expecting to be indicted in connection with a campaign finance investigation in New York. (Trump, who maintains his innocence, has yet to be charged on any counts.) AI and the future of fashion If dressing is an important form of self-expression, then an AI-generated outfit might not only diminish the power and messaging inherent in clothes — but a person’s autonomy. In the papacy, each garment holds religious significance. The color of the Pope’s vestments is specially selected to align with specific celebrations: red can only be worn during specific occasions, such as Palm Sunday, Good Friday and Pentecost, because it represents the blood of Jesus Christ, while pink is worn just twice a year. As such, fake images of the Pope wearing certain clothes outside these — or in countless other — contexts could cause offense, alarm or even mistrust within the Catholic community. Digitally altering someone’s outfit could have a lasting reputational damage, too. A doctored 2005 photo that appeared to show Paris Hilton at a nightclub wearing an inflammatory tank top that read “Stop Being Poor” became one of the most recognizable pop culture images of the early aughts. It added to the public perception of Hilton as an out-of-touch heiress. She publicly addressed the fake image in 2021, insisting people shouldn’t “believe everything you read.” (The vest,"
20240529,foxnews,Fox News AI Newsletter: Musk's AI prediction,"Welcome to Fox News’ Artificial Intelligence newsletter with the latest AI technology advancements. IN TODAY’S NEWSLETTER: - Elon Musk expects AI will replace all human jobs, lead to 'universal high income'- FCC’s proposal to regulate AI in political ads is misguided, commissioner says- Indian military ramps up AI capabilities in effort to keep up with regional powers  SHOW ME THE MONEY: Billionaire entrepreneur Elon Musk reiterated his stance this week that artificial intelligence will eventually eliminate the need for humans to work, giving his vision for how the future will look as the technology continues to rapidly advance. AI IN POLITICAL ADS: The Federal Communications Commission last week proposed a new regulation that would require the use of artificial intelligence in political advertisements to be disclosed, which has one commissioner slamming the move as regulatory overreach ahead of the election.  HI-TECH WAR PLANNING: India, a country blessed with a strong high-tech industry, is applying its brains not just to commercial artificial intelligence but also to its military, as its neighbor and regional rival China continues to pour billions into AI research. CASH INFLUX: Billionaire Elon Musk's artificial intelligence startup xAI announced Sunday that the company raised $6 billion in Series B funding that lifts the company's valuation to $24 billion after the investment.  DON’T BE DUPED: Advanced artificial intelligence scams are lurking behind innocuous search engine queries, leveraging what's known as ""search engine optimization"" to deceive users, according to expert advice from GuidePoint Security, highlighting how cybercriminals manipulate these systems. Subscribe now to get the Fox News Artificial Intelligence Newsletter in your inbox. FOLLOW FOX NEWS ON SOCIAL MEDIA FacebookInstagramYouTubeTwitterLinkedIn SIGN UP FOR OUR OTHER NEWSLETTERS Fox News FirstFox News OpinionFox News LifestyleFox News Health DOWNLOAD OUR APPS Fox NewsFox BusinessFox WeatherFox SportsTubi WATCH FOX NEWS ONLINE Fox News Go STREAM FOX NATION Fox Nation Stay up to date on the latest AI technology advancements and learn about the challenges and opportunities AI presents now and for the future with Fox News&nbsp;here."
